{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9819595,"sourceType":"datasetVersion","datasetId":6020780},{"sourceId":9900911,"sourceType":"datasetVersion","datasetId":6082008}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-14T11:15:41.062904Z","iopub.execute_input":"2024-11-14T11:15:41.063700Z","iopub.status.idle":"2024-11-14T11:15:42.619475Z","shell.execute_reply.started":"2024-11-14T11:15:41.063644Z","shell.execute_reply":"2024-11-14T11:15:42.618461Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/multimodal-dataset-data-intervals-categories/data_intervals_with_categories.txt\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/readme.txt\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/7/7/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/47/47/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/19/19/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/22/22/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/2/2/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/35/35/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/50/50/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/23/23/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/5/5/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/61/61/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/36/36/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/20/20/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/45/45/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/60/60/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/64/64/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/41/41/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/39/39/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/32/32/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/25/25/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/42/42/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/52/52/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/75/75/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/8/8/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/38/38/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/12/12/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/55/55/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/49/49/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/62/62/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/53/53/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/70/70/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/34/34/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/18/18/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/79/79/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/65/65/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/67/67/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/78/78/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/28/28/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/66/66/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/56/56/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/72/72/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/26/26/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/74/74/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/15/15/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/69/69/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/77/77/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/43/43/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/71/71/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/1/1/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/58/58/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/59/59/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/30/30/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/14/14/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/76/76/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/57/57/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/9/9/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/46/46/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/21/21/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/44/44/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/40/40/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/80/80/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/6/6/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/11/11/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/68/68/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/63/63/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/37/37/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/51/51/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/33/33/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/54/54/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/48/48/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/29/29/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/24/24/camera.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/Arousal_Valence.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/Panas.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/raw_gsr.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/raw_ppg.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/Emotions.csv\n/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/73/73/camera.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2nd column is valence, 3rd column is arousal \n\n# 31 rows x 4 columns","metadata":{}},{"cell_type":"code","source":"# /kaggle/input/raw-ppg-gsr-data-for-emotion-recognition\n\nsample_arousal_valence = pd.read_csv(\"/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Arousal_Valence.csv\")\n# sample_arousal_valence = pd.read_csv(\"/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/Arousal_Valence.csv\")\nprint(sample_arousal_valence)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T11:16:43.541344Z","iopub.execute_input":"2024-11-14T11:16:43.541766Z","iopub.status.idle":"2024-11-14T11:16:43.566570Z","shell.execute_reply.started":"2024-11-14T11:16:43.541727Z","shell.execute_reply":"2024-11-14T11:16:43.565456Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"    16  6  3  4\n0   17  3  5  6\n1   18  5  5  7\n2   19  6  7  6\n3   20  6  5  6\n4   21  7  4  5\n5   22  5  2  3\n6   23  5  3  5\n7    8  5  5  7\n8    9  7  2  5\n9   10  6  4  5\n10  11  7  4  5\n11  12  6  5  7\n12  13  8  3  6\n13  14  5  3  6\n14  15  7  2  6\n15  24  5  5  6\n16  25  6  7  8\n17  26  3  5  7\n18  27  6  6  7\n19  28  5  6  7\n20  29  5  6  5\n21  30  6  5  6\n22  31  6  7  5\n23   0  4  7  7\n24   1  6  6  6\n25   2  5  6  8\n26   3  3  5  9\n27   4  5  6  6\n28   5  3  5  9\n29   6  5  5  6\n30   7  5  7  7\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_arousal_valence.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:59:38.841113Z","iopub.execute_input":"2024-11-13T14:59:38.841955Z","iopub.status.idle":"2024-11-13T14:59:38.848536Z","shell.execute_reply.started":"2024-11-13T14:59:38.841915Z","shell.execute_reply":"2024-11-13T14:59:38.847595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# sample_raw_gsr = pd.read_csv('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', delimiter=',', encoding='latin1')\n# # print(sample_raw_ppg)\n# pd.read_csv(\"/kaggle/input/raw_data_ppg_gsr/10/10/Arousal_Valence.csv\")\n# with open('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\nwith open('/kaggle/input/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\n    for _ in range(5):\n        print(file.readline())\n        \n# sample_raw_gsr = pd.read_csv('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv')","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:59:41.30326Z","iopub.execute_input":"2024-11-13T14:59:41.304088Z","iopub.status.idle":"2024-11-13T14:59:41.312928Z","shell.execute_reply.started":"2024-11-13T14:59:41.304047Z","shell.execute_reply":"2024-11-13T14:59:41.311947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\nwith open('/kaggle/input/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\n    for i, line in enumerate(file):\n        if 40 <= i <= 50:  # Print lines near the problematic line\n            print(f\"Line {i}: {line}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:59:47.150315Z","iopub.execute_input":"2024-11-13T14:59:47.151045Z","iopub.status.idle":"2024-11-13T14:59:47.165236Z","shell.execute_reply.started":"2024-11-13T14:59:47.150986Z","shell.execute_reply":"2024-11-13T14:59:47.164298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.\nraw_gsr.csv: This CSV file contains the raw GSR data. For each line, the first item (if not null) represents the timestamp. The second item denotes the GSR data. The third item stores trigger information. Similar to camera.csv, we use k + 10 and k + 100 to indicate the start and stop of the k-th video, respectively.\n\n4.\nraw_ppg.csv: This CSV file contains the raw PPG data. The organization structure of PPG data is the same as GSR data in raw_gsr.csv. The only difference between raw_ppg.csv and raw_gsr.csv is the number of lines per second (100 lines per second for PPG and 4 lines for GSR due to different sampling rates).","metadata":{}},{"cell_type":"code","source":"# with open('/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\nwith open('/kaggle/input/raw_data_ppg_gsr/10/10/raw_gsr.csv', 'r') as file:\n    for _ in range(55):\n        print(file.readline())","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:59:53.029988Z","iopub.execute_input":"2024-11-13T14:59:53.030752Z","iopub.status.idle":"2024-11-13T14:59:53.038057Z","shell.execute_reply.started":"2024-11-13T14:59:53.030707Z","shell.execute_reply":"2024-11-13T14:59:53.037138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for gsr\n\n# sample_raw_gsr = pd.read_csv(\n#     '/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_gsr.csv', \n#     names=['timestamp', 'gsr_value', 'extra_column'],  # Define expected columns\n#     na_values=[''],  # Treat empty strings as NaN\n#     skiprows=1  # Skip header if needed\n# )\n\nsample_raw_gsr = pd.read_csv(\n    '/kaggle/input/raw_data_ppg_gsr/10/10/raw_gsr.csv', \n    names=['timestamp', 'gsr_value', 'start_stop_trigger'],  # Define expected columns\n    na_values=[''],  # Treat empty strings as NaN\n#     skiprows=1  # Skip header if needed\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T15:15:26.66999Z","iopub.execute_input":"2024-11-13T15:15:26.67083Z","iopub.status.idle":"2024-11-13T15:15:26.687773Z","shell.execute_reply.started":"2024-11-13T15:15:26.670784Z","shell.execute_reply":"2024-11-13T15:15:26.686862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get unique start_stop_triggers\nunique_triggers = sample_raw_gsr['start_stop_trigger'].unique().tolist()\n\n# Find the positions (row numbers) of each unique start_stop_trigger\ntrigger_positions = {trigger: sample_raw_gsr[sample_raw_gsr['start_stop_trigger'] == trigger].index.tolist()\n                     for trigger in unique_triggers}\n\n# Print the results\nprint(\"Unique start_stop_triggers:\", unique_triggers)\nprint(\"Positions of start_stop_triggers:\", trigger_positions)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T15:15:28.652842Z","iopub.execute_input":"2024-11-13T15:15:28.653219Z","iopub.status.idle":"2024-11-13T15:15:28.685084Z","shell.execute_reply.started":"2024-11-13T15:15:28.653182Z","shell.execute_reply":"2024-11-13T15:15:28.683959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sample_raw_gsr)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T15:11:28.081909Z","iopub.execute_input":"2024-11-13T15:11:28.082692Z","iopub.status.idle":"2024-11-13T15:11:28.094885Z","shell.execute_reply.started":"2024-11-13T15:11:28.082652Z","shell.execute_reply":"2024-11-13T15:11:28.093867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load the raw GSR data\n# sample_raw_gsr = pd.read_csv('/kaggle/input/raw_data_ppg_gsr/10/10/raw_gsr.csv',\n#                              names=['timestamp', 'gsr_value', 'start_stop_trigger'],\n#                              na_values=[''])\n\n# # Load the Arousal_Valence ratings\n# arousal_valence_df = pd.read_csv('/kaggle/input/raw_data_ppg_gsr/10/10/Arousal_Valence.csv', \n#                                  names=['video_id', 'valence', 'arousal', 'dominance'])\n\n# # Dictionary to store data intervals for each video ID\n# data_intervals = {}\n\n# for k in range(32):  # video IDs from 0 to 31\n#     # Define start and stop triggers\n#     start_trigger = k + 10\n#     stop_trigger = k + 100\n\n#     # Find the row indices for start and stop triggers\n#     start_index = sample_raw_gsr[sample_raw_gsr['start_stop_trigger'] == start_trigger].index\n#     stop_index = sample_raw_gsr[sample_raw_gsr['start_stop_trigger'] == stop_trigger].index\n\n#     # Ensure start and stop triggers exist and capture the interval\n#     if not start_index.empty and not stop_index.empty:\n#         interval_data = sample_raw_gsr.loc[start_index[0]:stop_index[0], ['timestamp', 'gsr_value']]\n#         data_intervals[k] = interval_data  # Store interval data for video ID k\n\n# # Print or use data_intervals as needed to analyze arousal and valence correlations\n# # Print each interval data for each video ID\n# for video_id, interval_data in data_intervals.items():\n#     print(f\"Video ID: {video_id}\")\n#     print(interval_data)\n#     print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between intervals","metadata":{"execution":{"iopub.status.busy":"2024-11-13T15:28:21.71928Z","iopub.execute_input":"2024-11-13T15:28:21.720223Z","iopub.status.idle":"2024-11-13T15:28:21.857855Z","shell.execute_reply.started":"2024-11-13T15:28:21.720168Z","shell.execute_reply":"2024-11-13T15:28:21.856862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the raw GSR data\nsample_raw_gsr = pd.read_csv('/kaggle/input/raw_data_ppg_gsr/10/10/raw_gsr.csv',\n                             names=['timestamp', 'gsr_value', 'start_stop_trigger'],\n                             na_values=[''])\n\n# Load the Arousal_Valence ratings\narousal_valence_df = pd.read_csv('/kaggle/input/raw_data_ppg_gsr/10/10/Arousal_Valence.csv', \n                                 names=['video_id', 'valence', 'arousal', 'dominance'])\n\n# Dictionary to store data intervals with arousal and valence labels for each video ID\ndata_intervals_with_labels = {}\n\nfor k in range(32):  # video IDs from 0 to 31\n    # Define start and stop triggers\n    start_trigger = k + 10\n    stop_trigger = k + 100\n\n    # Find the row indices for start and stop triggers\n    start_index = sample_raw_gsr[sample_raw_gsr['start_stop_trigger'] == start_trigger].index\n    stop_index = sample_raw_gsr[sample_raw_gsr['start_stop_trigger'] == stop_trigger].index\n\n    # Ensure start and stop triggers exist and capture the interval\n    if not start_index.empty and not stop_index.empty:\n        interval_data = sample_raw_gsr.loc[start_index[0]:stop_index[0], ['timestamp', 'gsr_value']]\n\n        # Retrieve the arousal and valence values for the current video ID\n        valence = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'valence'].values[0]\n        arousal = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'arousal'].values[0]\n\n        # Store interval data along with valence and arousal labels\n        data_intervals_with_labels[k] = {\n            'gsr_data': interval_data,\n            'valence': valence,\n            'arousal': arousal\n        }\n\n# Print the GSR data intervals with their corresponding valence and arousal values\nfor video_id, data in data_intervals_with_labels.items():\n    print(f\"Video ID: {video_id}\")\n    print(\"Valence:\", data['valence'])\n    print(\"Arousal:\", data['arousal'])\n    print(\"GSR Data:\")\n    print(data['gsr_data'])\n    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between intervals\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T16:08:03.159079Z","iopub.execute_input":"2024-11-13T16:08:03.159874Z","iopub.status.idle":"2024-11-13T16:08:03.332187Z","shell.execute_reply.started":"2024-11-13T16:08:03.159831Z","shell.execute_reply":"2024-11-13T16:08:03.331249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use start stop triggers to match data time intervals","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the raw PPG data\nsample_raw_ppg = pd.read_csv('/kaggle/input/raw_data_ppg_gsr/10/10/raw_ppg.csv',\n                             names=['timestamp', 'ppg_value', 'start_stop_trigger'],\n                             na_values=[''])\n\n# Load the Arousal_Valence ratings\narousal_valence_df = pd.read_csv('/kaggle/input/raw_data_ppg_gsr/10/10/Arousal_Valence.csv', \n                                 names=['video_id', 'valence', 'arousal', 'dominance'])\n\n# Dictionary to store data intervals with arousal and valence labels for each video ID\nppg_data_intervals_with_labels = {}\n\nfor k in range(32):  # video IDs from 0 to 31\n    # Define start and stop triggers\n    start_trigger = k + 10\n    stop_trigger = k + 100\n\n    # Find the row indices for start and stop triggers\n    start_index = sample_raw_ppg[sample_raw_ppg['start_stop_trigger'] == start_trigger].index\n    stop_index = sample_raw_ppg[sample_raw_ppg['start_stop_trigger'] == stop_trigger].index\n\n    # Ensure start and stop triggers exist and capture the interval\n    if not start_index.empty and not stop_index.empty:\n        interval_data = sample_raw_ppg.loc[start_index[0]:stop_index[0], ['timestamp', 'ppg_value']]\n\n        # Retrieve the arousal and valence values for the current video ID\n        valence = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'valence'].values[0]\n        arousal = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'arousal'].values[0]\n\n        # Store interval data along with valence and arousal labels\n        ppg_data_intervals_with_labels[k] = {\n            'ppg_data': interval_data,\n            'valence': valence,\n            'arousal': arousal\n        }\n\n# Print the PPG data intervals with their corresponding valence and arousal values\nfor video_id, data in ppg_data_intervals_with_labels.items():\n    print(f\"Video ID: {video_id}\")\n    print(\"Valence:\", data['valence'])\n    print(\"Arousal:\", data['arousal'])\n    print(\"PPG Data:\")\n    print(data['ppg_data'])\n    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between intervals\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# No need to run this as the data and categories as stored in text file\n# Iterate through all subject folders and obtain their data intervals based on start stop trigger & arousal and valence\n \n# Not normalized - need this for time & frequency domain features extraction, apply standard scaler after train test split","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nbase_dir = '/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/'\ndata_intervals_not_normalized = []\n\n# Function to load and process PPG or GSR data\ndef load_sensor_data(file_path, start_trigger, stop_trigger, value_column):\n    data = pd.read_csv(file_path, names=['timestamp', value_column, 'start_stop_trigger'], na_values=[''])\n    start_index = data[data['start_stop_trigger'] == start_trigger].index\n    stop_index = data[data['start_stop_trigger'] == stop_trigger].index\n    if not start_index.empty and not stop_index.empty:\n        return data.loc[start_index[0]:stop_index[0], value_column].values\n    else:\n        return None\n\n# Mapping function for arousal and valence levels\ndef map_level(value):\n    if 1 <= value <= 3:\n        return 'L'  # Low\n    elif 4 <= value <= 6:\n        return 'M'  # Medium\n    elif 7 <= value <= 9:\n        return 'H'  # High\n    \n# Iterate over each subject's folder\nfor subject_id in os.listdir(base_dir):\n    subject_path = os.path.join(base_dir, subject_id, subject_id)\n    \n    # Check if the folder name is purely numeric (indicating a subject)\n    if os.path.isdir(subject_path) and subject_id.isdigit():\n        \n        # Load arousal and valence labels for this subject\n        arousal_valence_path = os.path.join(subject_path, 'Arousal_Valence.csv')\n        arousal_valence_df = pd.read_csv(arousal_valence_path, names=['video_id', 'valence', 'arousal', 'dominance'])\n        \n        # Paths to GSR and PPG data files\n        gsr_path = os.path.join(subject_path, 'raw_gsr.csv')\n        ppg_path = os.path.join(subject_path, 'raw_ppg.csv')\n\n        # Only process if both GSR and PPG data files exist\n        if os.path.exists(gsr_path) and os.path.exists(ppg_path):\n\n            for k in range(32):  # Loop over each video ID's start/stop triggers\n                start_trigger = k + 10\n                stop_trigger = k + 100\n\n                # Load GSR data interval\n                gsr_interval = load_sensor_data(gsr_path, start_trigger, stop_trigger, 'gsr_value')\n\n                # Load PPG data interval\n                ppg_interval = load_sensor_data(ppg_path, start_trigger, stop_trigger, 'ppg_value')\n\n                # Check if intervals are valid and retrieve labels\n                if gsr_interval is not None and ppg_interval is not None:\n                    valence = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'valence'].values[0]\n                    arousal = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'arousal'].values[0]\n                    \n                    # Map arousal and valence to Low, Medium, High\n                    arousal_level = map_level(arousal)\n                    valence_level = map_level(valence)\n                    arousal_valence_label = f\"A{arousal_level}V{valence_level}\"\n\n                    # Convert GSR and PPG data to comma-separated strings\n                    gsr_data_str = \",\".join(map(str, gsr_interval))\n                    ppg_data_str = \",\".join(map(str, ppg_interval))\n\n                    # Append data to the intervals list for model input\n                    data_intervals_not_normalized.append({\n                        'subject_id': subject_id,\n                        'video_id': k,\n                        'gsr_data': gsr_data_str,  # Store as comma-separated string\n                        'ppg_data': ppg_data_str,  # Store as comma-separated string\n                        'valence': valence,\n                        'arousal': arousal,\n                        'arousal_level': arousal_level,\n                        'valence_level': valence_level,\n                        'arousal_valence_label': arousal_valence_label  # Store arousal_valence_label\n                    })\n\n# Print a few samples to verify data\nfor data in data_intervals_not_normalized[:5]:  # Print first 5 samples as a check\n    print(f\"Subject ID: {data['subject_id']}, Video ID: {data['video_id']}\")\n    print(\"Valence:\", data['valence'])\n    print(\"Arousal:\", data['arousal'])\n    print(\"GSR Data:\", data['gsr_data'])\n    print(\"PPG Data:\", data['ppg_data'])\n    print(\"Arousal Valence Label:\", data['arousal_valence_label'])  # Print the label\n    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between samples\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T11:18:07.769740Z","iopub.execute_input":"2024-11-14T11:18:07.770155Z","iopub.status.idle":"2024-11-14T11:22:41.215094Z","shell.execute_reply.started":"2024-11-14T11:18:07.770117Z","shell.execute_reply":"2024-11-14T11:22:41.214025Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Subject ID: 7, Video ID: 0\nValence: 4\nArousal: 5\nGSR Data: 179125.171875,178570.375,179391.03125,181556.21875,182420.515625,183646.203125,183727.078125,183940.0,184140.109375,184302.125,184497.453125,184425.84375,184151.828125,183712.84375,183200.3125,182930.90625,182558.046875,182371.484375,182351.875,182348.90625,182571.71875,182762.59375,182850.78125,183005.96875,183131.21875,183223.3125,183341.90625,183501.390625,183623.203125,183718.921875,183845.875,183990.0625,184070.0625,184106.53125,184181.75,184225.921875,184179.484375,184101.109375,184012.8125,183851.015625,183785.625,183725.21875,183676.765625,183682.390625,183646.03125,183785.234375,183910.671875,183894.5,184009.25,184028.03125,184072.859375,184162.5625,184114.109375,184188.609375,184215.5,184219.015625,184314.234375,184334.859375,184143.5625,183615.875,182805.71875,181927.59375,180957.71875,180380.265625,180069.375,179868.71875,179733.6875,179904.6875,180134.609375,180490.90625,180799.6875,181055.21875,181257.625,181439.09375,181634.703125,181800.921875,181920.0625,182107.703125,182163.03125,182333.625,182389.96875,182461.3125,182565.28125,182543.171875,182640.984375,182654.921875,182691.265625,182827.484375,182866.15625,182997.203125,183045.8125,183130.515625,183130.046875\nPPG Data: 612293.0,611442.0,610770.0,610083.0,609360.0,608668.0,608015.0,607283.0,606648.0,606141.0,605718.0,605469.0,605411.0,605379.0,605524.0,605586.0,605686.0,605714.0,605755.0,605828.0,605885.0,606094.0,606187.0,606323.0,606569.0,606617.0,606693.0,606799.0,606992.0,607040.0,607351.0,607450.0,607268.0,607372.0,607403.0,607371.0,607484.0,607483.0,607591.0,607719.0,607932.0,607964.0,608094.0,608120.0,608302.0,608429.0,608569.0,608718.0,608778.0,608751.0,608720.0,608518.0,608449.0,608176.0,607804.0,607342.0,606885.0,606293.0,605910.0,605547.0,605232.0,604828.0,604488.0,604220.0,604064.0,603821.0,603681.0,603590.0,603502.0,603622.0,603760.0,603865.0,603948.0,604190.0,604299.0,604396.0,604459.0,604532.0,604598.0,604668.0,604746.0,604769.0,604884.0,604968.0,605247.0,605471.0,605387.0,604904.0,604344.0,603848.0,603497.0,603061.0,602863.0,602659.0,602468.0,602252.0,602225.0,602215.0,602411.0,602603.0,602968.0,603240.0,603543.0,603810.0,603982.0,604086.0,604181.0,604163.0,604097.0,603984.0,603844.0,603706.0,603607.0,603499.0,603284.0,603061.0,602662.0,602100.0,601476.0,600667.0,599913.0,599019.0,598241.0,597379.0,596578.0,595831.0,595162.0,594543.0,593956.0,593647.0,592589.0,592278.0,593933.0,591758.0,591467.0,591177.0,590871.0,590733.0,590488.0,590248.0,589999.0,589728.0,589476.0,589197.0,588821.0,588516.0,588163.0,587790.0,587590.0,587188.0,586842.0,586517.0,586275.0,585997.0,585771.0,585523.0,585299.0,585187.0,585002.0,584783.0,584608.0,584471.0,584307.0,584174.0,584040.0,583878.0,583798.0,583644.0,583519.0,583313.0,583312.0,583228.0,583073.0,583000.0,582904.0,582774.0,582756.0,582632.0,582433.0,582094.0,581740.0,581169.0,580533.0,579851.0,579153.0,578406.0,577688.0,577074.0,576425.0,575768.0,575247.0,574769.0,574388.0,573950.0,573676.0,573342.0,573024.0,572826.0,572562.0,572373.0,572239.0,572106.0,571944.0,571810.0,571633.0,571510.0,571284.0,571087.0,570766.0,570657.0,570442.0,570190.0,569966.0,569753.0,569543.0,569374.0,569166.0,568933.0,568873.0,568699.0,568601.0,568428.0,568326.0,568244.0,568190.0,567903.0,568422.0,568257.0,566406.0,568712.0,568583.0,568445.0,568207.0,567569.0,568875.0,567434.0,567394.0,567349.0,567314.0,567231.0,567077.0,566889.0,566601.0,566161.0,565638.0,565128.0,564566.0,563961.0,563407.0,562838.0,562326.0,561897.0,561404.0,561095.0,560726.0,560417.0,560194.0,559945.0,559735.0,559590.0,559459.0,559317.0,559147.0,559085.0,559042.0,558920.0,558816.0,558691.0,558582.0,558321.0,558222.0,558059.0,557842.0,557669.0,557522.0,557335.0,557116.0,557055.0,556870.0,556765.0,556594.0,556521.0,556432.0,556303.0,556294.0,556205.0,556124.0,556069.0,556030.0,555960.0,555919.0,555921.0,555821.0,555858.0,555803.0,555771.0,555705.0,555756.0,555641.0,555575.0,555421.0,555224.0,554963.0,554614.0,554138.0,553765.0,553259.0,552805.0,552394.0,551946.0,551565.0,551242.0,550869.0,550634.0,550345.0,550134.0,549975.0,549738.0,549656.0,549525.0,549908.0,549405.0,549273.0,549211.0,550252.0,550034.0,550204.0,549899.0,547520.0,547402.0,547232.0,547126.0,548083.0,547978.0,547812.0,546794.0,547547.0,547434.0,547365.0,547282.0,547189.0,547138.0,547072.0,546983.0,547013.0,546933.0,546939.0,546837.0,546825.0,546823.0,546802.0,546773.0,546695.0,546770.0,546733.0,546731.0,546679.0,546577.0,546492.0,546337.0,546063.0,545742.0,545366.0,544954.0,544489.0,544103.0,543631.0,543178.0,542827.0,542481.0,542172.0,541843.0,541583.0,541369.0,541190.0,541019.0,540867.0,540748.0,540621.0,540496.0,540437.0,540372.0,540323.0,540157.0,540168.0,540039.0,539969.0,539846.0,539595.0,539521.0,539406.0,539268.0,539061.0,538962.0,538844.0,538747.0,538600.0,538483.0,538439.0,538328.0,538235.0,538149.0,538142.0,538039.0,537945.0,537911.0,537844.0,537760.0,537751.0,537721.0,537588.0,537576.0,537559.0,537519.0,537450.0,537373.0,537365.0,537326.0,537279.0,538143.0,538087.0,537858.0,537377.0,536724.0,535294.0,534284.0,536113.0,535245.0,533699.0,534355.0,532033.0,532986.0,532676.0,532403.0,532120.0,531881.0,531614.0,531430.0,531276.0,531102.0,530920.0,530854.0,530703.0,530622.0,530557.0,530459.0,530397.0,530256.0,530197.0,530123.0,530061.0,529956.0,529845.0,529700.0,529642.0,529466.0,529357.0,529332.0,529212.0,529091.0,529065.0,529008.0,528938.0,528882.0,528847.0,528798.0,528746.0,528728.0,528789.0,528763.0,528766.0,528782.0,528796.0,528712.0,528700.0,528733.0,528718.0,528736.0,528681.0,528664.0,528644.0,528686.0,528620.0,528623.0,528587.0,528630.0,528570.0,528602.0,528532.0,528546.0,528550.0,528515.0,528513.0,528551.0,528527.0,528490.0,528529.0,528507.0,528415.0,528480.0,528499.0,528480.0,528422.0,528218.0,528018.0,527663.0,527314.0,526866.0,526456.0,526033.0,525495.0,525103.0,524712.0,524387.0,524019.0,523715.0,523473.0,523220.0,523002.0,522815.0,522665.0,522488.0,522248.0,522170.0,522069.0,521985.0,522535.0,522238.0,522167.0,521298.0,520929.0,520966.0,520913.0,520917.0,520180.0,520062.0,519929.0,518498.0,520719.0,520648.0,520528.0,520498.0,520421.0,520384.0,520313.0,520310.0,520247.0,520261.0,520209.0,520183.0,520136.0,520216.0,520157.0,520155.0,520168.0,520111.0,520121.0,520085.0,520078.0,520054.0,520061.0,520061.0,520113.0,520016.0,520019.0,520002.0,520012.0,520014.0,519985.0,520013.0,520004.0,519979.0,519969.0,519965.0,519912.0,519933.0,519954.0,519912.0,519922.0,519928.0,519987.0,519935.0,519930.0,519904.0,519997.0,519948.0,519988.0,519970.0,520023.0,519988.0,520032.0,520065.0,520073.0,520045.0,520069.0,520063.0,520091.0,520131.0,520131.0,520070.0,520092.0,520144.0,520099.0,520049.0,519875.0,519599.0,519313.0,518913.0,518499.0,518020.0,517525.0,517062.0,516627.0,516163.0,515787.0,515396.0,515059.0,514858.0,513401.0,513580.0,513201.0,512866.0,514351.0,514154.0,513631.0,512474.0,513391.0,513292.0,513233.0,513198.0,513051.0,512925.0,512840.0,512712.0,512635.0,512504.0,512372.0,512220.0,512045.0,511976.0,511871.0,511766.0,511686.0,511601.0,511497.0,511475.0,511450.0,511388.0,511356.0,511368.0,511355.0,511374.0,511325.0,511340.0,511356.0,511390.0,511359.0,511336.0,511307.0,511383.0,511369.0,511395.0,511363.0,511372.0,511350.0,511421.0,511409.0,511442.0,511349.0,511418.0,511411.0,511491.0,511428.0,511477.0,511493.0,511511.0,511492.0,511559.0,511579.0,511560.0,511708.0,511635.0,511719.0,511699.0,511741.0,511775.0,511798.0,511839.0,511785.0,511875.0,511925.0,511919.0,511970.0,511990.0,512095.0,511999.0,511967.0,511859.0,511657.0,511362.0,510951.0,510604.0,510133.0,509695.0,509214.0,508856.0,508404.0,508064.0,507718.0,507483.0,507205.0,507080.0,506888.0,505481.0,505643.0,506948.0,506894.0,507068.0,506919.0,507043.0,506950.0,506521.0,506465.0,506524.0,506440.0,506457.0,506391.0,506319.0,506285.0,506234.0,506218.0,506118.0,506094.0,506052.0,506059.0,506031.0,506007.0,506039.0,506015.0,506054.0,506124.0,506152.0,506175.0,506208.0,506301.0,506350.0,506445.0,506509.0,506539.0,506641.0,506716.0,506816.0,506785.0,506827.0,506918.0,507000.0,507083.0,507127.0,507212.0,507225.0,507303.0,507421.0,507458.0,507482.0,507576.0,507612.0,507638.0,507742.0,507824.0,507865.0,507884.0,507968.0,508053.0,508119.0,508176.0,508260.0,508314.0,508420.0,508526.0,508638.0,508756.0,508779.0,508829.0,508734.0,508589.0,508371.0,508049.0,507738.0,507444.0,507024.0,506648.0,506302.0,505976.0,505637.0,505408.0,505261.0,505050.0,505017.0,504848.0,504758.0,504713.0,504685.0,504754.0,504827.0,504907.0,504927.0,505026.0,505024.0,505140.0,505135.0,505184.0,505190.0,504349.0,504431.0,504881.0,504258.0,505528.0,505501.0,505323.0,505589.0,505095.0,505193.0,504440.0,505320.0,505184.0,505280.0,505362.0,505392.0,505515.0,505622.0,505675.0,505769.0,505890.0,505971.0,506046.0,506170.0,506189.0,506295.0,506317.0,506431.0,506513.0,506675.0,506705.0,506753.0,506878.0,507013.0,507055.0,507145.0,507217.0,507282.0,507412.0,507520.0,507629.0,507773.0,507889.0,507962.0,508050.0,508168.0,508276.0,508323.0,508372.0,508472.0,508497.0,508590.0,508650.0,508695.0,508794.0,508828.0,508862.0,508904.0,508717.0,508543.0,508360.0,508028.0,507628.0,507219.0,506814.0,506413.0,505976.0,505737.0,505414.0,505218.0,504990.0,504780.0,504682.0,504640.0,504561.0,504485.0,504537.0,504597.0,504622.0,504780.0,504837.0,504897.0,505034.0,505091.0,505152.0,505141.0,505215.0,505171.0,505195.0,505197.0,505218.0,505179.0,505226.0,505202.0,505261.0,505231.0,505264.0,505520.0,505893.0,505890.0,505382.0,505328.0,505448.0,505548.0,505656.0,505626.0,505755.0,506181.0,506467.0,506620.0,506686.0,506802.0,506942.0,507105.0,507261.0,507357.0,507481.0,507553.0,507686.0,507805.0,507928.0,507997.0,508136.0,508228.0,508397.0,508419.0,508569.0,508659.0,508707.0,508778.0,508930.0,509060.0,509135.0,509201.0,509303.0,509411.0,509487.0,509596.0,509705.0,509779.0,509891.0,509975.0,510045.0,509973.0,509937.0,509763.0,509584.0,509349.0,509022.0,508687.0,508464.0,508067.0,507754.0,507527.0,507306.0,507092.0,506995.0,506822.0,506807.0,506782.0,506685.0,506698.0,506740.0,506832.0,506892.0,507036.0,507087.0,507198.0,507313.0,507385.0,507446.0,507484.0,507552.0,507618.0,507591.0,507555.0,507624.0,507682.0,507663.0,507677.0,507757.0,507723.0,507768.0,507858.0,507891.0,507952.0,508006.0,508086.0,508174.0,508241.0,508369.0,508519.0,508643.0,508690.0,508840.0,508912.0,507315.0,509319.0,509584.0,510691.0,510825.0,510883.0,510992.0,508471.0,508696.0,508731.0,508843.0,510131.0,510261.0,510112.0,509035.0,510516.0,510666.0,510613.0,510785.0,510863.0,510973.0,511063.0,511087.0,511155.0,511251.0,511341.0,511460.0,511561.0,511651.0,511798.0,511819.0,511902.0,512064.0,512121.0,512221.0,512309.0,512345.0,512354.0,512248.0,512123.0,511970.0,511720.0,511443.0,511135.0,510840.0,510567.0,510218.0,509990.0,509686.0,509534.0,509365.0,509251.0,509163.0,509083.0,509177.0,509193.0,509211.0,509291.0,509371.0,509405.0,509548.0,509611.0,509685.0,509777.0,509822.0,509944.0,509948.0,510034.0,510029.0,510094.0,510100.0,510102.0,510118.0,510116.0,510107.0,510151.0,510139.0,510212.0,510265.0,510334.0,510411.0,510444.0,510578.0,510657.0,510776.0,510819.0,511041.0,511065.0,511204.0,511345.0,511404.0,511480.0,511624.0,511702.0,511804.0,511884.0,512007.0,512108.0,512220.0,512288.0,511602.0,512421.0,512545.0,512613.0,512779.0,512824.0,513499.0,512921.0,513771.0,513087.0,514445.0,514431.0,513493.0,513607.0,513575.0,514096.0,513817.0,515094.0,513576.0,514030.0,514100.0,514079.0,514010.0,513840.0,513677.0,513479.0,513215.0,512908.0,512610.0,512452.0,512249.0,512030.0,511877.0,511757.0,511618.0,511647.0,511536.0,511541.0,511575.0,511628.0,511777.0,511779.0,511941.0,512027.0,512081.0,512176.0,512298.0,512380.0,512416.0,512462.0,512463.0,512507.0,512460.0,512493.0,512467.0,512399.0,512426.0,512446.0,512464.0,512459.0,512502.0,512488.0,512593.0,512614.0,512689.0,512782.0,512910.0,513014.0,513081.0,513150.0,513292.0,513345.0,513373.0,513477.0,513540.0,513622.0,513662.0,513754.0,513843.0,513903.0,514029.0,514077.0,514066.0,514218.0,514227.0,514223.0,514293.0,514400.0,514492.0,514521.0,514587.0,514664.0,514755.0,514827.0,514808.0,514930.0,515019.0,515136.0,515112.0,515221.0,515092.0,516172.0,513684.0,514002.0,513681.0,513480.0,514097.0,513808.0,513512.0,513144.0,512886.0,512630.0,512358.0,512171.0,512065.0,511918.0,511802.0,511738.0,511675.0,511698.0,511750.0,511807.0,511849.0,511917.0,511963.0,511999.0,512090.0,512164.0,512208.0,512217.0,512262.0,512231.0,512193.0,512245.0,512202.0,512173.0,512143.0,511912.0,512993.0,512178.0,512166.0,512164.0,512260.0,512244.0,512273.0,512348.0,512443.0,512487.0,512545.0,512719.0,512765.0,512794.0,512945.0,513021.0,513094.0,513142.0,513239.0,513281.0,513396.0,513446.0,513559.0,513584.0,513648.0,513776.0,513796.0,513808.0,513842.0,513963.0,514015.0,514096.0,514145.0,514206.0,514320.0,514363.0,514423.0,514488.0,514447.0,514554.0,514599.0,514615.0,514556.0,514368.0,514237.0,514022.0,513735.0,513377.0,513054.0,512865.0,512387.0,512170.0,511973.0,511738.0,511560.0,511440.0,511349.0,511261.0,511241.0,511187.0,511240.0,511239.0,511984.0,510472.0,511887.0,510613.0,510635.0,511627.0,511584.0,511671.0,511656.0,511650.0,511654.0,511709.0,511629.0,511590.0,511478.0,511575.0,511556.0,511538.0,511559.0,511567.0,511638.0,511621.0,511664.0,511789.0,511805.0,511899.0,511976.0,512046.0,512155.0,512265.0,512226.0,512369.0,512434.0,512498.0,512651.0,512669.0,512750.0,512780.0,513829.0,512613.0,513020.0,513036.0,513136.0,513167.0,513205.0,513274.0,513332.0,513440.0,513448.0,513531.0,513634.0,513663.0,513654.0,513727.0,513801.0,513837.0,513957.0,513905.0,513860.0,513783.0,513653.0,513417.0,513169.0,512864.0,512566.0,512229.0,511963.0,511684.0,511466.0,511228.0,511097.0,510912.0,510754.0,510714.0,510628.0,510601.0,510549.0,510634.0,510645.0,510676.0,510728.0,510783.0,510812.0,510933.0,510896.0,511004.0,510911.0,510951.0,510935.0,510926.0,510862.0,510808.0,510804.0,510821.0,510729.0,510743.0,510736.0,510761.0,510807.0,510819.0,511259.0,510981.0,511137.0,511245.0,511098.0,511172.0,511275.0,511323.0,511362.0,511425.0,511547.0,511601.0,511632.0,511678.0,511797.0,511806.0,511959.0,511966.0,512113.0,512119.0,512161.0,512224.0,512244.0,512315.0,512317.0,512422.0,512454.0,512500.0,512601.0,512615.0,512737.0,512735.0,513498.0,513362.0,513318.0,513500.0,511962.0,511886.0,511875.0,511830.0,511671.0,511344.0,512026.0,511716.0,511413.0,511047.0,510785.0,510522.0,510260.0,510082.0,509935.0,509845.0,509780.0,509673.0,509679.0,509648.0,509676.0,509780.0,509802.0,509833.0,509932.0,509993.0,510078.0,510066.0,510106.0,510110.0,510079.0,510077.0,510039.0,509992.0,510012.0,509999.0,510027.0,509977.0,510005.0,510078.0,510032.0,510087.0,510141.0,510187.0,510223.0,510262.0,510324.0,510429.0,510481.0,510578.0,510675.0,510711.0,510751.0,510815.0,510883.0,510933.0,510988.0,511050.0,511103.0,511176.0,511211.0,511269.0,511324.0,511356.0,510152.0,512718.0,510168.0,511335.0,511235.0,511645.0,510322.0,511642.0,511704.0,511785.0,511860.0,511801.0,511905.0,511914.0,511925.0,511805.0,511731.0,511499.0,511207.0,510851.0,510482.0,510139.0,509739.0,509363.0,508998.0,508749.0,508467.0,508215.0,508004.0,507861.0,507744.0,507625.0,507618.0,507550.0,507560.0,507570.0,507617.0,507662.0,507654.0,507798.0,507899.0,507715.0,508680.0,507746.0,507714.0,507634.0,507592.0,507616.0,507541.0,507523.0,507501.0,507515.0,507480.0,507490.0,507490.0,507525.0,507525.0,507516.0,507558.0,507595.0,507706.0,507788.0,507827.0,507893.0,507972.0,508042.0,508168.0,508203.0,508247.0,508277.0,508311.0,508457.0,508447.0,508510.0,508575.0,508651.0,508643.0,508733.0,508776.0,508826.0,508914.0,508921.0,508955.0,509016.0,509061.0,509066.0,509120.0,509121.0,509169.0,509260.0,509323.0,509334.0,509421.0,509457.0,509510.0,509589.0,509686.0,509659.0,509618.0,509527.0,509394.0,509225.0,508871.0,508494.0,508092.0,507650.0,507240.0,506834.0,506532.0,506218.0,505943.0,505682.0,505524.0,505349.0,505260.0,505170.0,505149.0,505025.0,505066.0,505096.0,505207.0,505196.0,505226.0,505299.0,505406.0,505382.0,506557.0,506532.0,506470.0,506410.0,504830.0,504915.0,505240.0,505486.0,505629.0,504978.0,505053.0,504834.0,504814.0,505220.0,505228.0,505289.0,505329.0,505401.0,505952.0,505478.0,505586.0,505723.0,505734.0,505829.0,505901.0,505939.0,506054.0,506110.0,506128.0,506214.0,506277.0,506267.0,506354.0,506375.0,506474.0,506436.0,506503.0,506598.0,506630.0,506621.0,506729.0,506775.0,506800.0,506847.0,506906.0,506922.0,507081.0,507163.0,507218.0,507268.0,507316.0,507405.0,507472.0,507527.0,507623.0,507728.0,507750.0,507793.0,507888.0,507896.0,508021.0,508047.0,508102.0,508184.0,508221.0,508235.0,508177.0,508032.0,507852.0,507557.0,507209.0,506891.0,506490.0,506139.0,505702.0,505437.0,505159.0,504932.0,504664.0,504529.0,504433.0,504336.0,504247.0,504265.0,504188.0,504154.0,504209.0,504209.0,504321.0,504416.0,504419.0,504466.0,504545.0,504531.0,504559.0,504583.0,504622.0,504563.0,504603.0,504574.0,504618.0,504596.0,504680.0,504705.0,503597.0,503930.0,503569.0,504090.0,505455.0,504857.0,505673.0,505377.0,505333.0,505397.0,505563.0,505684.0,505703.0,505866.0,505952.0,506238.0,506129.0,506211.0,506298.0,506340.0,506481.0,506516.0,506606.0,506660.0,506740.0,506751.0,506838.0,506906.0,507007.0,507055.0,507175.0,507187.0,507284.0,507309.0,507387.0,507444.0,507535.0,507612.0,507687.0,507772.0,507795.0,507885.0,507962.0,508095.0,508109.0,508233.0,508324.0,508476.0,508529.0,508630.0,508662.0,508836.0,508853.0,508834.0,508852.0,508735.0,508557.0,508298.0,508049.0,507650.0,507328.0,507036.0,506686.0,506375.0,506121.0,505876.0,505700.0,505507.0,505506.0,505457.0,505368.0,505446.0,505426.0,505499.0,505604.0,505622.0,505731.0,505904.0,505990.0,505998.0,506055.0,506133.0,506173.0,506215.0,506210.0,506212.0,506234.0,506255.0,506268.0,506213.0,506244.0,506281.0,506251.0,506320.0,506413.0,506223.0,505388.0,507081.0,505461.0,506858.0,506991.0,507689.0,507033.0,507350.0,507422.0,507538.0,508280.0,507714.0,507885.0,508017.0,508080.0,508192.0,508290.0,508392.0,508534.0,508606.0,508643.0,508798.0,508903.0,509010.0,509008.0,509139.0,509251.0,509264.0,509366.0,509434.0,509529.0,509612.0,509713.0,509874.0,509971.0,510052.0,510133.0,510246.0,510321.0,510399.0,510541.0,510628.0,510706.0,510811.0,510714.0,510644.0,510462.0,510282.0,509984.0,509732.0,509422.0,509065.0,508760.0,508490.0,508288.0,508073.0,507909.0,507740.0,507692.0,507596.0,507562.0,507563.0,507494.0,507596.0,507661.0,507694.0,507778.0,507895.0,507948.0,508010.0,508136.0,508194.0,508154.0,508231.0,508204.0,508202.0,508228.0,508233.0,508213.0,508229.0,508273.0,508301.0,508329.0,508362.0,508473.0,508497.0,508630.0,508684.0,508792.0,508793.0,508927.0,509078.0,509186.0,509277.0,509410.0,509479.0,509535.0,509705.0,508984.0,509016.0,509175.0,509076.0,509911.0,510003.0,510080.0,510178.0,510598.0,510679.0,510766.0,510971.0,510812.0,510990.0,511017.0,511152.0,511169.0,511253.0,511331.0,511386.0,511510.0,511568.0,511669.0,511743.0,511851.0,511943.0,512022.0,512090.0,512150.0,512270.0,512278.0,512428.0,512424.0,512484.0,512405.0,512219.0,512059.0,511823.0,511541.0,511319.0,510985.0,510702.0,510397.0,510169.0,509959.0,509785.0,509670.0,509565.0,509440.0,509437.0,509408.0,509403.0,509407.0,509477.0,509521.0,509640.0,509698.0,509790.0,509871.0,509914.0,509990.0,510020.0,510036.0,510114.0,510102.0,510164.0,510135.0,510185.0,510191.0,510225.0,510282.0,510352.0,510391.0,510406.0,510480.0,510533.0,510335.0,510561.0,511682.0,511760.0,509826.0,509930.0,510040.0,510238.0,511507.0,511544.0,511667.0,511734.0,511848.0,511917.0,512043.0,512122.0,512189.0,512328.0,512363.0,512433.0,512503.0,512583.0,512639.0,512719.0,512816.0,512863.0,512929.0,513011.0,513109.0,513160.0,513213.0,513311.0,513375.0,513455.0,513541.0,512250.0,512248.0,512356.0,512409.0,514409.0,514557.0,513886.0,514701.0,513933.0,514218.0,514822.0,513936.0,514562.0,513709.0,513942.0,513189.0,512910.0,512678.0,512400.0,512183.0,511970.0,511781.0,511686.0,511571.0,511492.0,511531.0,511490.0,511541.0,511562.0,511548.0,511671.0,511772.0,511872.0,511900.0,512027.0,512056.0,512172.0,512198.0,512254.0,512311.0,512338.0,512357.0,512336.0,512370.0,512376.0,512324.0,512395.0,512450.0,512497.0,512494.0,512539.0,512619.0,512666.0,512745.0,512865.0,512919.0,512992.0,513132.0,513147.0,513259.0,513351.0,513439.0,513575.0,513658.0,513737.0,513817.0,513865.0,513956.0,514057.0,514087.0,514232.0,514232.0,514332.0,514442.0,514546.0,514596.0,514619.0,514732.0,514863.0,514876.0,514960.0,515033.0,515059.0,515196.0,515299.0,515418.0,515533.0,515620.0,515873.0,515858.0,516020.0,516087.0,516242.0,516178.0,516296.0,516220.0,517204.0,514416.0,515535.0,513922.0,515043.0,514762.0,514640.0,514359.0,514181.0,514065.0,513952.0,513788.0,513776.0,513686.0,513659.0,513771.0,513796.0,513769.0,513879.0,513907.0,513952.0,513978.0,514068.0,514152.0,514154.0,514183.0,514222.0,514213.0,514252.0,514239.0,514219.0,514216.0,514178.0,514207.0,514192.0,514226.0,514283.0,514267.0,514302.0,514347.0,514368.0,514420.0,514500.0,514635.0,514656.0,514691.0,514763.0,514850.0,514913.0,514993.0,515041.0,515165.0,515174.0,515260.0,515334.0,515392.0,515472.0,515551.0,515598.0,515652.0,515755.0,515772.0\nArousal Valence Label: AMVM\n\n==================================================\n\nSubject ID: 7, Video ID: 1\nValence: 3\nArousal: 5\nGSR Data: 182172.671875,182666.59375,183080.09375,184354.546875,184697.40625,184221.859375,183468.21875,182109.46875,180501.46875,178840.546875,177298.078125,176210.34375,175583.6875,175138.1875,174994.609375,176081.46875,170977.109375,170643.640625,168521.328125,166966.5625,164121.546875,160664.15625,157288.796875,154527.703125,152405.125,151519.71875,152286.609375,151923.328125,154318.0625,158793.78125,166923.625,177891.15625,181941.25,184065.109375,185817.71875,186943.078125,187935.078125,188777.34375,189399.484375,189776.421875,189263.5,187841.8125,186088.421875,184679.390625,183782.890625,183261.125,183351.296875,183796.734375,184552.78125,185564.65625,186554.15625,187332.828125,188134.328125,188871.0625,177889.21875,175532.171875,172782.984375,172488.828125,172459.609375,172936.125,190661.046875,190057.203125,180669.0,180682.859375,187152.78125,191719.859375,194764.734375,195105.171875,195310.21875,195417.40625,195464.75,195308.71875,195312.515625,195309.109375,195367.234375,195474.203125,195581.40625,195717.328125,195734.578125,195916.296875,195996.8125,195969.671875,196045.234375,196042.5625,196043.546875,196086.65625,196170.0,196257.078125,196420.34375,196489.828125,196715.9375,196756.578125,196912.3125,197061.96875,197094.9375,197204.71875,197211.640625,197231.1875,197285.46875,197461.546875,197422.359375,197547.59375,197593.71875,197603.953125,197651.875,197707.1875,197808.859375,197883.609375,197955.75,198007.203125,198081.328125,198115.28125,198157.046875,198206.609375,198167.25,198228.171875,198142.25,198151.25,198125.0625,198244.515625,198343.0,198333.796875,198407.734375,198414.28125,198562.34375,198578.890625,198625.390625,198627.359375,198591.953125\nPPG Data: 551232.0,551091.0,550905.0,550746.0,550549.0,550368.0,550218.0,550018.0,549858.0,549670.0,549535.0,549427.0,549303.0,549231.0,549020.0,548862.0,548702.0,548578.0,548362.0,548022.0,547538.0,546964.0,546343.0,545537.0,544796.0,544090.0,543293.0,542529.0,541830.0,541218.0,540637.0,540171.0,539729.0,539288.0,538974.0,538649.0,538251.0,538108.0,537917.0,537787.0,537556.0,537413.0,537305.0,537179.0,536978.0,536814.0,536701.0,536514.0,536395.0,536145.0,535869.0,535631.0,535421.0,535175.0,534910.0,534726.0,534469.0,534275.0,534030.0,533858.0,533769.0,533662.0,533600.0,533532.0,533556.0,533475.0,533478.0,533488.0,533424.0,533370.0,533340.0,533352.0,533264.0,533308.0,533318.0,533163.0,533194.0,533152.0,533111.0,533032.0,533041.0,532955.0,532807.0,532532.0,532167.0,531750.0,531281.0,530758.0,530190.0,529594.0,529107.0,528571.0,528135.0,527761.0,527502.0,527234.0,527053.0,526849.0,526686.0,526580.0,526553.0,526596.0,526628.0,526665.0,526682.0,526761.0,526821.0,526897.0,526953.0,526931.0,526973.0,526902.0,526926.0,527008.0,526519.0,526656.0,526795.0,527254.0,527521.0,527832.0,528252.0,528592.0,528985.0,529374.0,529687.0,529948.0,530275.0,530513.0,530782.0,531016.0,531298.0,531522.0,531649.0,531833.0,531952.0,532148.0,532225.0,532221.0,532217.0,532100.0,531949.0,531759.0,531385.0,531047.0,530748.0,530457.0,530199.0,529825.0,529516.0,528952.0,528359.0,527792.0,527136.0,526388.0,525733.0,525066.0,524314.0,523657.0,523012.0,522457.0,521938.0,521558.0,521106.0,520773.0,520435.0,520161.0,519925.0,519708.0,519535.0,519391.0,519182.0,519030.0,518859.0,518705.0,518469.0,518323.0,518182.0,517932.0,517784.0,517603.0,517362.0,517220.0,517012.0,516938.0,516704.0,516613.0,516544.0,516416.0,516337.0,516304.0,516346.0,516281.0,516222.0,516216.0,516165.0,516171.0,516189.0,516156.0,516243.0,516193.0,516243.0,516270.0,516277.0,516306.0,516345.0,516375.0,516363.0,516420.0,516340.0,516331.0,516100.0,515950.0,515639.0,515504.0,513868.0,514399.0,514163.0,515191.0,513142.0,512722.0,512366.0,512110.0,511844.0,511715.0,511510.0,511399.0,511332.0,511300.0,511241.0,511283.0,511343.0,511464.0,511492.0,511532.0,511597.0,511629.0,511628.0,511627.0,511610.0,511600.0,511571.0,511442.0,511372.0,511277.0,511263.0,511216.0,511214.0,511122.0,511173.0,511162.0,511159.0,511152.0,511200.0,511215.0,511287.0,511385.0,511423.0,511490.0,511558.0,511593.0,511673.0,511801.0,511807.0,511883.0,511973.0,512051.0,512079.0,512165.0,512211.0,512206.0,512191.0,512116.0,511933.0,511638.0,511384.0,511091.0,510643.0,510300.0,509952.0,509648.0,509321.0,509052.0,508894.0,508709.0,508540.0,508541.0,508412.0,508418.0,508472.0,508576.0,508586.0,508724.0,508839.0,508949.0,509058.0,509150.0,509214.0,509294.0,509325.0,509334.0,509305.0,509208.0,509190.0,509212.0,509215.0,509184.0,509272.0,509178.0,509253.0,509278.0,509310.0,509406.0,508957.0,509153.0,509558.0,509250.0,510341.0,510034.0,509787.0,510419.0,510558.0,510713.0,510842.0,510944.0,511129.0,511243.0,511443.0,511565.0,511653.0,511749.0,511786.0,511696.0,511574.0,511336.0,511065.0,510775.0,510420.0,510023.0,509713.0,509439.0,509168.0,508963.0,508751.0,508694.0,508629.0,508620.0,508642.0,508703.0,508769.0,508821.0,509053.0,509148.0,509302.0,509478.0,509651.0,509854.0,509987.0,510064.0,510154.0,510301.0,510330.0,510406.0,510508.0,510590.0,510649.0,510747.0,510836.0,510928.0,510993.0,511154.0,511280.0,511438.0,511566.0,511694.0,511940.0,512077.0,512229.0,512404.0,512575.0,512727.0,512881.0,513109.0,513253.0,513402.0,513530.0,513740.0,513909.0,514108.0,514239.0,514454.0,514618.0,514872.0,515269.0,515735.0,516352.0,517086.0,517829.0,518756.0,519710.0,520606.0,521523.0,522413.0,523096.0,523828.0,524330.0,524832.0,525226.0,525509.0,525937.0,525559.0,525112.0,524223.0,524382.0,523994.0,522053.0,524026.0,522746.0,524665.0,523142.0,525124.0,523596.0,523162.0,522349.0,521505.0,520835.0,520302.0,519903.0,519694.0,519519.0,519381.0,519450.0,519650.0,519638.0,519977.0,520420.0,521162.0,522101.0,522827.0,523320.0,523546.0,523467.0,523434.0,523734.0,524219.0,524978.0,525864.0,526815.0,527856.0,529135.0,530432.0,531833.0,533326.0,534971.0,536709.0,538521.0,540401.0,542135.0,543623.0,544987.0,546181.0,547077.0,547873.0,549062.0,550699.0,552575.0,554269.0,555337.0,556151.0,556755.0,557147.0,557521.0,557949.0,558478.0,558963.0,559316.0,559508.0,559551.0,559565.0,559569.0,559534.0,559482.0,559602.0,559674.0,559898.0,560148.0,560462.0,560802.0,561268.0,561751.0,561792.0,561379.0,560788.0,560360.0,559874.0,559329.0,558821.0,558322.0,557933.0,557621.0,557398.0,557331.0,557388.0,557597.0,557713.0,557570.0,557364.0,557009.0,556871.0,556697.0,556664.0,556598.0,556702.0,556050.0,556642.0,556137.0,558013.0,558091.0,558216.0,557663.0,556744.0,559195.0,555807.0,559394.0,559917.0,560399.0,560947.0,561462.0,561946.0,562512.0,563038.0,563519.0,564023.0,564484.0,564884.0,565122.0,565336.0,565441.0,565605.0,565722.0,565841.0,565999.0,566190.0,566328.0,566611.0,566841.0,567087.0,567343.0,567551.0,568095.0,568920.0,569595.0,569511.0,569295.0,569017.0,568715.0,568254.0,567671.0,566954.0,566194.0,565484.0,564678.0,563943.0,563099.0,562266.0,561630.0,561130.0,560895.0,560798.0,560873.0,560887.0,561117.0,561372.0,561773.0,562319.0,562930.0,563792.0,564706.0,565694.0,566708.0,567728.0,568731.0,569553.0,570502.0,571341.0,572248.0,573102.0,574043.0,574978.0,575851.0,576799.0,577631.0,578526.0,579255.0,580133.0,580953.0,581818.0,582598.0,583203.0,583728.0,584236.0,584678.0,585163.0,585673.0,586070.0,586628.0,587086.0,587112.0,587497.0,587744.0,588007.0,588897.0,588563.0,588855.0,589067.0,590712.0,590784.0,590842.0,590920.0,589689.0,589706.0,589694.0,589687.0,589489.0,589801.0,589908.0,590037.0,590224.0,590480.0,590788.0,590949.0,590907.0,590840.0,590853.0,590872.0,590757.0,590682.0,590537.0,590335.0,590151.0,590028.0,589756.0,589329.0,588818.0,588045.0,587243.0,586354.0,585502.0,584558.0,583429.0,582222.0,580870.0,579558.0,578183.0,577059.0,576146.0,575422.0,574779.0,574116.0,573476.0,572813.0,572091.0,571427.0,570817.0,570305.0,569977.0,569701.0,569404.0,569072.0,568802.0,568502.0,568280.0,568065.0,567870.0,567734.0,567679.0,567695.0,567682.0,567649.0,567659.0,567607.0,567556.0,567352.0,567103.0,566828.0,566683.0,566757.0,567137.0,567522.0,568152.0,568803.0,569630.0,570370.0,571093.0,571927.0,572705.0,573403.0,574177.0,574860.0,575514.0,576186.0,576921.0,577626.0,578426.0,579279.0,580171.0,581853.0,581732.0,582589.0,583331.0,585094.0,585813.0,586608.0,586579.0,587479.0,588282.0,588907.0,589665.0,590257.0,590847.0,591305.0,591677.0,592179.0,592581.0,593174.0,593724.0,594258.0,594872.0,595531.0,596063.0,596644.0,597075.0,597386.0,597409.0,597323.0,597332.0,597236.0,597113.0,597086.0,596983.0,596905.0,596944.0,596943.0,597131.0,597301.0,597527.0,597896.0,598311.0,598702.0,599223.0,599684.0,600068.0,600387.0,600662.0,600930.0,601125.0,601371.0,601437.0,601562.0,601676.0,601871.0,602110.0,602365.0,602598.0,602925.0,603276.0,603687.0,604081.0,604548.0,605026.0,605551.0,606063.0,606650.0,607120.0,607650.0,608069.0,608291.0,608384.0,608394.0,608352.0,607849.0,607268.0,606659.0,606051.0,605394.0,604734.0,604073.0,603397.0,602608.0,601815.0,601050.0,600321.0,599565.0,598811.0,598203.0,597588.0,597057.0,596500.0,595963.0,595402.0,594926.0,594473.0,594004.0,593652.0,593362.0,593025.0,591546.0,592755.0,592682.0,591855.0,592245.0,591168.0,590898.0,590985.0,590554.0,590016.0,589584.0,588839.0,588004.0,587243.0,586501.0,585626.0,584827.0,584000.0,583269.0,582521.0,581810.0,581227.0,580595.0,580052.0,579562.0,579103.0,578655.0,578152.0,577799.0,577434.0,577045.0,576704.0,576305.0,575900.0,575575.0,575218.0,574837.0,574428.0,574024.0,573623.0,573238.0,572821.0,572393.0,571967.0,571664.0,571339.0,570911.0,570504.0,570244.0,569843.0,569515.0,569278.0,568962.0,568678.0,568394.0,568208.0,567972.0,567740.0,567544.0,567319.0,567109.0,566905.0,566732.0,566567.0,566349.0,566186.0,565998.0,565838.0,565681.0,565544.0,565380.0,565243.0,565113.0,564906.0,564780.0,564679.0,564522.0,564382.0,564259.0,564115.0,564023.0,563943.0,563792.0,563702.0,563571.0,563497.0,563419.0,563316.0,563245.0,563053.0,562821.0,562550.0,562145.0,561630.0,561141.0,560491.0,559869.0,559214.0,558569.0,557888.0,558916.0,556788.0,556321.0,556601.0,555273.0,555703.0,556054.0,555071.0,553508.0,554415.0,553007.0,553468.0,553781.0,553058.0,553167.0,553442.0,553040.0,553062.0,552749.0,551995.0,552614.0,551728.0,551218.0,551022.0,550723.0,550463.0,550259.0,550014.0,549786.0,549590.0,549483.0,549287.0,549142.0,549059.0,548897.0,548789.0,548743.0,548556.0,548528.0,548467.0,548384.0,548313.0,548216.0,548185.0,548126.0,548043.0,547902.0,547881.0,547800.0,547745.0,547690.0,547595.0,547600.0,547462.0,547422.0,547358.0,547342.0,547264.0,547184.0,547223.0,547094.0,547034.0,547064.0,547004.0,546999.0,547013.0,546943.0,546907.0,546827.0,546649.0,546427.0,546113.0,545649.0,545192.0,544631.0,544088.0,543434.0,542833.0,542306.0,541730.0,541351.0,540877.0,540531.0,540270.0,539951.0,539609.0,539449.0,539293.0,539102.0,538977.0,538865.0,538778.0,538714.0,538597.0,538527.0,538418.0,538353.0,538173.0,538064.0,537888.0,536180.0,537885.0,538284.0,537974.0,537691.0,536770.0,536734.0,536674.0,536518.0,536465.0,536413.0,536334.0,536301.0,536358.0,536347.0,536349.0,536251.0,536360.0,536395.0,536442.0,536465.0,536491.0,536504.0,536530.0,536533.0,536529.0,536613.0,536696.0,536713.0,536675.0,536721.0,536712.0,536706.0,536740.0,536681.0,536746.0,536754.0,536807.0,536835.0,536886.0,536805.0,536873.0,536923.0,537001.0,536914.0,536882.0,536749.0,536531.0,536186.0,535773.0,535343.0,534785.0,534265.0,533780.0,533199.0,532798.0,532323.0,531978.0,531752.0,531461.0,531259.0,531129.0,530986.0,530990.0,530959.0,530889.0,530953.0,531025.0,531128.0,531128.0,531267.0,531274.0,531243.0,531241.0,531236.0,531137.0,531106.0,531068.0,530943.0,530802.0,530757.0,530649.0,530545.0,530564.0,530538.0,530505.0,530526.0,530576.0,530593.0,530672.0,530783.0,530829.0,530908.0,530985.0,531068.0,531173.0,531209.0,531331.0,531390.0,531497.0,532580.0,531672.0,530719.0,530694.0,533039.0,531921.0,532054.0,532066.0,532147.0,532243.0,532251.0,532287.0,532357.0,532453.0,532510.0,532625.0,532569.0,532469.0,532298.0,532069.0,531681.0,531324.0,530859.0,530285.0,529742.0,529237.0,528732.0,528289.0,527937.0,527637.0,527348.0,527269.0,527099.0,526969.0,526886.0,526851.0,526801.0,526901.0,526898.0,526998.0,527008.0,527059.0,527029.0,527154.0,527100.0,527136.0,527033.0,527026.0,526923.0,526814.0,526753.0,526683.0,526664.0,526592.0,526472.0,526521.0,526535.0,526526.0,526576.0,526602.0,526737.0,526779.0,526842.0,526935.0,527044.0,527162.0,527292.0,527371.0,527444.0,527496.0,527695.0,527763.0,527819.0,527955.0,528025.0,528092.0,528201.0,528294.0,528302.0,528448.0,528526.0,528582.0,528644.0,528751.0,528836.0,528902.0,528946.0,529050.0,529077.0,529119.0,529064.0,528913.0,528602.0,528208.0,527819.0,527322.0,526823.0,526322.0,525798.0,525426.0,525006.0,524695.0,524488.0,524214.0,524100.0,523981.0,523944.0,523879.0,523879.0,523974.0,524038.0,524020.0,524176.0,524335.0,524406.0,524468.0,524554.0,524606.0,524572.0,524605.0,524569.0,524545.0,524521.0,524444.0,524509.0,524436.0,524448.0,524442.0,524489.0,524509.0,524606.0,524697.0,524792.0,524887.0,525025.0,525123.0,525286.0,525425.0,525555.0,525709.0,525786.0,525953.0,526011.0,526137.0,526222.0,526377.0,526478.0,526624.0,526716.0,526788.0,526885.0,527005.0,527099.0,527168.0,527279.0,527330.0,527465.0,527575.0,527696.0,527845.0,527916.0,528013.0,528083.0,528142.0,528042.0,527863.0,527645.0,527324.0,526936.0,526441.0,525958.0,525510.0,525109.0,524722.0,524409.0,524119.0,523978.0,523761.0,523717.0,523716.0,523764.0,523695.0,523795.0,523890.0,523997.0,524169.0,524288.0,524420.0,524720.0,525072.0,525594.0,526125.0,526750.0,527353.0,527882.0,528452.0,529147.0,529777.0,530467.0,531179.0,531856.0,532468.0,532864.0,533005.0,533430.0,533627.0,533899.0,533868.0,533276.0,533096.0,533145.0,532607.0,531955.0,531286.0,531174.0,531657.0,532483.0,533518.0,534079.0,534112.0,534199.0,534393.0,534844.0,535635.0,536503.0,537334.0,538141.0,538891.0,539668.0,540341.0,540971.0,541383.0,541676.0,541990.0,542522.0,543211.0,544018.0,544962.0,545881.0,546734.0,547550.0,548380.0,548871.0,549223.0,549534.0,549927.0,550496.0,551264.0,552033.0,552862.0,553474.0,554153.0,554889.0,555719.0,556607.0,557498.0,558430.0,559251.0,560109.0,561024.0,561796.0,562547.0,563362.0,563927.0,564402.0,564655.0,564750.0,564848.0,564812.0,564823.0,564985.0,565331.0,565669.0,565454.0,564973.0,564732.0,564702.0,564718.0,564648.0,564578.0,564650.0,564865.0,565207.0,565491.0,565873.0,566073.0,566278.0,566416.0,566537.0,566767.0,566863.0,567188.0,567647.0,568036.0,568382.0,568549.0,568628.0,568685.0,568764.0,568859.0,568885.0,568874.0,568932.0,569081.0,569182.0,569405.0,569792.0,570200.0,570646.0,571169.0,571588.0,572045.0,572647.0,573096.0,573550.0,573754.0,573906.0,573843.0,573771.0,573628.0,573493.0,573409.0,573464.0,573336.0,573111.0,572859.0,573942.0,573852.0,572427.0,572237.0,572089.0,571903.0,571714.0,571408.0,571001.0,570506.0,570000.0,569431.0,568850.0,568160.0,567497.0,566637.0,565782.0,564817.0,563945.0,563082.0,562042.0,560976.0,559839.0,558778.0,557705.0,556717.0,555784.0,554841.0,553986.0,553127.0,552414.0,551843.0,551225.0,550585.0,550019.0,549474.0,548996.0,548695.0,548514.0,548416.0,548339.0,548221.0,548075.0,547977.0,547893.0,547877.0,547929.0,548033.0,548056.0,548239.0,548430.0,548677.0,548817.0,549134.0,549339.0,549502.0,549712.0,549941.0,550259.0,550545.0,550770.0,551010.0,551276.0,551584.0,551902.0,552239.0,552500.0,552958.0,553336.0,553815.0,554236.0,554438.0,554618.0,554774.0,554092.0,554865.0,554845.0,554724.0,554374.0,553992.0,553448.0,553169.0,553087.0,553172.0,553291.0,553416.0,553426.0,553365.0,553413.0,553534.0,553774.0,553935.0,554178.0,554400.0,554624.0,554784.0,554930.0,555086.0,555220.0,555255.0,555379.0,555546.0,555764.0,555954.0,556200.0,556549.0,556946.0,557452.0,557940.0,558533.0,559081.0,559735.0,560437.0,561190.0,562017.0,562837.0,563747.0,564688.0,565674.0,566727.0,567715.0,568717.0,569695.0,570728.0,571650.0,572594.0,573508.0,574400.0,575351.0,576322.0,577358.0,578384.0,579305.0,580291.0,581318.0,582207.0,583029.0,583857.0,584650.0,585336.0,586004.0,586443.0,586748.0,586802.0,586727.0,586366.0,585731.0,585047.0,584080.0,582986.0,581853.0,580525.0,579012.0,577508.0,575976.0,574555.0,573231.0,572012.0,570886.0,569870.0,568982.0,568042.0,567210.0,566401.0,565737.0,565067.0,564422.0,563905.0,563455.0,563064.0,562699.0,562369.0,562059.0,561741.0,561409.0,561039.0,560721.0,560354.0,560028.0,559667.0,559330.0,558931.0,558589.0,558153.0,557806.0,557498.0,557162.0,556802.0,556594.0,556274.0,556028.0,555848.0,555583.0,555337.0,555165.0,555003.0,554791.0,554556.0,554473.0,553854.0,555267.0,555207.0,555052.0,552873.0,552378.0,553268.0,553125.0,553018.0,552817.0,552664.0,552566.0,552342.0,552191.0,552027.0,551893.0,551712.0,551542.0,551478.0,551321.0,551186.0,551057.0,550993.0,550852.0,550797.0,550697.0,550574.0,550485.0,550289.0,550104.0,549616.0,549147.0,548569.0,547909.0,547261.0,546664.0,546002.0,545380.0,544772.0,544255.0,543801.0,543403.0,543074.0,542781.0,542456.0,542241.0,541945.0,541726.0,541582.0,541402.0,541292.0,541112.0,540981.0,540827.0,540667.0,540500.0,540312.0,540157.0,539984.0,539778.0,539566.0,539417.0,539176.0,538972.0,538772.0,538606.0,538467.0,538333.0,538262.0,538113.0,538037.0,537935.0,537884.0,537864.0,537770.0,537759.0,537713.0,537680.0,537654.0,537591.0,537570.0,537517.0,537529.0,537481.0,537459.0,537418.0,537362.0,537348.0,537350.0,537312.0,537280.0,537280.0,537193.0,537218.0,537151.0,537156.0,537105.0,537107.0,537106.0,537765.0,537574.0,537798.0,537842.0,535910.0,535841.0,535993.0,535081.0,537291.0,537230.0,537201.0,537132.0,536953.0,536629.0,536230.0,535718.0,535299.0,534715.0,534131.0,533658.0,533055.0,532642.0,532271.0,531915.0,531602.0,531373.0,531212.0,530992.0,530869.0,530813.0,530653.0,530578.0,530513.0,530575.0,530572.0,530557.0,530587.0,530488.0,530415.0,530408.0,530300.0,530248.0,530165.0,530067.0,529958.0,529819.0,529806.0,529674.0,529712.0,529653.0,529640.0,529645.0,529700.0,529700.0,529755.0,529810.0,529861.0,529919.0,529986.0,530091.0,530170.0,530255.0,530281.0,530326.0,530426.0,530467.0,530511.0,530589.0,530640.0,530676.0,530763.0,530822.0,530870.0,530931.0,530976.0,530995.0,531054.0,531055.0,531175.0,531176.0,531179.0,531265.0,531286.0,531397.0,531444.0,531503.0,531608.0,531658.0,531683.0,531603.0,531522.0,531329.0,530932.0,530583.0,530055.0,529598.0,529113.0,528628.0,528206.0,527749.0,527435.0,527126.0,526889.0,526696.0,527210.0,526397.0,527239.0,526442.0,524419.0,526035.0,526016.0,526062.0,526024.0,526079.0,526122.0,526091.0,526082.0,526066.0,526072.0,526042.0,526024.0,525979.0,525894.0,525857.0,525791.0,525825.0,525759.0,525738.0,525799.0,525879.0,525988.0,525979.0,526056.0,526155.0,526269.0,526367.0,526478.0,526554.0,526662.0,526701.0,526872.0,526945.0,527043.0,527134.0,527269.0,527334.0,527332.0,527465.0,527554.0,527690.0,527698.0,527844.0,527900.0,528051.0,528136.0,528203.0,528287.0,528345.0,528403.0,528539.0,528651.0,528708.0,528805.0,528925.0,529021.0,529082.0,529214.0,529293.0,529450.0,529517.0,529593.0,529743.0,529679.0,529557.0,529332.0,529056.0,528737.0,528286.0,527810.0,527380.0,526904.0,526494.0,526129.0,525796.0,525519.0,525332.0,525143.0,525097.0,524929.0,524906.0,525396.0,525522.0,525526.0,525296.0,525990.0,525910.0,525961.0,526243.0,524421.0,524769.0,524585.0,524114.0,525568.0,525549.0,525613.0,525601.0,525578.0,525626.0,525628.0,525678.0,525752.0,525742.0,525851.0,525963.0,526053.0,526189.0,526255.0,526518.0,526543.0,526694.0,526892.0,526954.0,527087.0,527217.0,527374.0,527522.0,527619.0,527746.0,527851.0,527916.0,528099.0,528209.0,528272.0,528402.0,528541.0,528649.0,528791.0,528882.0,528980.0,529082.0,529178.0,529328.0,529447.0,529574.0,529673.0,529858.0,529897.0,530057.0,530170.0,530353.0,530483.0,530646.0,530761.0,530947.0,530980.0,531108.0,531168.0,531074.0,530875.0,530631.0,530328.0,529913.0,529476.0,529069.0,528597.0,528292.0,527917.0,527666.0,527533.0,527268.0,527161.0,527051.0,527017.0,527042.0,527013.0,526978.0,527085.0,527191.0,527204.0,527360.0,527382.0,527536.0,527614.0,527676.0,527747.0,527834.0,527891.0,527897.0,527883.0,528710.0,528913.0,528280.0,528347.0,528692.0,526783.0,526941.0,528328.0,529251.0,529412.0,529480.0,529804.0,529196.0,529752.0,529360.0,529730.0,529517.0,529143.0,529556.0,529746.0,529651.0,530396.0,530063.0,530686.0,530837.0,531000.0,531142.0,531235.0,531462.0,531544.0,531729.0,531879.0,531990.0,532108.0,532254.0,532391.0,532544.0,532679.0,532857.0,533003.0,533111.0,533229.0,533345.0,533509.0,533669.0,533794.0,533968.0,534105.0,534244.0,534352.0,534453.0,534411.0,534227.0,534018.0,533750.0,533470.0,533102.0,532704.0,532371.0,532048.0,531697.0,531531.0,531359.0,531185.0,531127.0,531025.0,530982.0,531046.0,531038.0,531158.0,531245.0,531426.0,531459.0,531633.0,531808.0,531890.0,532018.0,532168.0,532228.0,532306.0,532372.0,532535.0,532478.0,532537.0,532615.0,532693.0,532742.0,532856.0,532968.0,533034.0,533139.0,533266.0,533426.0,533541.0,533736.0,533903.0,534169.0,534197.0,534469.0,534628.0,534910.0,535149.0,535154.0,535441.0,535894.0,536117.0,535883.0,536434.0,536410.0,536047.0,536173.0,536118.0,536760.0,536874.0,536972.0,537187.0,537313.0,537418.0,537593.0,537722.0,537933.0,538032.0,538157.0,538290.0,538480.0,538626.0,538853.0,539015.0,539166.0,539310.0,539439.0,539525.0,539513.0,539475.0,539294.0,539074.0,538744.0,538432.0,538068.0,537729.0,537430.0,537143.0,536835.0,536643.0,536469.0,536377.0,536322.0,536325.0,536318.0,536276.0,536373.0,536409.0,536552.0,536663.0,536813.0,536861.0,537024.0,537090.0,537207.0,537318.0,537356.0,537352.0,537356.0,537421.0,537419.0,537498.0,537482.0,537524.0,537577.0,537637.0,537729.0,537841.0,537882.0,537974.0,538042.0,538184.0,538386.0,538561.0,538670.0,538833.0,538960.0,539115.0,539246.0,539507.0,539569.0,539723.0,539954.0,539961.0,540124.0,540276.0,540369.0,540557.0,540697.0,540775.0,540939.0,540865.0,540950.0,541067.0,541239.0,541775.0,542053.0,542205.0,542091.0,542057.0,542235.0,542450.0,542576.0,542802.0,543028.0,543217.0,543398.0,543518.0,543625.0,543559.0,543522.0,543306.0,543022.0,542884.0,542669.0,542401.0,542147.0,541990.0,541846.0,541758.0,541689.0,541632.0,541638.0,541567.0,541555.0,541634.0,541623.0,541808.0,541941.0,542203.0,542491.0,542764.0,543027.0,543262.0,543462.0,543634.0,543821.0,543917.0,544088.0,544180.0,544253.0,544324.0,544410.0,544468.0,544532.0,544605.0,544688.0,544793.0,544790.0,544899.0,545025.0,545134.0,545197.0,545254.0,545308.0,545411.0,545491.0,545572.0,545675.0,545730.0,545783.0,545826.0,545848.0,545886.0,545942.0,545972.0,546016.0,545997.0,545999.0,546042.0,546071.0,546126.0,546160.0,546161.0,546196.0,546222.0,546208.0,546302.0,546392.0,546457.0,546479.0,546545.0,546634.0,546710.0,546682.0,546753.0,546754.0,546762.0,546601.0,546353.0,546407.0,546045.0,545602.0,545143.0,544780.0,543847.0,543922.0,543126.0,543238.0,542910.0,542799.0,542685.0,542598.0,542317.0,542241.0,542198.0,542214.0,542361.0,542417.0,542475.0,542584.0,542599.0,542704.0,542748.0,542787.0,542855.0,542882.0,542813.0,542810.0,542767.0,542848.0,542797.0,542777.0,542813.0,542813.0,542865.0,542973.0,542992.0,543054.0,543186.0,543201.0,543392.0,543514.0,543531.0,543662.0,543834.0,543903.0,544041.0,544194.0,544245.0,544377.0,544486.0,544610.0,544748.0,544802.0,544959.0,545019.0,545138.0,545227.0,545345.0,545485.0,545573.0,545616.0,545700.0,545790.0,545915.0,546024.0,546120.0,546260.0,546412.0,546516.0,546624.0,546726.0,546859.0,546880.0,546848.0,546753.0,546568.0,546213.0,545939.0,545558.0,545198.0,544786.0,544505.0,544152.0,543852.0,543596.0,543435.0,543266.0,543175.0,543162.0,543177.0,543136.0,543216.0,543270.0,543392.0,543416.0,543664.0,543758.0,543839.0,544481.0,544441.0,544398.0,544637.0,544087.0,544057.0,544076.0,543971.0,544981.0,545176.0,545353.0,545528.0,544346.0,544433.0,544488.0,544610.0,544674.0,544861.0,544952.0,545113.0,545209.0,545343.0,545505.0,545616.0,545763.0,545923.0,546049.0,546205.0,546269.0,546426.0,546472.0,546675.0,546757.0,546896.0,547081.0,547144.0,547253.0,547360.0,547484.0,547564.0,547747.0,547879.0,547990.0,548100.0,548230.0,548333.0,548450.0,548500.0,548351.0,548186.0,547914.0,547581.0,547286.0,546847.0,546441.0,546083.0,545699.0,545346.0,545079.0,544813.0,544680.0,544455.0,544401.0,544327.0,544282.0,544286.0,544323.0,544357.0,544416.0,544545.0,544651.0,544730.0,544756.0,544799.0,544901.0,544908.0,544885.0,544898.0,544910.0,544789.0,544850.0,544798.0,544773.0,544789.0,544825.0,544859.0,544862.0,544968.0,545053.0,545161.0,545266.0,545381.0,545553.0,545598.0,545727.0,545748.0,545829.0,545966.0,545996.0,546627.0,546452.0,546468.0,546766.0,547181.0,547637.0,548161.0,548418.0,547230.0,547283.0,547420.0,547537.0,547659.0,547766.0,547886.0,547898.0,548085.0,548186.0,548258.0,548414.0,548463.0,548547.0,548598.0,548495.0,548247.0,547961.0,547616.0,547223.0,546812.0,546400.0,545942.0,545578.0,545294.0,544999.0,544821.0,544603.0,544490.0,544432.0,544438.0,544407.0,544414.0,544390.0,544498.0,544610.0,544712.0,544811.0,544910.0,545007.0,545055.0,545117.0,545177.0,545230.0,545298.0,545262.0,545274.0,545242.0,545244.0,545265.0,545309.0,545401.0,545453.0,545528.0,545697.0,545723.0,545895.0,546012.0,546116.0,546312.0,546385.0,546542.0,546694.0,546886.0,547056.0,547166.0,547312.0,547492.0,547618.0,547736.0,547858.0,547973.0,548087.0,548228.0,548324.0,548475.0,548574.0,548708.0,548819.0,548921.0,548990.0,549193.0,549339.0,549476.0,549574.0,549735.0,549847.0,550200.0,550900.0,550294.0,550949.0,550071.0,549830.0,549390.0,549131.0,547288.0,547817.0,546558.0,546898.0,547186.0,546984.0,546734.0,546599.0,546546.0,546473.0,546421.0,546483.0,546439.0,546507.0,546618.0,546706.0,546850.0,547031.0,547047.0,547225.0,547317.0,547376.0,547440.0,547518.0,547530.0,547567.0,547569.0,547612.0,547635.0,547676.0,547762.0,547843.0,547522.0,547497.0,548109.0,548211.0,548334.0,548462.0,548617.0,548812.0,548973.0,549099.0,549273.0,549420.0,549524.0,549736.0,549858.0,549962.0,550156.0,550318.0,550441.0,550621.0,550724.0,550885.0,551057.0,551120.0,551243.0,551419.0,551539.0,551709.0,551848.0,551945.0,552146.0,552304.0,552411.0,552592.0,552734.0,552893.0,553032.0,553133.0,553151.0,553139.0,552857.0,552716.0,552507.0,552163.0,551908.0,551552.0,551261.0,550973.0,550720.0,550529.0,550381.0,550323.0,550232.0,550223.0,550224.0,550239.0,550266.0,550407.0,550846.0,550620.0,552038.0,552159.0,552237.0,552287.0,550564.0,551568.0,550944.0,551271.0,551499.0,551511.0,551536.0,551581.0,551673.0,551724.0,551788.0,551871.0,551911.0,552053.0,552227.0,552290.0,552420.0,552559.0,552710.0,552866.0,553066.0,553174.0,553308.0,553474.0,553672.0,553838.0,553969.0,554091.0,554274.0,554374.0,554497.0,554637.0,554725.0,554876.0,555032.0,554790.0,555721.0,555084.0,557175.0,555623.0,555737.0,555922.0,556039.0,556170.0,556350.0,556463.0,556632.0,556756.0,556839.0,556943.0,556949.0,556888.0,556753.0,556627.0,556373.0,556057.0,555748.0,555530.0,555266.0,554967.0,554804.0,554608.0,554472.0,554357.0,554245.0,554289.0,554298.0,554310.0,554362.0,554429.0,554545.0,554609.0,554723.0,554826.0,554946.0,555016.0,555175.0,555211.0,555308.0,555288.0,555395.0,555383.0,555439.0,555456.0,555459.0,555529.0,555616.0,555705.0,555768.0,555848.0,555986.0,556107.0,556208.0,556326.0,557589.0,557841.0,557973.0,557059.0,558357.0,556376.0,556965.0,556836.0,556726.0,557667.0,557784.0,557925.0,558033.0,558135.0,558272.0,558386.0,558502.0,558624.0,558709.0,558865.0,558947.0,559112.0,559264.0,559358.0,559451.0,559624.0,559648.0,559851.0,559974.0,560062.0,560133.0,560279.0,560363.0,560409.0,560427.0,560320.0,560167.0,559954.0,559685.0,559396.0,559609.0,559226.0,558847.0,558591.0,559159.0,557375.0,557281.0,557377.0,557255.0,557182.0,557149.0,557165.0,557186.0,557204.0,557303.0,557342.0,557486.0,557516.0,557565.0,557617.0,557628.0,557793.0,557792.0,557796.0,557876.0,557795.0,557790.0,557881.0,557873.0,557857.0,557873.0,557938.0,558057.0,558114.0,558158.0,558246.0,558367.0,558429.0,558529.0,558588.0,558703.0,558785.0,558900.0,558997.0,559061.0,559199.0,559260.0,559401.0,559543.0,559682.0,559770.0,559838.0,559932.0,560041.0,560166.0,560254.0,560295.0,560402.0,560480.0,560615.0,561781.0,559859.0,559463.0,560698.0,560596.0,560767.0,560694.0,561341.0,561378.0,561535.0,561657.0,561737.0,561866.0,561943.0,561970.0,561896.0,561870.0,561748.0,561529.0,561271.0,560941.0,560571.0,560187.0,559834.0,559475.0,559141.0,558818.0,558595.0,558386.0,558208.0,558064.0,558013.0,557937.0,557864.0,557823.0,557857.0,557857.0,558803.0,557962.0,557999.0,558779.0,557563.0,558236.0,557644.0,557936.0,557721.0,556753.0,559124.0,557892.0,557854.0,557851.0,557807.0,557874.0,557879.0,557881.0,557909.0,557966.0,557979.0,558023.0,558072.0,558190.0,558270.0,558319.0,558392.0,558563.0,558598.0,558699.0,558739.0,558887.0,558921.0,559037.0,559108.0,559144.0,559264.0,559362.0,559443.0,559502.0,559597.0,559681.0,559824.0,559900.0,559979.0,560076.0,560157.0,560176.0,560310.0,560397.0,560511.0,560574.0,560703.0,560823.0,560901.0,561058.0,561165.0,561261.0,561300.0,561387.0,561462.0,561512.0,561481.0,561828.0,560970.0,560602.0,560986.0,560087.0,559801.0,559414.0,558940.0,558689.0,558321.0,558119.0,557917.0,557690.0,557577.0,557516.0,557407.0,557420.0,557349.0,557380.0,557382.0,557457.0,557519.0,557612.0,557594.0,557619.0,557618.0,557636.0,557605.0,557535.0,557522.0,557455.0,557428.0,557394.0,557343.0,557265.0,557064.0,557059.0,557035.0,557094.0,556137.0,556395.0,556612.0,558456.0,556704.0,556910.0,556939.0,556930.0,557887.0,557962.0,557443.0,558144.0,558274.0,558317.0,558481.0,558466.0,558495.0,558636.0,558741.0,558759.0,558884.0,558957.0,558953.0,559075.0,559168.0,559163.0,559245.0,559349.0,559417.0,559471.0,559607.0,559624.0,559772.0,559789.0,559900.0,559967.0,560056.0,560105.0,560084.0,560014.0,559879.0,559711.0,559379.0,559105.0,558689.0,558334.0,557918.0,557546.0,557188.0,556906.0,556596.0,556388.0,556165.0,556026.0,555913.0,555847.0,555790.0,555763.0\nArousal Valence Label: AMVL\n\n==================================================\n\nSubject ID: 7, Video ID: 2\nValence: 4\nArousal: 5\nGSR Data: 168857.21875,168781.234375,169132.53125,169823.0,170710.34375,171388.953125,172187.515625,172716.078125,173357.40625,173904.28125,174323.5625,174718.265625,175014.71875,175309.296875,175530.109375,175777.78125,175984.109375,176195.375,176381.4375,176576.296875,176749.59375,176954.421875,177131.3125,177294.34375,177346.453125,177531.609375,177766.203125,177894.71875,177950.171875,178103.15625,178181.046875,178255.0,178363.609375,178503.625,178543.53125,178680.40625,178786.609375,178833.0625,178843.828125,179016.65625,179132.59375,179219.21875,179301.3125,179397.328125,179474.671875,179588.25,179678.40625,179676.828125,179090.765625,177644.015625,175528.703125,173140.734375,171016.78125,169359.78125,168094.109375,167512.359375,167284.734375,167543.796875,168265.234375,169093.046875,169971.828125,170824.0625,171611.28125,172348.578125,172893.34375,173391.296875,173790.359375,174225.703125,174506.8125,174826.515625,175100.28125,175360.75,175597.671875,175794.1875,176006.546875,176196.96875,176361.34375,176525.625,176728.859375,176877.265625,177051.09375,177108.890625,177245.3125,177345.75,177275.859375,176441.875,175111.859375,173636.34375,172269.125,171142.0,170401.078125,169992.875,169993.890625,170285.96875,170852.0,171463.015625,172214.46875,172766.828125,173270.65625,173752.75,173954.890625,173402.5625,171716.21875,169250.234375,166271.671875,163741.578125,161828.671875,160911.34375,160645.578125,160936.40625,161622.578125,162663.734375,163788.28125,164938.421875,165878.953125,166861.53125,167621.125,168391.609375,169140.03125,169697.65625,170152.859375,170501.0625,170859.03125,171183.90625,171558.15625,171780.59375,172048.140625,172277.703125,172467.484375\nPPG Data: 605678.0,605853.0,606119.0,606351.0,606579.0,606769.0,607007.0,607253.0,607461.0,607620.0,607842.0,608037.0,608306.0,608448.0,608667.0,608855.0,609069.0,609291.0,609425.0,609642.0,609932.0,610125.0,610256.0,610579.0,610784.0,611056.0,611287.0,611514.0,611654.0,611746.0,611636.0,611449.0,611136.0,610584.0,610026.0,609417.0,608837.0,608262.0,607743.0,607359.0,607022.0,606744.0,606540.0,606445.0,606329.0,606268.0,606358.0,606327.0,606375.0,606543.0,606663.0,606834.0,606958.0,607096.0,607212.0,607329.0,607455.0,607436.0,607453.0,607463.0,607457.0,607437.0,607499.0,607558.0,607631.0,607699.0,607762.0,607910.0,608074.0,608214.0,608341.0,608603.0,608765.0,608952.0,609168.0,609381.0,609607.0,609818.0,610038.0,610252.0,610372.0,610683.0,610914.0,611065.0,611281.0,611536.0,611779.0,611920.0,612183.0,612342.0,612593.0,612763.0,612978.0,613172.0,613388.0,613648.0,613747.0,613946.0,614198.0,614475.0,614708.0,614886.0,615178.0,615468.0,615711.0,615913.0,616041.0,616133.0,616019.0,615753.0,615466.0,614929.0,614316.0,613761.0,613129.0,612544.0,612025.0,611578.0,611249.0,610910.0,610662.0,610522.0,610372.0,610222.0,610175.0,610137.0,610105.0,610129.0,610244.0,610315.0,610416.0,610566.0,610600.0,610745.0,610873.0,610850.0,610896.0,610871.0,610893.0,610936.0,610927.0,610926.0,610949.0,610983.0,611118.0,611243.0,611407.0,611405.0,611604.0,611728.0,611926.0,612116.0,612219.0,612454.0,612651.0,612857.0,613012.0,613192.0,613319.0,613557.0,613736.0,613923.0,614125.0,614315.0,614503.0,614727.0,614905.0,615061.0,615266.0,615456.0,615654.0,615816.0,615935.0,616184.0,616389.0,616514.0,616704.0,616886.0,617140.0,617359.0,617552.0,617776.0,618015.0,618201.0,618433.0,618609.0,618839.0,619035.0,619254.0,619448.0,619730.0,619963.0,620163.0,620379.0,620623.0,620817.0,621010.0,621020.0,620935.0,620653.0,620218.0,619689.0,619063.0,618376.0,617645.0,616926.0,616308.0,615754.0,615323.0,614949.0,614529.0,614285.0,614029.0,613725.0,613539.0,613362.0,613153.0,613052.0,612983.0,612902.0,612858.0,612804.0,612874.0,612814.0,612808.0,612719.0,612673.0,612566.0,612468.0,612408.0,612352.0,612284.0,612223.0,612168.0,612153.0,612141.0,612181.0,612126.0,612292.0,612237.0,612346.0,612436.0,612460.0,612595.0,612718.0,612837.0,612987.0,613023.0,613155.0,613336.0,613451.0,613544.0,613708.0,613839.0,613924.0,613997.0,614152.0,614264.0,614356.0,614463.0,614640.0,614747.0,614902.0,615015.0,615123.0,615214.0,615432.0,615516.0,615662.0,615822.0,615919.0,616069.0,616234.0,616432.0,616541.0,616661.0,616827.0,617006.0,617084.0,617281.0,617474.0,617615.0,617779.0,617958.0,618100.0,618333.0,618528.0,618755.0,618901.0,618976.0,618943.0,618740.0,618420.0,617868.0,617254.0,616501.0,615715.0,614910.0,614135.0,613452.0,612807.0,612263.0,611794.0,611378.0,611058.0,610774.0,610415.0,610119.0,609907.0,609715.0,609540.0,609359.0,609285.0,609243.0,609160.0,609056.0,609035.0,608944.0,608916.0,608773.0,608668.0,608559.0,608397.0,608280.0,608188.0,608074.0,607919.0,607913.0,607885.0,607861.0,607913.0,607897.0,607912.0,607971.0,608105.0,608156.0,608303.0,608343.0,608448.0,608623.0,608658.0,608755.0,608978.0,609085.0,609172.0,609342.0,609462.0,609610.0,609660.0,609849.0,609982.0,610074.0,610267.0,610419.0,610575.0,610642.0,610884.0,610978.0,611119.0,611294.0,611479.0,611706.0,611838.0,612043.0,612199.0,612339.0,612540.0,612743.0,612877.0,613080.0,613261.0,613462.0,613682.0,613825.0,614071.0,614261.0,614404.0,614605.0,614777.0,615030.0,615251.0,615451.0,615628.0,615893.0,616085.0,616199.0,616289.0,616272.0,615993.0,615596.0,615032.0,614312.0,613610.0,612792.0,611982.0,611219.0,610591.0,610004.0,609495.0,608982.0,608572.0,608279.0,608059.0,607749.0,607553.0,607399.0,607235.0,607228.0,607142.0,607110.0,607085.0,607098.0,607069.0,607007.0,607036.0,606973.0,606927.0,606792.0,606736.0,606646.0,606483.0,606330.0,606305.0,606265.0,606264.0,606240.0,606264.0,606309.0,606348.0,606448.0,606507.0,606635.0,606741.0,606842.0,607026.0,607137.0,607314.0,607427.0,607567.0,607740.0,607879.0,608064.0,608185.0,608347.0,608509.0,608723.0,608839.0,608986.0,609137.0,609347.0,609442.0,609573.0,609715.0,609883.0,610054.0,610289.0,610395.0,610611.0,610749.0,610883.0,611086.0,611320.0,611503.0,611640.0,611865.0,612041.0,612323.0,612514.0,612702.0,612929.0,613139.0,613392.0,613605.0,613772.0,613789.0,613759.0,613460.0,613069.0,612481.0,611848.0,611152.0,610310.0,609615.0,608932.0,608344.0,607808.0,607404.0,607025.0,606739.0,606537.0,606350.0,606204.0,606083.0,606004.0,605972.0,606041.0,606027.0,606099.0,606069.0,606174.0,606197.0,606207.0,606171.0,606147.0,606085.0,606057.0,605973.0,605862.0,605839.0,605765.0,605711.0,605689.0,605649.0,605760.0,605762.0,605898.0,606320.0,604453.0,606130.0,606280.0,606470.0,606521.0,606713.0,606920.0,607069.0,607337.0,607480.0,607639.0,607767.0,607987.0,608135.0,608307.0,608458.0,608661.0,608861.0,608997.0,609163.0,609337.0,609458.0,609661.0,609862.0,609976.0,610211.0,610361.0,610574.0,610759.0,610951.0,611099.0,611334.0,611498.0,611673.0,611823.0,612084.0,612292.0,612508.0,612714.0,612970.0,613172.0,613434.0,613647.0,613914.0,614134.0,614241.0,614335.0,614209.0,613975.0,613512.0,612956.0,612334.0,611685.0,610973.0,610220.0,609625.0,609105.0,608607.0,608184.0,607889.0,607630.0,607509.0,607288.0,607258.0,607206.0,607192.0,607135.0,607200.0,607328.0,607374.0,607461.0,607519.0,607620.0,607663.0,607665.0,607651.0,607703.0,607580.0,607574.0,607478.0,607404.0,607370.0,607315.0,607347.0,607376.0,607427.0,607424.0,607580.0,607658.0,607738.0,607910.0,608092.0,608257.0,608448.0,608597.0,608750.0,609002.0,609843.0,610366.0,610175.0,610692.0,608104.0,610125.0,610385.0,610497.0,610721.0,610923.0,611092.0,611291.0,611464.0,611631.0,611881.0,611992.0,612184.0,612395.0,612561.0,612801.0,613015.0,613166.0,613356.0,613601.0,613799.0,614042.0,614258.0,614418.0,614582.0,614801.0,615021.0,615240.0,615527.0,615691.0,615917.0,616156.0,616407.0,616633.0,616914.0,617187.0,617408.0,617606.0,617745.0,617703.0,617523.0,617187.0,616693.0,616076.0,615313.0,614567.0,613851.0,613078.0,612396.0,611839.0,611306.0,610971.0,610511.0,610217.0,610000.0,609874.0,609688.0,609601.0,609539.0,609435.0,609459.0,609441.0,609483.0,609488.0,609548.0,609572.0,609542.0,609530.0,609584.0,609458.0,609422.0,609326.0,609272.0,609190.0,609123.0,609075.0,609040.0,609083.0,609060.0,609058.0,609183.0,609311.0,609406.0,609533.0,609626.0,609722.0,609888.0,610023.0,610231.0,610432.0,610595.0,610767.0,610978.0,611106.0,611302.0,611453.0,611625.0,611888.0,612006.0,612143.0,613661.0,613431.0,612761.0,612948.0,612966.0,613190.0,613359.0,613567.0,613771.0,613984.0,614158.0,614356.0,614530.0,614727.0,614906.0,615091.0,615309.0,615532.0,615686.0,615853.0,616155.0,616354.0,616589.0,616737.0,617050.0,617293.0,617518.0,617564.0,617642.0,617422.0,617139.0,616668.0,616059.0,615446.0,614701.0,613945.0,613231.0,612514.0,611988.0,611513.0,611113.0,610708.0,610522.0,610294.0,610099.0,610012.0,609889.0,609819.0,609791.0,609820.0,609833.0,609899.0,609930.0,609992.0,610037.0,610046.0,610013.0,609992.0,609991.0,609960.0,609837.0,609838.0,609802.0,609648.0,609692.0,609683.0,609703.0,609709.0,609817.0,609819.0,609928.0,609981.0,610145.0,610289.0,610397.0,610577.0,610713.0,610838.0,611070.0,611251.0,611494.0,611605.0,611816.0,612020.0,612229.0,612347.0,612603.0,612728.0,612945.0,613066.0,613369.0,613470.0,613639.0,613809.0,614025.0,615470.0,615537.0,615613.0,615975.0,614336.0,614744.0,615515.0,616730.0,617245.0,615822.0,616059.0,616218.0,616434.0,616634.0,616841.0,617081.0,617289.0,617498.0,617690.0,617719.0,617704.0,617454.0,617127.0,616585.0,615967.0,615369.0,614703.0,614044.0,613371.0,612738.0,612202.0,611777.0,611467.0,611153.0,610935.0,610770.0,610666.0,610574.0,610466.0,610402.0,610511.0,610538.0,610577.0,610651.0,610704.0,610739.0,610731.0,610663.0,610648.0,610580.0,610550.0,610404.0,610324.0,610176.0,610115.0,610063.0,610005.0,609956.0,609916.0,609919.0,609878.0,610006.0,610057.0,610193.0,610278.0,610441.0,610438.0,610658.0,610766.0,610916.0,611030.0,611147.0,611336.0,611539.0,611681.0,611831.0,611978.0,612184.0,612252.0,612427.0,612563.0,612656.0,612821.0,612991.0,613080.0,613226.0,613368.0,613486.0,613695.0,613874.0,614025.0,614183.0,614465.0,614580.0,614764.0,614959.0,615063.0,615067.0,615495.0,615223.0,614933.0,614356.0,613112.0,611302.0,610615.0,610036.0,609343.0,610013.0,609534.0,609319.0,608837.0,608568.0,608363.0,608250.0,608134.0,608105.0,608077.0,608104.0,608121.0,608216.0,608183.0,608320.0,608393.0,608379.0,608446.0,608477.0,608377.0,608347.0,608269.0,608156.0,608122.0,608069.0,607974.0,607951.0,608023.0,607992.0,608037.0,608106.0,608216.0,608265.0,608388.0,608573.0,608677.0,608852.0,608982.0,609162.0,609335.0,609443.0,609626.0,609813.0,610030.0,610194.0,610387.0,610538.0,610757.0,610905.0,611070.0,611240.0,611320.0,611521.0,611712.0,611846.0,611950.0,612160.0,612342.0,612532.0,612722.0,612832.0,612991.0,613221.0,613425.0,613637.0,613819.0,613948.0,613928.0,613781.0,613378.0,612874.0,612254.0,611597.0,610888.0,610059.0,609280.0,608589.0,607982.0,607414.0,606943.0,606582.0,606264.0,605930.0,605815.0,605540.0,605457.0,605310.0,605212.0,605204.0,605177.0,605185.0,605269.0,605235.0,605240.0,605250.0,603845.0,605219.0,603746.0,605065.0,605381.0,605316.0,605400.0,604302.0,603972.0,604693.0,604702.0,604625.0,604727.0,604815.0,604895.0,605002.0,605136.0,605341.0,605439.0,605624.0,605773.0,605916.0,606118.0,606327.0,606421.0,606590.0,606832.0,606933.0,607131.0,607334.0,607475.0,607628.0,607724.0,607976.0,608083.0,608209.0,608464.0,608599.0,608758.0,608923.0,609124.0,609267.0,609487.0,609643.0,609801.0,609997.0,610228.0,610402.0,610637.0,610905.0,611046.0,611221.0,611339.0,611374.0,611202.0,610775.0,610297.0,609658.0,608902.0,608173.0,607421.0,606666.0,605961.0,605342.0,604810.0,604286.0,603925.0,603624.0,603294.0,603016.0,602811.0,602619.0,602435.0,602342.0,602283.0,602296.0,602316.0,602325.0,602321.0,602290.0,602405.0,602344.0,602311.0,602292.0,602270.0,602241.0,602245.0,602132.0,602382.0,601973.0,602099.0,601997.0,602399.0,602812.0,602949.0,603116.0,603186.0,603270.0,603386.0,603300.0,603504.0,603691.0,603852.0,604052.0,604255.0,604447.0,604678.0,604901.0,605013.0,605221.0,605442.0,605616.0,605799.0,605971.0,606212.0,606393.0,606574.0,606783.0,606968.0,607171.0,607449.0,607617.0,607776.0,607994.0,608156.0,608392.0,608586.0,608759.0,609053.0,609250.0,609455.0,609690.0,609943.0,610207.0,610486.0,610620.0,610814.0,610786.0,610631.0,610285.0,609765.0,609172.0,608472.0,607745.0,606965.0,606257.0,605627.0,605101.0,604628.0,604247.0,603903.0,603635.0,603385.0,603168.0,602945.0,602782.0,602710.0,602608.0,602569.0,602574.0,602630.0,602594.0,602662.0,602722.0,602733.0,602739.0,602758.0,602761.0,602758.0,602775.0,602703.0,602737.0,602716.0,602735.0,602815.0,602863.0,602991.0,603108.0,603195.0,603322.0,603496.0,603621.0,603907.0,604018.0,604231.0,604353.0,604608.0,604781.0,605032.0,605101.0,606344.0,606573.0,605858.0,607062.0,606108.0,606384.0,606565.0,606747.0,606979.0,607165.0,607333.0,607535.0,607800.0,608070.0,608202.0,608425.0,608611.0,608799.0,609020.0,609236.0,609429.0,609707.0,609887.0,610096.0,610299.0,610531.0,610762.0,611007.0,611218.0,611492.0,611710.0,611922.0,612162.0,612401.0,612708.0,612929.0,613081.0,613375.0,613387.0,613292.0,613016.0,612578.0,612007.0,611304.0,610554.0,609890.0,609180.0,608568.0,607907.0,607484.0,607105.0,606730.0,606517.0,606268.0,606055.0,605946.0,605795.0,605716.0,605627.0,605571.0,605603.0,605633.0,605667.0,605719.0,605705.0,605760.0,605730.0,605763.0,605749.0,605740.0,605646.0,605624.0,605533.0,605617.0,605589.0,605599.0,605651.0,605628.0,605683.0,605901.0,605970.0,606104.0,606228.0,606352.0,606518.0,606726.0,606828.0,607023.0,607189.0,607438.0,607619.0,607790.0,607962.0,608139.0,608328.0,608502.0,607636.0,609652.0,609699.0,610046.0,608077.0,608183.0,608489.0,608623.0,610105.0,610356.0,610585.0,610660.0,610886.0,611092.0,611320.0,611497.0,611715.0,611921.0,612117.0,612335.0,612524.0,612709.0,612895.0,613088.0,613364.0,613521.0,613730.0,613957.0,614154.0,614443.0,614643.0,614903.0,615053.0,615298.0,615486.0,615696.0,615718.0,615531.0,615273.0,614804.0,614244.0,613694.0,612988.0,612303.0,611728.0,611033.0,610513.0,610110.0,609782.0,609499.0,609227.0,609086.0,608954.0,608829.0,608730.0,608704.0,608695.0,608739.0,608812.0,608821.0,608936.0,608916.0,609022.0,609032.0,609037.0,609025.0,609006.0,609002.0,608952.0,608891.0,608855.0,608823.0,608877.0,608900.0,608938.0,608983.0,609060.0,609203.0,609306.0,609494.0,609614.0,609768.0,609906.0,610128.0,610334.0,610540.0,610656.0,610901.0,611130.0,611234.0,611432.0,611628.0,611790.0,612009.0,612177.0,612367.0,612534.0,612735.0,612925.0,613066.0,613284.0,613424.0,613639.0,614860.0,614972.0,615230.0,615260.0,614906.0,613427.0,615332.0,613864.0,615235.0,615493.0,615452.0,615818.0,616289.0,616523.0,616778.0,617027.0,617220.0,617504.0,617714.0,618016.0,618228.0,618366.0,618482.0,618496.0,618323.0,617973.0,617468.0,616852.0,616167.0,615483.0,614772.0,614078.0,613438.0,612867.0,612453.0,611992.0,611603.0,611334.0,611097.0,610847.0,610645.0,610503.0,610350.0,610247.0,610179.0,610154.0,610121.0,610105.0,610082.0,610035.0,609990.0,609962.0,609882.0,609839.0,609752.0,609683.0,609587.0,609519.0,609510.0,609499.0,609481.0,609443.0,609506.0,609597.0,609668.0,609674.0,609808.0,609933.0,610046.0,610198.0,610285.0,610471.0,610637.0,610750.0,610936.0,611102.0,611314.0,611525.0,611678.0,611839.0,612052.0,612242.0,612351.0,612581.0,612769.0,612844.0,613068.0,613189.0,613381.0,613530.0,613684.0,613814.0,613992.0,614208.0,614379.0,614574.0,614767.0,614985.0,615149.0,615395.0,615581.0,615765.0,615926.0,614754.0,616576.0,616720.0,617153.0,616352.0,615681.0,616944.0,616135.0,617683.0,618439.0,618512.0,618575.0,618622.0,618734.0,618762.0,618703.0,618469.0,618019.0,617475.0,616803.0,616023.0,615259.0,614464.0,613787.0,613125.0,612560.0,612023.0,611562.0,611219.0,610847.0,610515.0,610227.0,610081.0,609848.0,609710.0,609581.0,609525.0,609425.0,609417.0,609311.0,609304.0,609221.0,609122.0,609091.0,608943.0,608875.0,608811.0,608631.0,608467.0,608455.0,608298.0,608270.0,608233.0,608208.0,608141.0,608125.0,608110.0,608245.0,608217.0,608313.0,608435.0,608548.0,608605.0,608757.0,608819.0,608986.0,609054.0,609205.0,609342.0,609468.0,609622.0,609748.0,609940.0,610087.0,610242.0,610358.0,610486.0,610611.0,610790.0,610970.0,611088.0,611212.0,611327.0,611516.0,611672.0,611831.0,611961.0,612150.0,612231.0,612420.0,612545.0,612782.0,612916.0,613045.0,612580.0,613988.0,613042.0,612782.0,613051.0,614727.0,614471.0,614701.0,614923.0,615032.0,615119.0,615103.0,614865.0,614516.0,613931.0,613261.0,612537.0,611729.0,610936.0,610219.0,609488.0,608828.0,608292.0,607825.0,607434.0,607100.0,606810.0,606595.0,606397.0,606229.0,606067.0,605987.0,605883.0,605888.0,605893.0,605854.0,605885.0,605862.0,605843.0,605794.0,605636.0,605577.0,605502.0,605343.0,605243.0,605163.0,605071.0,605021.0,604963.0,604923.0,604889.0,604881.0,604889.0,605030.0,605056.0,605141.0,605267.0,605355.0,605451.0,605620.0,605766.0,605916.0,606042.0,606191.0,606345.0,606519.0,606611.0,606793.0,606939.0,607099.0,607271.0,607423.0,607544.0,607686.0,607865.0,607991.0,608200.0,608289.0,608495.0,608642.0,608739.0,608927.0,609084.0,609226.0,609390.0,609531.0,609757.0,609950.0,610084.0,610292.0,610489.0,610653.0,610859.0,611061.0,611235.0,611398.0,611442.0,611351.0,611081.0,610669.0,610936.0,610346.0,608310.0,606360.0,605857.0,606612.0,605408.0,605612.0,604331.0,604146.0,603806.0,603498.0,603297.0,603176.0,603025.0,602917.0,602817.0,602816.0,602846.0,602898.0,602896.0,602964.0,602930.0,602951.0,602910.0,602863.0,602782.0,602702.0,602531.0,602478.0,602347.0,602237.0,602228.0,602153.0,602145.0,602050.0,602137.0,602170.0,602304.0,602382.0,602505.0,602606.0,602738.0,602837.0,602962.0,603193.0,603440.0,603570.0,603783.0,603924.0,604131.0,604321.0,604443.0,604692.0,604864.0,604976.0,605223.0,605392.0,605595.0,605761.0,605945.0,606101.0,606279.0,606423.0,606621.0,606749.0,606975.0,607108.0,607309.0,607537.0,607759.0,607973.0,608139.0,608361.0,608542.0,608675.0,608596.0,608394.0,608008.0,607411.0,606752.0,605867.0,605067.0,604261.0,603460.0,602700.0,602096.0,601481.0,601012.0,600542.0,600173.0,599903.0,599662.0,599420.0,599267.0,599089.0,599010.0,598955.0,598945.0,598960.0,599181.0,598905.0,598943.0,598867.0,598848.0,598828.0,598663.0,598666.0,598500.0,598376.0,598323.0,598247.0,598194.0,598194.0,598196.0,598142.0,598171.0,598242.0,598326.0,598493.0,598587.0,598741.0,598882.0,599067.0,599225.0,599391.0,599552.0,599767.0,599947.0,600081.0,600334.0,600518.0,600649.0,600853.0,601002.0,601250.0,601413.0,601556.0,601695.0,601876.0,602074.0,602243.0,602429.0,602591.0,602818.0,602976.0,603151.0,603392.0,603545.0,603787.0,603966.0,604183.0,604440.0,604662.0,604882.0,605012.0,605054.0,604888.0,604551.0,604054.0,603379.0,602679.0,601878.0,601067.0,600299.0,599622.0,599012.0,598459.0,598118.0,597752.0,597411.0,597118.0,596969.0,596821.0,596621.0,596580.0,596512.0,596474.0,596462.0,596599.0,596587.0,596629.0,596644.0,596667.0,596713.0,596687.0,596683.0,596605.0,596621.0,596581.0,596561.0,596503.0,596517.0,596614.0,596586.0,596607.0,596654.0,596833.0,596956.0,597068.0,597282.0,597487.0,597216.0,598022.0,598033.0,598251.0,598426.0,598653.0,598877.0,599164.0,599306.0,599543.0,599800.0,599990.0,600200.0,600380.0,600572.0,600887.0,601032.0,601184.0,601386.0,601621.0,601821.0,602022.0,602232.0,602441.0,602674.0,602869.0,603063.0,603299.0,603541.0,603781.0,604045.0,604299.0,604629.0,604857.0,605058.0,605323.0,605397.0,605281.0,605035.0,604612.0,604008.0,603369.0,602641.0,601878.0,601162.0,600486.0,599863.0,599407.0,599013.0,598607.0,598341.0,598138.0,597920.0,597821.0,597759.0,597693.0,597626.0,597642.0,597717.0,597774.0,597904.0,597929.0,598024.0,598142.0,598099.0,598166.0,598200.0,598185.0,598152.0,598182.0,598155.0,598151.0,598236.0,598246.0,598321.0,598409.0,598533.0,598622.0,598771.0,598892.0,599105.0,599287.0,599534.0,599641.0,599971.0,600166.0,600379.0,600567.0,600815.0,601052.0,601361.0,601544.0,601763.0,601971.0,602224.0,602438.0,602668.0,602895.0,603060.0,603305.0,603577.0,603808.0,604093.0,604283.0,604551.0,604818.0,604975.0,605198.0,605463.0,605737.0,605952.0,606177.0,606442.0,606684.0,606977.0,607260.0,607448.0,607791.0,608046.0,608339.0,608570.0,608742.0,608793.0,608753.0,608418.0,607926.0,607398.0,606720.0,607060.0,606535.0,605860.0,605140.0,602085.0,599373.0,602699.0,602427.0,602213.0,602035.0,601949.0,601883.0,601813.0,601795.0,601900.0,601916.0,602027.0,602153.0,602223.0,602320.0,602428.0,602500.0,602510.0,602522.0,602530.0,602541.0,602578.0,602449.0,602504.0,602575.0,602606.0,602674.0,602705.0,602766.0,602887.0,602965.0,603189.0,603287.0,603462.0,603716.0,603944.0,604112.0,604321.0,604510.0,604724.0,604946.0,605164.0,605407.0,605534.0,605828.0,606056.0,606178.0,606420.0,606552.0,606808.0,606943.0,607118.0,607333.0,607537.0,607669.0,607847.0,608002.0,608249.0,608488.0,608706.0,608910.0,609191.0,609332.0,609620.0,609946.0,610135.0,610367.0,610648.0,610890.0,611051.0,611134.0,611061.0,610875.0,610480.0,609868.0,609363.0,608707.0,608121.0,607534.0,606983.0,606540.0,606148.0,605862.0,605617.0,605489.0,605372.0,605409.0,605723.0,605899.0,606130.0,606202.0,604886.0,604932.0,605942.0,605779.0,606585.0,606407.0,606502.0,607668.0,606434.0,606464.0,607729.0,606478.0,606494.0,606559.0,606581.0,606545.0,606610.0,606706.0,606824.0,606992.0,607180.0,607373.0,607605.0,607830.0,608078.0,608315.0,608545.0,608885.0,609050.0,609308.0,609574.0,609753.0,610077.0,610293.0,610515.0,610799.0,611032.0,611244.0,611508.0,611762.0,612003.0,612239.0,612476.0,612725.0,612973.0,613186.0,613385.0,613640.0,613879.0,614147.0,614406.0,614602.0,614850.0,615103.0,615376.0,615614.0,615873.0,616150.0,616430.0,616630.0,616822.0,616803.0,616707.0,616427.0,616014.0,615451.0,614807.0,614105.0,613412.0,612781.0,612137.0,611650.0,611245.0,610848.0,610517.0,610285.0,610061.0,609918.0,609743.0,609612.0,609562.0,609491.0,609458.0,609542.0,609578.0,609651.0,609668.0,609726.0,609739.0,609720.0,609765.0,609813.0,609774.0,609725.0,609694.0,610003.0,610023.0,610034.0,610057.0,608425.0,609130.0,609581.0,609512.0,610112.0,610273.0,610399.0,610622.0,610661.0,610850.0,611057.0,611200.0,611452.0,610338.0,611726.0,611929.0,612087.0,612261.0,612487.0,612629.0,612838.0,613015.0,613238.0,613389.0,613607.0,613736.0,613980.0,614180.0,614263.0,614500.0,614736.0,614891.0,615177.0,615338.0,615509.0,615704.0,615942.0,616130.0,616384.0,616579.0,616864.0,617097.0,617291.0,617510.0,617807.0,617967.0,618053.0,618027.0,617807.0,617503.0,616934.0,616372.0,615659.0,614931.0,614182.0,613512.0,612887.0,612320.0,611846.0,611554.0,611244.0,610966.0,610697.0,610461.0,610357.0,610191.0,610103.0,610020.0,610007.0,610036.0,610034.0,610069.0,610114.0,610059.0,610036.0,610082.0,610037.0,609999.0,609936.0,609858.0,609830.0,609809.0,609763.0,609757.0,609737.0,609739.0,609796.0,609868.0,610008.0,610131.0,610168.0,610367.0,610549.0,610626.0,610869.0,611033.0,611172.0,611404.0,611583.0,611751.0,611924.0,612096.0,612244.0,612425.0,612614.0,612823.0,612918.0,613358.0,613455.0,612258.0,613630.0,613497.0,615232.0,613894.0,615699.0,614473.0,614658.0,614857.0,615035.0,615220.0,615465.0,615647.0,615851.0,616039.0,616363.0,616516.0,616790.0,617021.0,617211.0,617414.0,617395.0,617433.0,617135.0,616764.0,616171.0,615508.0,614869.0,614078.0,613363.0,612747.0,612073.0,611545.0,611076.0,610728.0,610407.0,610162.0,610048.0,609919.0,609791.0,609704.0,609682.0,609676.0,609674.0,609731.0,609796.0,609848.0,609890.0,609992.0,609935.0,609933.0,609943.0,609921.0,609858.0,609783.0,609742.0,609725.0,609631.0,609680.0,609710.0,609802.0,609759.0,609871.0,610035.0,610062.0,610213.0,610349.0,610509.0,610680.0,610862.0,610993.0,611196.0,611347.0,611573.0,611681.0,611877.0,612099.0,612302.0,612481.0,612662.0,612784.0,613009.0,613171.0,613366.0,613556.0,613948.0,613879.0,614051.0,614195.0,614165.0,614916.0,614986.0,615159.0,616104.0,615342.0,615481.0,615742.0,615945.0,616150.0,616320.0,616609.0,616718.0,616877.0,616794.0,616533.0,616157.0,615581.0,615141.0,614437.0,613727.0,613125.0,612532.0,611943.0,611451.0,611086.0,610744.0,610504.0,610294.0,610151.0,610102.0,610024.0,610032.0,610026.0,610145.0,610180.0,610242.0,610315.0,610474.0,610494.0,610576.0,610593.0,610571.0,610581.0,610530.0,610501.0,610400.0,610392.0,610398.0,610418.0,610385.0,610467.0,610491.0,610644.0,610687.0,610812.0,610926.0,611124.0,611349.0,611463.0,611652.0,611936.0,612118.0,612335.0,612521.0,612738.0,612944.0,613114.0,613327.0,613526.0,613730.0,613917.0,614094.0,614332.0,614475.0,614761.0,614883.0,615119.0,615339.0,615538.0,615767.0,616015.0,616142.0,616426.0,616672.0,616855.0,617095.0,617343.0,617599.0,617768.0,617936.0,617950.0,617902.0,617618.0,617195.0,616670.0,615868.0,614519.0,614616.0,614589.0,614775.0,613061.0,613868.0,611791.0,611001.0,610922.0,610428.0,610448.0,611318.0,611263.0,611254.0,611241.0,611363.0,611393.0,611454.0,611566.0,611590.0,611740.0,611751.0,611836.0,611875.0,611845.0,611847.0,611857.0,611850.0,611900.0,611850.0,611896.0,611906.0,612016.0,612076.0,612168.0,612289.0,612422.0,612547.0,612724.0,612883.0,613123.0,613322.0,613528.0,613754.0,613935.0,614120.0,614366.0,614580.0,614716.0,614934.0,615145.0,615352.0,615534.0,615851.0,616006.0,616158.0,616332.0,616509.0,616730.0,616938.0,617159.0,617380.0,617557.0,617741.0,617972.0,618204.0,618428.0,618677.0,618923.0,619105.0,619279.0,619446.0,619481.0,619381.0,619171.0,618772.0,618278.0,617769.0,617133.0,616533.0,615952.0,615393.0,614994.0,614552.0,614239.0,613971.0,613731.0,613534.0,613388.0,613273.0,613206.0,613119.0,613114.0,613185.0,613319.0,613340.0,613435.0,614405.0,613514.0,613653.0,614751.0,614764.0,612225.0,613292.0,612135.0,612138.0,613456.0,613461.0,613502.0,613583.0,613606.0,613720.0,613775.0,613928.0,614081.0,614233.0,614339.0,614518.0,614645.0,614852.0,614996.0,615242.0,615422.0,615598.0,615771.0,615938.0,616120.0,616269.0,616463.0,616648.0,616875.0,617003.0,617231.0,617443.0,617600.0,617911.0,617974.0,618193.0,618328.0,618568.0,618718.0,618910.0,619118.0,619297.0,619495.0,619712.0,619906.0,620135.0,620362.0,620576.0,620748.0,620844.0,620836.0,620618.0,620258.0,619806.0,619227.0,618625.0,618025.0,617377.0,616804.0,616230.0,615816.0,615382.0,615065.0,614847.0,614597.0,614415.0,614230.0,614143.0,613938.0,613932.0,613925.0,613953.0,613996.0,614060.0,614086.0,614156.0,614139.0,614181.0,614153.0,614116.0,614104.0,614064.0,613986.0,613955.0,613960.0,613962.0,613957.0,613957.0,613971.0,614080.0,614096.0,614894.0,613821.0,615138.0,615178.0,614747.0,614882.0,615193.0,615187.0,615182.0,615086.0,616718.0,614315.0,615813.0,616052.0,616198.0,616388.0,616487.0,616649.0,616817.0,616967.0,617124.0,617258.0,617472.0,617597.0,617755.0,617940.0,618163.0,618224.0,618409.0,618613.0,618745.0,618935.0,619149.0,619317.0,619536.0,619714.0,619887.0,620128.0,620292.0,620463.0,620421.0,620337.0,620070.0,619605.0,619082.0,618484.0,617809.0,617158.0,616500.0,615919.0,615377.0,614935.0,614538.0,614198.0,613944.0,613721.0,613576.0,613436.0,613359.0,613314.0,613260.0,613279.0,613354.0,613341.0,613359.0,613442.0,613447.0,613429.0,613366.0,613269.0,613261.0,613146.0,613065.0,612943.0,612878.0,612793.0,612733.0,612797.0,612740.0,612775.0,612822.0,612849.0,612926.0,613058.0,613189.0,613310.0,613401.0,613566.0,613664.0,613789.0,613978.0,614168.0,614318.0,614517.0,614669.0,614813.0,614982.0,615129.0,615282.0,615400.0,615578.0,615738.0,615832.0,615957.0,616121.0,616264.0,616392.0,616573.0,616737.0,616910.0,617114.0,617015.0,617960.0,617788.0,617865.0,619325.0,619454.0,619548.0,619210.0,617990.0,617718.0,617199.0,616631.0,615975.0,615253.0,614546.0,613924.0,613334.0,612810.0,612367.0,611930.0,611631.0,611456.0,611136.0,610977.0,610822.0,610726.0,610633.0,610622.0,610641.0,610662.0,610722.0,610732.0,610751.0,610769.0,610733.0,610749.0,610622.0,610594.0,610550.0,610473.0,610342.0,610311.0,610319.0,610287.0,610256.0,610317.0,610306.0,610367.0,610435.0,610562.0,610668.0,610781.0,610924.0,611063.0,611274.0,611348.0,611445.0,611645.0,611804.0,611971.0,612123.0,612321.0,612443.0,612554.0,612801.0,612929.0,613117.0,613337.0,613424.0,613654.0,613753.0,613916.0,614043.0,614233.0,614383.0,614536.0,614709.0,614875.0,615086.0,615277.0,615475.0,615694.0,615886.0,616094.0,616312.0,616496.0,616755.0,616882.0,616947.0,616937.0,616711.0,616284.0,615796.0,615068.0,614349.0,613629.0,612851.0,612120.0,611468.0,611142.0,610730.0,610322.0,609696.0,607905.0,609129.0,608926.0,608746.0,608436.0,608275.0,608181.0,608163.0,608103.0,608107.0,608095.0,608073.0,608113.0,608014.0,607999.0,607923.0,607867.0,607751.0,607702.0,607595.0,607476.0,607471.0,607487.0,607450.0,607378.0,607419.0,607474.0,607581.0,607593.0,607706.0,607856.0,607946.0,608064.0,608149.0,608380.0,608510.0,608689.0,608854.0,608998.0,609165.0,609332.0,609514.0,609676.0,609808.0,609939.0,610108.0,610298.0,610421.0,610642.0,610756.0,610948.0,611075.0,611272.0,611440.0,611666.0,611809.0,611966.0,612207.0,612369.0,612547.0,612712.0,612881.0,613100.0,613324.0,613539.0,613754.0,614050.0,614215.0,614396.0,614583.0,614652.0,614553.0,614264.0,613805.0,613265.0,612568.0,611763.0,611016.0,610340.0,609611.0,608990.0,608748.0,608183.0,607789.0,607650.0,607706.0,607440.0,607525.0,605095.0,604931.0,606646.0,604747.0,605952.0,605936.0,605920.0,605926.0,605960.0,605933.0,605992.0,605874.0,605857.0,605804.0,605781.0,605675.0,605660.0,605539.0,605529.0,605567.0,605536.0,605575.0,605630.0,605669.0,605806.0,605894.0,605970.0,606083.0,606267.0,606491.0,606611.0,605982.0,604776.0,607091.0,607247.0,607405.0,607604.0,607764.0,607927.0,608175.0,608311.0,608475.0,608682.0,608837.0,609007.0,609145.0,609385.0,609546.0,609751.0,609837.0,610094.0,610281.0,610423.0,610647.0,610850.0,610916.0,611134.0,611367.0,611490.0,611661.0,611882.0,612075.0,612277.0,612481.0,612776.0,612929.0,613166.0,613323.0,613476.0,613586.0,613431.0,613126.0,612691.0,612019.0,611402.0,610693.0,609919.0,609199.0,608450.0,607884.0,607380.0,606869.0,606595.0,606312.0,606037.0\nArousal Valence Label: AMVM\n\n==================================================\n\nSubject ID: 7, Video ID: 3\nValence: 5\nArousal: 6\nGSR Data: 178895.96875,178088.4375,177970.546875,177919.5,177681.84375,179547.46875,183581.625,183554.5,181730.046875,183094.25,184921.90625,185759.40625,193949.0,212580.3125,207168.6875,205223.546875,203409.546875,202621.34375,203351.578125,203553.21875,203282.546875,203407.5625,203321.328125,203368.5625,203303.375,203458.375,203452.40625,203533.546875,203588.765625,203693.40625,203662.6875,203671.765625,203751.03125,203763.78125,203782.859375,203855.609375,203791.71875,203841.359375,203901.71875,203986.03125,204088.0625,204112.546875,204112.609375,204225.171875,204269.78125,204347.125,204423.578125,204462.265625,204555.5625,204624.078125,204697.859375,204697.265625,204721.015625,204736.984375,204847.546875,204914.25,204946.5,205017.265625,205041.5,205142.9375,205114.140625,205190.734375,205215.484375,205239.921875,205280.5625,205357.671875,205399.953125,205485.875,205461.21875,205481.703125,205548.203125,205645.875,205615.46875,205745.984375,205798.109375,205795.203125,205807.40625,205920.578125,205904.375,205979.109375,206000.03125,206049.921875,206027.234375,206141.125,206079.890625,206148.359375,206160.28125,206173.0,206229.796875,206369.25,206388.234375,206439.953125,206398.625,206510.640625,206465.796875,206484.765625,207288.953125,208950.0625,207716.765625,207234.21875,207374.15625,207267.71875,207224.78125,207061.6875,206884.984375,206744.796875,206551.3125,206510.171875,206212.453125,206082.828125,205530.03125,204906.015625,204696.625,204966.09375,202004.5,202368.375,202367.703125,202228.671875,202066.40625,202147.28125,202302.84375,202507.796875,202621.375,202712.75,202639.21875,202703.046875,202756.609375,202730.09375,202825.4375\nPPG Data: 565274.0,565598.0,566007.0,566224.0,566272.0,566043.0,565650.0,565267.0,564843.0,564295.0,563860.0,563398.0,562999.0,562599.0,562206.0,561821.0,561513.0,561134.0,560587.0,559963.0,559113.0,558391.0,557718.0,557113.0,556601.0,556148.0,555805.0,555587.0,555388.0,555391.0,555552.0,555731.0,556067.0,556432.0,556935.0,557574.0,558164.0,558669.0,559097.0,559565.0,559979.0,560436.0,560794.0,561190.0,561708.0,562206.0,562486.0,562638.0,562688.0,562784.0,562871.0,562973.0,563071.0,563272.0,563347.0,563551.0,563785.0,563910.0,564072.0,564164.0,564325.0,564481.0,564585.0,564657.0,564643.0,564642.0,564590.0,564612.0,564629.0,564628.0,564837.0,564982.0,565099.0,565115.0,565157.0,565133.0,565306.0,565067.0,565591.0,566094.0,566157.0,566357.0,566725.0,566740.0,566703.0,566688.0,566657.0,566553.0,566368.0,566002.0,565625.0,565123.0,564547.0,563913.0,563330.0,562864.0,562392.0,562008.0,561687.0,561535.0,561458.0,561479.0,561619.0,561723.0,561979.0,562315.0,562822.0,563331.0,563781.0,564166.0,564482.0,564687.0,564635.0,564456.0,564026.0,563594.0,563106.0,562607.0,562131.0,561599.0,561093.0,560573.0,560098.0,559697.0,559445.0,559156.0,558946.0,558842.0,558707.0,558690.0,558657.0,558728.0,558771.0,558866.0,558868.0,558878.0,558969.0,559006.0,559058.0,559211.0,559256.0,559234.0,559292.0,559221.0,559273.0,559473.0,559713.0,560091.0,560459.0,560743.0,560932.0,561112.0,561256.0,561369.0,561366.0,561428.0,561454.0,561339.0,561145.0,560734.0,560091.0,559259.0,558523.0,557688.0,556646.0,555574.0,554400.0,553466.0,552621.0,551907.0,551381.0,550918.0,550368.0,549853.0,549311.0,548789.0,548327.0,546352.0,547571.0,547164.0,546740.0,546268.0,545698.0,545129.0,544504.0,543903.0,543247.0,542616.0,542058.0,541498.0,540983.0,540499.0,539916.0,539416.0,538813.0,538325.0,537838.0,537344.0,536978.0,536615.0,536316.0,536025.0,535707.0,535393.0,535121.0,535165.0,534996.0,534611.0,534206.0,533788.0,533529.0,533232.0,532955.0,532624.0,532286.0,531855.0,531339.0,530525.0,529687.0,529366.0,529423.0,529580.0,529807.0,530164.0,530714.0,531396.0,531943.0,532483.0,532877.0,533306.0,533549.0,533879.0,534235.0,534587.0,535228.0,535915.0,536912.0,538096.0,539780.0,541986.0,544206.0,546135.0,547976.0,549735.0,551358.0,552941.0,554595.0,556251.0,557916.0,559417.0,561082.0,562768.0,564586.0,566410.0,568243.0,570217.0,572306.0,574439.0,576764.0,579322.0,581904.0,584493.0,587118.0,589620.0,592014.0,594092.0,596067.0,597945.0,599777.0,601444.0,602969.0,604095.0,604938.0,605693.0,605937.0,606194.0,606476.0,606719.0,607053.0,607394.0,607985.0,608455.0,608973.0,609374.0,609746.0,610166.0,610743.0,611112.0,611601.0,612175.0,612854.0,613574.0,614586.0,615471.0,616482.0,617648.0,618804.0,620115.0,621503.0,622848.0,624316.0,625854.0,627382.0,628969.0,630618.0,632230.0,633721.0,635228.0,636700.0,638092.0,639413.0,640475.0,641203.0,641530.0,641496.0,641053.0,640378.0,639725.0,639196.0,639167.0,639204.0,638870.0,638426.0,638639.0,638970.0,639031.0,638708.0,638366.0,638234.0,638173.0,638078.0,638018.0,637881.0,637798.0,637702.0,637599.0,637556.0,637509.0,637487.0,637247.0,637186.0,637120.0,637034.0,636953.0,636871.0,636808.0,636770.0,636772.0,636770.0,636839.0,636868.0,636954.0,637062.0,636992.0,637013.0,636866.0,636852.0,636936.0,637044.0,637387.0,637798.0,638186.0,638504.0,638935.0,639155.0,639273.0,639348.0,639460.0,639723.0,639998.0,640250.0,640446.0,640573.0,640619.0,640596.0,640450.0,640137.0,639724.0,639153.0,638644.0,638218.0,637827.0,637455.0,637173.0,636951.0,636583.0,636225.0,636006.0,635760.0,635497.0,634992.0,634618.0,634336.0,634171.0,634098.0,633846.0,633516.0,633266.0,632971.0,632777.0,632593.0,632419.0,632337.0,632103.0,631534.0,631060.0,630693.0,630441.0,630254.0,630137.0,630014.0,629891.0,629789.0,629581.0,629394.0,629166.0,629006.0,628808.0,628627.0,628585.0,628423.0,628333.0,628346.0,628234.0,628197.0,627962.0,627750.0,627471.0,627224.0,626911.0,626711.0,626499.0,626375.0,626191.0,625913.0,625656.0,625305.0,624844.0,624343.0,623872.0,623453.0,623035.0,622656.0,622243.0,621748.0,621170.0,620465.0,619712.0,618887.0,617982.0,616996.0,616048.0,615056.0,614078.0,613123.0,612274.0,611460.0,610667.0,609984.0,609250.0,608671.0,608080.0,607513.0,607012.0,606552.0,606125.0,605602.0,605218.0,604778.0,604266.0,603699.0,603212.0,602727.0,602130.0,601530.0,600957.0,600417.0,599746.0,599223.0,598644.0,598083.0,597567.0,597068.0,596599.0,596131.0,595678.0,595324.0,594900.0,594581.0,594227.0,593907.0,593557.0,593296.0,592957.0,592667.0,592466.0,592039.0,591837.0,591586.0,591319.0,591076.0,590809.0,590543.0,590315.0,590071.0,589796.0,589553.0,589337.0,589032.0,588838.0,588548.0,588323.0,588016.0,587709.0,587204.0,586609.0,585905.0,584979.0,583942.0,582858.0,581669.0,580504.0,579405.0,578316.0,577312.0,576284.0,575480.0,574783.0,574005.0,573341.0,572777.0,572267.0,571731.0,571297.0,570809.0,570459.0,570066.0,569747.0,569371.0,568956.0,568570.0,568155.0,567765.0,567301.0,566719.0,566330.0,565819.0,565278.0,564870.0,564371.0,563945.0,563577.0,563151.0,562838.0,562505.0,562220.0,561905.0,561683.0,561477.0,561224.0,561035.0,560870.0,560597.0,560491.0,560323.0,560125.0,560015.0,559886.0,559682.0,559542.0,559395.0,559251.0,559046.0,558922.0,558770.0,558603.0,558506.0,558380.0,558240.0,558120.0,558032.0,557881.0,557773.0,557643.0,557536.0,557242.0,556886.0,556314.0,555563.0,554751.0,553828.0,552858.0,551880.0,550883.0,549965.0,549174.0,548398.0,547756.0,547174.0,546783.0,546293.0,545975.0,545637.0,545373.0,545129.0,544986.0,544866.0,544761.0,544647.0,544475.0,544424.0,544233.0,544077.0,543950.0,543741.0,543484.0,543199.0,542936.0,542606.0,542349.0,542146.0,541892.0,541694.0,541497.0,541282.0,541228.0,541108.0,541019.0,540972.0,540889.0,540786.0,540828.0,540844.0,540758.0,540837.0,540859.0,540919.0,540920.0,540928.0,540921.0,540978.0,541011.0,541027.0,541020.0,541063.0,541133.0,541104.0,541124.0,541235.0,541241.0,541275.0,541325.0,541361.0,541432.0,541547.0,541574.0,541629.0,541649.0,541621.0,541309.0,540984.0,540495.0,539798.0,539151.0,538442.0,537787.0,537106.0,536524.0,535976.0,535467.0,535147.0,534901.0,534617.0,534556.0,534425.0,534379.0,534392.0,534403.0,534487.0,534626.0,534745.0,534926.0,535012.0,535074.0,535190.0,535270.0,535271.0,535228.0,535273.0,535277.0,535207.0,535135.0,535098.0,535083.0,535119.0,535167.0,535220.0,535280.0,535305.0,535434.0,535552.0,535742.0,535866.0,536069.0,536238.0,536372.0,536564.0,536724.0,536963.0,537115.0,537291.0,537543.0,537657.0,537843.0,538043.0,538256.0,538410.0,538600.0,538645.0,538822.0,539019.0,539146.0,539383.0,539569.0,539665.0,539849.0,540046.0,540147.0,540329.0,540508.0,540637.0,540697.0,540619.0,540458.0,540116.0,539737.0,539263.0,538760.0,538285.0,537781.0,537323.0,536949.0,536681.0,536429.0,536311.0,536190.0,536171.0,536162.0,536321.0,536283.0,536492.0,536577.0,536830.0,537017.0,537268.0,537456.0,537646.0,537783.0,537938.0,538074.0,538160.0,538197.0,538273.0,538317.0,538351.0,538401.0,538424.0,538499.0,538612.0,538666.0,538813.0,538903.0,539102.0,539324.0,539420.0,539641.0,539845.0,540047.0,540295.0,540476.0,540704.0,540936.0,541159.0,541391.0,541520.0,541772.0,541958.0,542164.0,542351.0,542531.0,542697.0,542837.0,543022.0,543156.0,543341.0,543525.0,543685.0,543890.0,544073.0,544188.0,544420.0,544461.0,544712.0,544824.0,544807.0,544755.0,544525.0,544255.0,543858.0,543367.0,542862.0,542318.0,541850.0,541465.0,541074.0,540758.0,540574.0,540367.0,540276.0,540254.0,540199.0,540154.0,540188.0,540309.0,540435.0,540513.0,540672.0,540873.0,540969.0,541189.0,541333.0,541423.0,541526.0,541578.0,541675.0,541734.0,541744.0,541778.0,541795.0,541871.0,541920.0,541939.0,542043.0,542149.0,542299.0,542391.0,542587.0,542734.0,542938.0,543071.0,543255.0,543470.0,543608.0,543852.0,544100.0,544292.0,544461.0,544752.0,544962.0,545083.0,545270.0,545408.0,545665.0,545888.0,546019.0,546184.0,546355.0,546518.0,546689.0,546806.0,547037.0,547213.0,547392.0,547473.0,547689.0,547835.0,548031.0,548206.0,548377.0,548585.0,548630.0,548796.0,548795.0,548606.0,548358.0,548035.0,547681.0,547207.0,546721.0,546283.0,545838.0,545432.0,545107.0,544883.0,544727.0,544299.0,545104.0,543791.0,545021.0,545026.0,544489.0,544553.0,544750.0,544848.0,545013.0,545149.0,545286.0,545496.0,545703.0,545760.0,545853.0,545930.0,545946.0,545970.0,546037.0,546083.0,546089.0,546172.0,546213.0,546334.0,546432.0,546492.0,546715.0,546792.0,546937.0,547086.0,547323.0,547484.0,547716.0,547900.0,548111.0,548221.0,548450.0,548625.0,548842.0,549055.0,549240.0,549386.0,549594.0,549748.0,549883.0,550060.0,550256.0,550376.0,550475.0,550611.0,550838.0,550945.0,551060.0,551198.0,551394.0,551518.0,551617.0,551824.0,551853.0,552061.0,552188.0,552343.0,552474.0,552490.0,552486.0,552339.0,552132.0,551825.0,551446.0,551025.0,550570.0,550168.0,549685.0,549281.0,548942.0,548686.0,548442.0,548336.0,548271.0,548229.0,548096.0,548092.0,548183.0,548213.0,548264.0,548426.0,548564.0,548687.0,548850.0,548943.0,549012.0,549144.0,549208.0,549217.0,548045.0,548335.0,548344.0,548762.0,549412.0,550397.0,549298.0,549572.0,550013.0,549705.0,550720.0,548231.0,549973.0,550133.0,550223.0,550356.0,550493.0,550684.0,550852.0,551005.0,551174.0,551320.0,551487.0,551661.0,551772.0,551955.0,552205.0,552269.0,552437.0,552545.0,552706.0,552836.0,552973.0,553130.0,553301.0,553374.0,553577.0,553687.0,553781.0,553947.0,554061.0,554215.0,554398.0,554555.0,554667.0,554741.0,554852.0,554916.0,554915.0,554805.0,554524.0,554182.0,553828.0,553417.0,553023.0,552582.0,552160.0,551777.0,551493.0,551219.0,551109.0,551023.0,550890.0,550822.0,550834.0,550773.0,550935.0,551034.0,551201.0,551291.0,551511.0,551631.0,551793.0,551978.0,552078.0,552178.0,552187.0,552248.0,552268.0,552211.0,552203.0,552262.0,552277.0,552295.0,552355.0,552401.0,552518.0,552611.0,552711.0,552806.0,552931.0,553083.0,553213.0,553383.0,553588.0,553775.0,553949.0,554117.0,554282.0,554436.0,554264.0,554505.0,554961.0,555158.0,553973.0,555985.0,555383.0,556082.0,555983.0,555972.0,556144.0,556202.0,557989.0,556453.0,556600.0,556755.0,556890.0,556977.0,557172.0,557268.0,557361.0,557371.0,557244.0,557024.0,556715.0,556274.0,555774.0,555232.0,554714.0,554189.0,553685.0,553205.0,552785.0,552432.0,552112.0,551886.0,551681.0,551585.0,551452.0,551398.0,551364.0,551281.0,551389.0,551364.0,551487.0,551519.0,551587.0,551607.0,551668.0,551738.0,551594.0,551665.0,551522.0,551472.0,551417.0,551322.0,551246.0,551229.0,551179.0,551270.0,551203.0,551216.0,551261.0,551329.0,551391.0,551402.0,551597.0,551672.0,551784.0,551873.0,552041.0,552106.0,552271.0,552372.0,552503.0,552591.0,552684.0,552796.0,552929.0,553055.0,553137.0,553274.0,553315.0,553420.0,553446.0,553576.0,553659.0,553784.0,553773.0,553816.0,554012.0,554112.0,554184.0,554298.0,554341.0,554434.0,554458.0,554592.0,554522.0,554771.0,554534.0,553887.0,553396.0,553101.0,552239.0,551660.0,550960.0,550484.0,549752.0,549262.0,548768.0,548327.0,547985.0,548618.0,547591.0,547442.0,547348.0,547267.0,547204.0,547232.0,547210.0,547253.0,547226.0,547347.0,547334.0,547319.0,547273.0,547206.0,547098.0,547046.0,546905.0,546823.0,546728.0,546598.0,546485.0,546480.0,546466.0,546431.0,546456.0,546422.0,546431.0,546514.0,546637.0,546654.0,546792.0,546873.0,546883.0,547076.0,547200.0,547249.0,547375.0,547466.0,547568.0,547669.0,547769.0,547805.0,547950.0,548017.0,548095.0,548217.0,548307.0,548419.0,548391.0,548517.0,548587.0,548688.0,548791.0,548904.0,548975.0,549070.0,549121.0,549219.0,549345.0,549415.0,549494.0,549619.0,549660.0,549557.0,549352.0,549043.0,548559.0,548050.0,547468.0,546878.0,546168.0,545591.0,545016.0,544530.0,544105.0,543743.0,543517.0,543232.0,543063.0,542966.0,542833.0,542794.0,542756.0,542803.0,542870.0,542925.0,542972.0,543075.0,543419.0,543491.0,543419.0,543317.0,543334.0,543327.0,543161.0,543126.0,542942.0,542840.0,542781.0,542709.0,542708.0,542699.0,542690.0,542738.0,542825.0,542849.0,542938.0,543040.0,543145.0,543247.0,543394.0,543469.0,543583.0,543788.0,543909.0,544037.0,544199.0,544307.0,544424.0,544487.0,544637.0,544706.0,544826.0,544924.0,545050.0,545062.0,545209.0,545232.0,545361.0,545473.0,545546.0,545603.0,545719.0,545792.0,545863.0,545964.0,546092.0,546190.0,546291.0,546438.0,546527.0,546657.0,546766.0,546907.0,546965.0,546958.0,546767.0,546585.0,546258.0,545754.0,545212.0,544592.0,544027.0,543439.0,542945.0,542471.0,542120.0,541747.0,541483.0,541316.0,541196.0,541159.0,541100.0,541102.0,541131.0,541214.0,541414.0,541489.0,541594.0,541813.0,541899.0,541984.0,542035.0,542053.0,542112.0,542119.0,542044.0,542040.0,542002.0,540929.0,541838.0,541449.0,541226.0,542423.0,542302.0,542401.0,542766.0,542304.0,542459.0,542599.0,542713.0,542740.0,542923.0,543084.0,543258.0,543389.0,543540.0,543697.0,543898.0,543956.0,544181.0,544312.0,545483.0,544631.0,544759.0,544923.0,545020.0,545121.0,545276.0,545368.0,545522.0,545605.0,545740.0,545856.0,545992.0,546101.0,546267.0,546397.0,546537.0,546687.0,546866.0,547027.0,547148.0,547261.0,547409.0,547415.0,547293.0,547067.0,546758.0,546363.0,545849.0,545292.0,544780.0,544265.0,543817.0,543369.0,543053.0,542790.0,542523.0,542397.0,542276.0,542224.0,542269.0,542270.0,542335.0,542387.0,542555.0,542716.0,542821.0,543000.0,543093.0,543206.0,543251.0,543352.0,543352.0,543315.0,543277.0,543225.0,543245.0,543190.0,543154.0,543163.0,543114.0,543171.0,543238.0,543281.0,543318.0,543424.0,543565.0,543600.0,543802.0,543937.0,544066.0,544145.0,544254.0,544475.0,544563.0,544777.0,543503.0,543657.0,543835.0,543872.0,544958.0,544112.0,544941.0,546161.0,546008.0,545768.0,546563.0,545995.0,546284.0,546342.0,546465.0,546620.0,546723.0,546762.0,546881.0,547045.0,547115.0,547256.0,547417.0,547490.0,547656.0,547786.0,547896.0,548083.0,548140.0,548298.0,548390.0,548373.0,548247.0,547984.0,547558.0,547063.0,546526.0,545980.0,545387.0,544846.0,544319.0,543909.0,543494.0,543205.0,543016.0,542786.0,542653.0,542540.0,542496.0,542517.0,542449.0,542527.0,542588.0,542664.0,542745.0,542872.0,542945.0,543008.0,543068.0,543115.0,543173.0,543182.0,543159.0,543078.0,543055.0,543065.0,542979.0,543032.0,543022.0,543007.0,543109.0,543153.0,543167.0,543316.0,543411.0,543496.0,543575.0,543727.0,543880.0,544029.0,544210.0,544304.0,544553.0,544668.0,544836.0,544947.0,545145.0,545259.0,545389.0,545513.0,545657.0,545790.0,545915.0,546004.0,546138.0,546232.0,546326.0,546479.0,546567.0,546737.0,546772.0,546897.0,547045.0,547153.0,547233.0,547369.0,547557.0,548787.0,548064.0,548257.0,548318.0,548051.0,548513.0,548142.0,548565.0,547190.0,547160.0,547085.0,546629.0,547595.0,547162.0,546557.0,546028.0,545388.0,544890.0,544368.0,543997.0,544529.0,543385.0,543160.0,543008.0,542919.0,542821.0,542792.0,542788.0,542732.0,542858.0,542902.0,543049.0,543163.0,543210.0,543369.0,543427.0,543424.0,543482.0,543519.0,543502.0,543473.0,543415.0,543405.0,543373.0,543318.0,543360.0,543382.0,543382.0,543424.0,543525.0,543567.0,543697.0,543774.0,543877.0,543997.0,544124.0,544242.0,544385.0,544479.0,544664.0,544779.0,544924.0,545060.0,545223.0,545361.0,545436.0,545612.0,545709.0,545790.0,545946.0,546075.0,546155.0,546261.0,546352.0,546461.0,546570.0,546680.0,546772.0,546911.0,546984.0,547071.0,547197.0,547325.0,547476.0,547586.0,547650.0,547832.0,547965.0,548061.0,548187.0,548349.0,548326.0,549869.0,549904.0,548384.0,548336.0,547940.0,546382.0,546652.0,546644.0,546089.0,545597.0,545081.0,544624.0,544289.0,543928.0,543683.0,543546.0,543389.0,543376.0,543240.0,543322.0,543356.0,543412.0,543519.0,543651.0,543796.0,543936.0,544069.0,544111.0,544194.0,544230.0,544308.0,544257.0,544251.0,544239.0,544102.0,544094.0,544026.0,544032.0,544065.0,544059.0,544086.0,544119.0,544111.0,544248.0,544333.0,544459.0,544552.0,544692.0,544837.0,544929.0,545086.0,545215.0,545412.0,545533.0,545650.0,545763.0,545897.0,546115.0,546209.0,546314.0,546385.0,546550.0,546680.0,546827.0,546839.0,546997.0,547026.0,547115.0,547256.0,547291.0,547432.0,547479.0,547635.0,547690.0,547803.0,547906.0,548058.0,548170.0,548265.0,548345.0,548318.0,548217.0,547939.0,547602.0,547063.0,546528.0,545954.0,545400.0,544763.0,544286.0,543788.0,543363.0,542990.0,542745.0,542414.0,540968.0,542208.0,541854.0,542001.0,541216.0,541268.0,541502.0,541595.0,542152.0,542262.0,542284.0,542377.0,542362.0,542376.0,542387.0,542311.0,542331.0,542177.0,542088.0,542020.0,541924.0,541858.0,541806.0,541799.0,541822.0,541916.0,541895.0,541884.0,541948.0,542055.0,542172.0,542299.0,542359.0,542482.0,542552.0,542746.0,542826.0,542953.0,543031.0,543120.0,543261.0,543336.0,543470.0,543620.0,543633.0,543760.0,543823.0,543861.0,544014.0,544051.0,544098.0,544169.0,544271.0,544341.0,544477.0,544502.0,544580.0,544676.0,544828.0,544898.0,544972.0,545065.0,545178.0,545269.0,545366.0,545356.0,545243.0,545028.0,544584.0,544091.0,543548.0,542897.0,542270.0,541567.0,540982.0,540388.0,539932.0,539505.0,539101.0,538912.0,538614.0,538502.0,538287.0,538260.0,538184.0,538219.0,538143.0,538215.0,538306.0,538365.0,538416.0,538451.0,538487.0,538538.0,538519.0,538469.0,538403.0,538346.0,538230.0,538175.0,537268.0,538638.0,538121.0,539043.0,538696.0,539035.0,538939.0,536743.0,537532.0,536881.0,537038.0,537883.0,538555.0,538025.0,539331.0,539013.0,539127.0,539232.0,539337.0,539487.0,539554.0,539689.0,539791.0,539903.0,540082.0,540119.0,540244.0,540267.0,540369.0,540425.0,540546.0,540570.0,540648.0,540661.0,540778.0,540824.0,540918.0,541007.0,541056.0,541128.0,541239.0,541366.0,541404.0,541572.0,541699.0,541800.0,541908.0,542024.0,542118.0,542023.0,541906.0,541643.0,541178.0,540671.0,540056.0,539375.0,538733.0,538064.0,537437.0,536901.0,536446.0,536092.0,535841.0,535600.0,535431.0,535272.0,535228.0,535139.0,535147.0,535200.0,535187.0,535332.0,535434.0,535575.0,535619.0,535705.0,535773.0,535805.0,535822.0,535753.0,535791.0,535701.0,535642.0,535603.0,535541.0,535598.0,535594.0,535572.0,535655.0,535663.0,535764.0,535806.0,535924.0,536003.0,536150.0,536285.0,536497.0,536588.0,536737.0,536916.0,537101.0,537223.0,537389.0,537505.0,537632.0,537752.0,537922.0,538176.0,538670.0,538728.0,539234.0,539359.0,539590.0,538754.0,538742.0,539169.0,539745.0,540019.0,539226.0,539379.0,539458.0,539632.0,539758.0,539871.0,540016.0,540114.0,540251.0,540442.0,540604.0,540667.0,540787.0,540682.0,540551.0,540266.0,539837.0,539333.0,538831.0,538289.0,537731.0,537153.0,536673.0,536246.0,535914.0,535622.0,535463.0,535313.0,535202.0,535111.0,535199.0,535254.0,535285.0,535454.0,535562.0,535793.0,536016.0,536128.0,536289.0,536377.0,536458.0,536534.0,536590.0,536564.0,536560.0,536526.0,536507.0,536465.0,536478.0,536517.0,536533.0,536643.0,536729.0,536760.0,536929.0,537016.0,537201.0,537403.0,537472.0,537656.0,537824.0,538070.0,538240.0,538363.0,538567.0,538735.0,538873.0,539060.0,539208.0,539385.0,539525.0,539717.0,539846.0,539998.0,540178.0,540294.0,540430.0,540592.0,540655.0,540761.0,540910.0,540960.0,540142.0,541730.0,542267.0,541406.0,542670.0,541809.0,542035.0,542138.0,542222.0,542320.0,542375.0,542354.0,542076.0,541747.0,541319.0,540761.0,540151.0,539621.0,539024.0,538549.0,538058.0,537662.0,537299.0,537076.0,536876.0,536751.0,536646.0,536506.0,536586.0,536572.0,536639.0,536737.0,536822.0,536585.0,537458.0,537531.0,537798.0,536717.0,536559.0,536741.0,537578.0,537591.0,537597.0,537541.0,537515.0,537576.0,537609.0,537607.0,537661.0,537719.0,537767.0,537843.0,537963.0,538072.0,538220.0,538341.0,538492.0,538694.0,538878.0,538987.0,539215.0,539368.0,539538.0,539660.0,539831.0,540035.0,540211.0,540282.0,540460.0,540541.0,540775.0,540825.0,540998.0,541050.0,541240.0,541373.0,541485.0,541615.0,541782.0,541848.0,541969.0,542096.0,542239.0,542347.0,542452.0,542603.0,542717.0,542866.0,543016.0,543157.0,543303.0,543403.0,543397.0,543394.0,543217.0,542925.0,542465.0,541997.0,541474.0,540950.0,541527.0,538813.0,539980.0,540294.0,538667.0,538351.0,538206.0,538108.0,537979.0,537884.0,537898.0,537905.0,538011.0,538056.0,538139.0,538249.0,538405.0,538503.0,538607.0,538710.0,538869.0,538916.0,538953.0,538948.0,538932.0,538958.0,538921.0,538932.0,538972.0,539028.0,539045.0,539139.0,539191.0,539223.0,539361.0,539446.0,538274.0,539760.0,539886.0,541320.0,541918.0,540342.0,540555.0,540605.0,540773.0,540998.0,541107.0,541218.0,541393.0,541485.0,541577.0,541745.0,541890.0,541952.0,542140.0,542207.0,542331.0,542450.0,542625.0,542759.0,542991.0,543460.0,544256.0,545403.0,546828.0,548273.0,549673.0,551039.0,552256.0,553309.0,554423.0,555490.0,556433.0,557221.0,557945.0,558788.0,559345.0,559755.0,559948.0,560041.0,560025.0,559984.0,560059.0,560226.0,559958.0,559710.0,559675.0,559778.0,559605.0,559120.0,558664.0,558138.0,558207.0,558339.0,558134.0,558151.0,558152.0,558125.0,558021.0,557894.0,556833.0,557342.0,557317.0,557370.0,557425.0,557620.0,557974.0,558406.0,558989.0,559447.0,559771.0,559857.0,559618.0,559521.0,559455.0,559478.0,559883.0,560867.0,562049.0,563266.0,564031.0,564330.0,564595.0,564804.0,565137.0,565649.0,566255.0,566844.0,567370.0,567898.0,568566.0,570627.0,570264.0,571684.0,573129.0,574927.0,576681.0,577843.0,579039.0,580406.0,582046.0,583337.0,584324.0,585285.0,586039.0,586745.0,587538.0,588180.0,588973.0,589714.0,590478.0,591216.0,591876.0,592381.0,592847.0,593314.0,593646.0,594024.0,594234.0,594418.0,594557.0,594522.0,594508.0,594449.0,594394.0,594116.0,593817.0,593638.0,593267.0,592898.0,592213.0,591668.0,591298.0,590918.0,590602.0,590427.0,590358.0,590348.0,590426.0,590562.0,590732.0,590945.0,591205.0,591736.0,592377.0,593098.0,593610.0,594108.0,594550.0,594895.0,595280.0,595522.0,595848.0,596127.0,596574.0,597002.0,597491.0,598132.0,598956.0,599764.0,600344.0,602000.0,600954.0,603388.0,601812.0,603308.0,602757.0,604631.0,605231.0,605882.0,606418.0,607018.0,607529.0,608012.0,608498.0,608963.0,609485.0,610024.0,610752.0,611474.0,612212.0,613013.0,613703.0,614313.0,614846.0,615295.0,615635.0,616135.0,616619.0,617264.0,617870.0,618485.0,618979.0,619456.0,619976.0,619031.0,619440.0,619811.0,620184.0,621097.0,621205.0,621057.0,623319.0,621538.0,621281.0,621122.0,620741.0,620335.0,620015.0,619640.0,619220.0,618976.0,618675.0,618488.0,618457.0,618373.0,618299.0,618342.0,618477.0,618626.0,618722.0,618911.0,619133.0,619309.0,619443.0,619708.0,619842.0,620033.0,620200.0,620404.0,620609.0,620763.0,620991.0,621183.0,621307.0,621554.0,621826.0,621960.0,622165.0,622377.0,622660.0,622953.0,623201.0,623372.0,623700.0,623996.0,624250.0,624562.0,624850.0,625259.0,625603.0,626002.0,626310.0,626707.0,627037.0,627421.0,627802.0,628210.0,628620.0,628908.0,628082.0,629974.0,629947.0,630294.0,630633.0,630903.0,631154.0,631561.0,631853.0,632162.0,632449.0,632795.0,633084.0,633479.0,633754.0,634039.0,634437.0,634767.0,635047.0,635211.0,635281.0,635235.0,635062.0,634691.0,634305.0,633870.0,633483.0,633033.0,632604.0,632854.0,632558.0,631874.0,631431.0,631237.0,629275.0,629083.0,629005.0,628860.0,629939.0,630620.0,630464.0,630627.0,630317.0,630522.0,631017.0,631011.0,631337.0,631688.0,632050.0,632412.0,632776.0,633061.0,633425.0,633708.0,634032.0,634227.0,634477.0,634739.0,635062.0,635289.0,635451.0,635520.0,635611.0,635630.0,635709.0,635586.0,635559.0,635492.0,635522.0,635486.0,635524.0,635562.0,635649.0,635754.0,635926.0,636119.0,636258.0,636396.0,636492.0,636608.0,636664.0,636748.0,636776.0,636883.0,636952.0,637155.0,637230.0,637390.0,637548.0,637642.0,637822.0,638122.0,638435.0,638655.0,638831.0,639053.0,639264.0,639480.0,639699.0,639912.0,640076.0,640119.0,640160.0,640067.0,639766.0,639333.0,638852.0,638308.0,637629.0,636937.0,636064.0,635270.0,634352.0,633480.0,632776.0,632094.0,631793.0,631372.0,630481.0,629501.0,628719.0,628332.0,627868.0,627097.0,626230.0,625693.0,625045.0,624381.0,623840.0,624224.0,623274.0,623028.0,621188.0,619102.0,618651.0,618194.0,617637.0,618088.0,617444.0,616813.0,616212.0,615546.0,614938.0,614332.0,613721.0,613229.0,612741.0,612308.0,611825.0,611245.0,610759.0,610234.0,609777.0,609353.0,608840.0,608419.0,607969.0,607561.0,607178.0,606866.0,606584.0,606218.0,606002.0,605797.0,605678.0,605688.0,605581.0,605554.0,605480.0,605351.0,605265.0,605253.0,605251.0,605176.0,605267.0,605565.0,605869.0,606122.0,606335.0,606424.0,606203.0,605865.0,605460.0,604820.0,604112.0,603388.0,602706.0,602006.0,601308.0,600668.0,600087.0,599443.0,598855.0,598254.0,597735.0,597252.0,596804.0,596483.0,596143.0,595964.0,595811.0,595551.0,595414.0,595178.0,594995.0,594691.0,594483.0,594236.0,594000.0,593853.0,593575.0,593303.0,593100.0,592846.0,592640.0,592516.0,592319.0,592200.0,591999.0,591882.0,591736.0,591726.0,591736.0,592233.0,591762.0,591749.0,592515.0,593437.0,593230.0,593093.0,593359.0,593127.0,593375.0,593545.0,593732.0,593850.0,593873.0,593969.0,594013.0,594032.0,593960.0,593815.0,593753.0,593567.0,593293.0,593041.0,592713.0,592396.0,592014.0,591595.0,591119.0,590761.0,590462.0,590108.0,589628.0,589001.0,588202.0,587444.0,586619.0,585728.0,584788.0,583998.0,583116.0,582294.0,581544.0,580824.0,580267.0,579717.0,579098.0,578533.0,578055.0,577560.0,577029.0,576682.0,576325.0,575950.0,575570.0,575212.0,574878.0,574506.0,574122.0,573692.0,573259.0,572800.0,572293.0,571711.0,571274.0,570741.0,570124.0,569634.0,569101.0,568587.0,568061.0,567575.0,567043.0,566681.0,566196.0,565728.0,565293.0,564792.0,564351.0,563912.0,563466.0,563047.0,562584.0,562117.0,561687.0,561195.0,560896.0,560522.0,560203.0,560008.0,559740.0,559728.0,559675.0,559708.0,559731.0,559786.0,559842.0,559881.0,559895.0,559999.0,560147.0,560231.0,560399.0,560542.0,560616.0,560599.0,560468.0,560174.0,559717.0,558683.0,558138.0,557539.0,556505.0,556896.0,556245.0,555928.0,555646.0,556209.0,556204.0,555859.0,555447.0,555063.0,555030.0,554930.0,555009.0,555035.0,555484.0,555146.0,555257.0,555297.0,555422.0,555420.0,555415.0,555395.0,555472.0,555464.0,555360.0,555339.0,555245.0,555187.0,555100.0,554983.0,554950.0,554882.0,554838.0,554796.0,554863.0,554784.0,554866.0,554858.0,554896.0,554927.0,554887.0,554899.0,554932.0,554926.0,554914.0,554919.0,554903.0,554918.0,554919.0,554862.0,554872.0,554771.0,554768.0,554733.0,554679.0,554711.0,554641.0,554649.0,554598.0,554542.0,554605.0,554520.0,554554.0,554567.0,554545.0,554517.0,554485.0,554373.0,554093.0,553760.0,553292.0,552778.0,552147.0,551518.0,550913.0,550404.0,549787.0,549271.0,548837.0,548412.0,548055.0,547846.0,547527.0,547375.0,547131.0,546997.0,546772.0,546651.0,545336.0,545369.0,545223.0,546863.0,546840.0,545863.0,545926.0,545446.0,545676.0,545980.0,545245.0,545012.0,544922.0,544743.0,544507.0,544363.0,544222.0,544091.0,543940.0,543821.0,543707.0,543622.0,543528.0,543534.0,543439.0,543326.0,543269.0,543259.0,543142.0,543136.0,543056.0,543038.0,543003.0,542935.0,542845.0,542853.0,542791.0,542772.0,542633.0,542648.0,542543.0,542477.0,542425.0,542359.0,542314.0,542256.0,542197.0,542063.0,542137.0,542077.0,542034.0,542045.0,541965.0,541967.0,541767.0,541621.0,541241.0,540822.0,540280.0,539733.0,539123.0,538553.0,537894.0,537371.0,536885.0,536409.0,536011.0,535671.0,535343.0,535115.0,534889.0,534663.0,534531.0,534338.0,534136.0,534066.0,533913.0,533834.0,533714.0,533578.0,533419.0,533282.0,533143.0,532988.0,532838.0,532574.0,532432.0,532225.0,532061.0,531957.0,531734.0,531566.0,531386.0,531269.0,531140.0,530991.0,530926.0,530831.0,530665.0,530654.0,530909.0,530824.0,530441.0,528972.0,528986.0,528865.0,529408.0,529942.0,529802.0,530381.0,529717.0,529925.0,529840.0,529768.0,529747.0,529631.0,529616.0,529579.0,529469.0,529449.0,529360.0,529296.0,529273.0,529194.0,529173.0,529120.0,529115.0,529088.0,529071.0,529044.0,529012.0,528996.0,528962.0,528918.0,528911.0,528749.0,528564.0,528305.0,527897.0,527426.0,526898.0,526332.0,525853.0,525358.0,524851.0,524368.0,524009.0,523675.0,523399.0,523104.0,522924.0,522756.0,522575.0,522389.0,522297.0,522112.0,521989.0,521949.0,521811.0,521754.0,521705.0,521567.0,521435.0,521404.0,521271.0,521103.0,520927.0,520784.0,520671.0,520448.0,520356.0,520253.0,520085.0,519988.0,519857.0,519764.0,519735.0,519655.0,519647.0,519587.0,519597.0,519549.0,519500.0,519476.0,519488.0,519487.0,519392.0,519388.0,519363.0,519351.0,519336.0,519317.0,519296.0,519288.0,519325.0,519238.0,519135.0,519167.0,519160.0,519099.0,519108.0,519113.0,519074.0\nArousal Valence Label: AMVM\n\n==================================================\n\nSubject ID: 7, Video ID: 4\nValence: 6\nArousal: 6\nGSR Data: 195028.421875,195278.421875,195498.46875,195720.578125,195599.078125,195685.34375,195864.4375,195874.765625,196017.140625,195950.28125,195464.09375,194437.359375,192860.515625,191304.8125,189919.0,188801.359375,188064.59375,187241.25,187217.359375,187484.875,188008.953125,188610.8125,189122.203125,189672.546875,190210.546875,190667.15625,191160.6875,191543.703125,191841.6875,192076.203125,192395.421875,192498.34375,192613.46875,192670.53125,192613.171875,192695.046875,192719.0,192749.296875,192937.625,193042.9375,193212.484375,193348.765625,193526.25,193727.890625,193843.84375,193848.03125,193988.6875,194044.0625,194218.890625,194243.671875,194293.125,194434.96875,194525.25,194541.734375,194652.859375,194700.140625,194880.765625,194962.96875,195010.046875,195058.84375,195164.046875,195154.90625,195236.5,195281.34375,195177.90625,194956.03125,194540.703125,194333.578125,194120.203125,193965.4375,193984.375,193518.046875,193330.765625,193474.65625,193724.46875,193953.09375,194169.953125,194378.65625,194511.9375,194696.296875,194827.359375,194811.234375,194916.234375,194918.875,194999.34375,195177.546875,195164.21875,195301.0,195394.25,195469.265625,195567.421875,195642.390625,195643.5625,195759.046875,195831.8125,195932.4375,195930.6875,196044.609375,196085.640625,196123.21875,196209.75,196200.4375,196248.984375,196355.078125,196395.6875,196432.28125,196472.65625,196522.59375,196700.921875,196691.8125,196680.015625,196866.6875,196876.375,196910.40625,196966.59375,196935.078125,196993.28125,197098.234375,197121.390625,197140.9375,197202.390625,197205.9375,197404.328125,197445.4375,197410.328125,197493.359375,197578.140625,197571.109375,197645.84375\nPPG Data: 549417.0,549378.0,549404.0,549342.0,549404.0,549395.0,549309.0,549292.0,549196.0,549138.0,549007.0,548969.0,548754.0,548726.0,548593.0,548503.0,548388.0,548328.0,548206.0,548172.0,548065.0,548035.0,547998.0,548022.0,547944.0,547968.0,547993.0,547946.0,548026.0,548096.0,548111.0,548072.0,548088.0,548108.0,548200.0,548320.0,548334.0,548371.0,548426.0,548523.0,548563.0,548698.0,548761.0,548795.0,548926.0,549010.0,549147.0,549174.0,549302.0,549420.0,549553.0,549602.0,549715.0,549843.0,549955.0,550069.0,550168.0,550363.0,550418.0,550469.0,550539.0,550528.0,550364.0,550100.0,549824.0,549371.0,548966.0,548466.0,548025.0,547458.0,547052.0,546589.0,546229.0,545493.0,546184.0,545506.0,545301.0,546261.0,544433.0,546151.0,543792.0,544243.0,544375.0,544335.0,544447.0,545519.0,545618.0,545675.0,545799.0,545838.0,545931.0,545982.0,546030.0,546027.0,546036.0,546102.0,546077.0,546142.0,546165.0,546249.0,546297.0,546338.0,546406.0,546497.0,546543.0,546641.0,546763.0,546882.0,546949.0,547143.0,547246.0,547493.0,547615.0,547705.0,547822.0,548019.0,548147.0,548269.0,548428.0,548561.0,548650.0,548774.0,548882.0,549069.0,549164.0,549272.0,549389.0,549567.0,549687.0,549794.0,549928.0,550097.0,550280.0,550376.0,550499.0,550517.0,550579.0,550550.0,550307.0,550028.0,549765.0,549358.0,548966.0,548530.0,548093.0,547609.0,547252.0,546909.0,546628.0,546339.0,546143.0,546097.0,545971.0,545934.0,545982.0,545983.0,546094.0,546147.0,546294.0,546418.0,546618.0,546710.0,546860.0,546983.0,547103.0,547141.0,547232.0,547284.0,547341.0,547377.0,547442.0,547478.0,547464.0,547494.0,547535.0,547635.0,548370.0,547707.0,547846.0,547918.0,548000.0,548099.0,548213.0,548376.0,548473.0,548698.0,548751.0,548910.0,549075.0,549174.0,549286.0,549475.0,549599.0,549715.0,549930.0,550031.0,550145.0,550278.0,550423.0,550536.0,550670.0,550696.0,550820.0,550991.0,551034.0,551186.0,551305.0,551350.0,551475.0,551480.0,551475.0,551462.0,551325.0,550958.0,550626.0,550163.0,549704.0,549124.0,548590.0,548046.0,547531.0,547061.0,546683.0,546242.0,545960.0,545764.0,545608.0,545509.0,545379.0,545300.0,545339.0,545358.0,545401.0,545553.0,545728.0,545996.0,546273.0,546529.0,546744.0,546938.0,547234.0,547308.0,547488.0,547599.0,547673.0,547662.0,547809.0,547808.0,547833.0,547855.0,547833.0,547866.0,547868.0,547937.0,548031.0,547969.0,548090.0,548195.0,548397.0,548610.0,548721.0,548923.0,549019.0,549148.0,549289.0,549450.0,549566.0,549777.0,549750.0,549867.0,549923.0,549983.0,550051.0,550119.0,549819.0,550388.0,550070.0,550429.0,550240.0,550440.0,550520.0,550528.0,550599.0,550633.0,550587.0,550629.0,550678.0,550602.0,550396.0,550084.0,549715.0,549259.0,548663.0,548072.0,547442.0,546845.0,546222.0,545744.0,545272.0,544921.0,544570.0,544332.0,544100.0,543913.0,543783.0,543697.0,543564.0,543528.0,543485.0,543480.0,543440.0,543454.0,543344.0,543395.0,543366.0,543335.0,543241.0,543175.0,543117.0,543011.0,542896.0,542827.0,542685.0,542610.0,542536.0,542472.0,542383.0,542425.0,542366.0,542392.0,542443.0,542485.0,542570.0,542659.0,542730.0,542793.0,542920.0,542976.0,543109.0,543189.0,543383.0,543438.0,543596.0,543681.0,543810.0,543857.0,543969.0,544001.0,544139.0,544188.0,544270.0,544422.0,544456.0,544576.0,544696.0,544783.0,544841.0,544946.0,545053.0,545096.0,545082.0,545112.0,545034.0,544822.0,544520.0,544166.0,543668.0,543105.0,542560.0,541929.0,541370.0,540859.0,540312.0,539846.0,539442.0,539148.0,538879.0,538749.0,538671.0,538528.0,538467.0,538452.0,538484.0,538535.0,538651.0,538739.0,538840.0,538866.0,539046.0,539177.0,539248.0,539383.0,539488.0,539627.0,539769.0,539859.0,539986.0,540041.0,539916.0,541570.0,540280.0,540269.0,540439.0,540420.0,540421.0,540472.0,540509.0,540636.0,540780.0,540870.0,541002.0,541136.0,541208.0,541271.0,541363.0,541518.0,541590.0,541665.0,541790.0,541864.0,541961.0,542009.0,542183.0,542179.0,542345.0,542384.0,542547.0,542592.0,542650.0,542741.0,542778.0,542760.0,542730.0,542533.0,542198.0,541789.0,541281.0,540705.0,539989.0,539386.0,538710.0,538058.0,537499.0,536957.0,536516.0,536157.0,535816.0,535572.0,535465.0,535360.0,535267.0,535226.0,535294.0,535310.0,535366.0,535382.0,535414.0,535426.0,535464.0,535457.0,535484.0,535421.0,535325.0,535230.0,535133.0,534927.0,534839.0,534705.0,534585.0,534494.0,534437.0,534407.0,534382.0,532915.0,534364.0,534309.0,534398.0,534372.0,534456.0,534449.0,534523.0,534553.0,534682.0,534722.0,534905.0,534921.0,535017.0,535066.0,535151.0,535226.0,535333.0,535410.0,535515.0,535556.0,535612.0,535696.0,535798.0,535854.0,535910.0,535918.0,535829.0,535641.0,535328.0,534860.0,534314.0,533697.0,533126.0,532447.0,531872.0,531326.0,530711.0,530181.0,529799.0,529606.0,529310.0,529124.0,528975.0,528808.0,528707.0,528772.0,528770.0,528807.0,528918.0,529070.0,529077.0,529151.0,529139.0,529109.0,529146.0,529115.0,529116.0,529131.0,529229.0,529363.0,529459.0,529645.0,529784.0,529860.0,529951.0,530045.0,530061.0,530113.0,530215.0,530217.0,530333.0,530514.0,530469.0,530442.0,530533.0,530571.0,530560.0,530548.0,530547.0,530536.0,530579.0,530578.0,530486.0,530483.0,530610.0,530729.0,530793.0,530887.0,530988.0,531068.0,531155.0,531222.0,531246.0,531390.0,531433.0,531422.0,531288.0,531023.0,530560.0,530070.0,529505.0,528781.0,528198.0,527484.0,526889.0,526334.0,525843.0,525456.0,525105.0,524699.0,524380.0,524155.0,524041.0,523874.0,523690.0,523609.0,523586.0,523436.0,523372.0,523250.0,523179.0,523142.0,522972.0,523065.0,522904.0,522677.0,522909.0,522624.0,523179.0,522249.0,522135.0,522051.0,521969.0,521903.0,521864.0,521870.0,521861.0,521944.0,521995.0,522055.0,522197.0,522306.0,522398.0,522562.0,522751.0,522861.0,522952.0,523133.0,523273.0,523378.0,523491.0,523563.0,523656.0,523822.0,523901.0,524003.0,524058.0,524247.0,524267.0,524383.0,524387.0,524540.0,524584.0,524671.0,524767.0,524792.0,524787.0,524708.0,524515.0,524126.0,523776.0,523280.0,522690.0,522106.0,521468.0,520933.0,520388.0,519912.0,519465.0,519153.0,518846.0,518661.0,518472.0,518340.0,518286.0,518187.0,518051.0,518062.0,518093.0,518088.0,518124.0,518148.0,518221.0,518242.0,518202.0,518213.0,518140.0,518109.0,518054.0,517906.0,517835.0,517737.0,517639.0,517637.0,517629.0,517700.0,517707.0,517840.0,517966.0,518046.0,518124.0,518292.0,518450.0,518627.0,518706.0,518860.0,518986.0,518872.0,519302.0,519201.0,519242.0,519859.0,520989.0,519666.0,521160.0,520180.0,520052.0,519026.0,520223.0,522043.0,520420.0,520470.0,520591.0,520648.0,520785.0,520897.0,520950.0,521082.0,521113.0,521227.0,521322.0,521389.0,521533.0,521640.0,521564.0,521417.0,521204.0,520854.0,520427.0,519955.0,519482.0,518967.0,518535.0,518106.0,517686.0,517389.0,517106.0,516931.0,516755.0,516718.0,516639.0,516684.0,516674.0,516693.0,516804.0,516946.0,517025.0,517152.0,517304.0,517427.0,517528.0,517636.0,517758.0,517862.0,517886.0,517988.0,518053.0,518049.0,518103.0,518249.0,518274.0,518371.0,518419.0,518538.0,518687.0,518817.0,518926.0,519070.0,519266.0,519355.0,519567.0,519651.0,519843.0,520026.0,520208.0,520296.0,520500.0,520616.0,520784.0,520962.0,521075.0,521271.0,521397.0,521579.0,521694.0,521867.0,522007.0,522102.0,522250.0,522482.0,522604.0,522713.0,522818.0,522947.0,523088.0,523184.0,523325.0,523466.0,523574.0,523745.0,523866.0,523949.0,524133.0,524269.0,524354.0,525045.0,525124.0,525198.0,524917.0,525243.0,520916.0,523474.0,523018.0,522514.0,522109.0,521606.0,521243.0,520804.0,520559.0,520335.0,520128.0,520025.0,519978.0,519917.0,519971.0,519961.0,520039.0,520167.0,520270.0,520389.0,520518.0,520696.0,520791.0,520878.0,521030.0,521153.0,521174.0,521198.0,521277.0,521327.0,521345.0,521426.0,521491.0,521600.0,521619.0,521697.0,521762.0,521843.0,521984.0,522099.0,522320.0,522367.0,522522.0,522755.0,522874.0,522995.0,523153.0,523254.0,523405.0,523550.0,523639.0,523841.0,523985.0,524111.0,524182.0,524296.0,524448.0,524573.0,524700.0,524791.0,524912.0,525044.0,525150.0,525237.0,525373.0,525450.0,525538.0,525681.0,525753.0,525907.0,526028.0,526096.0,526228.0,526407.0,526430.0,526543.0,526671.0,526826.0,526965.0,527023.0,527254.0,527336.0,527414.0,527606.0,527682.0,527766.0,527839.0,529133.0,528111.0,528098.0,527052.0,525536.0,526036.0,525431.0,525405.0,525164.0,524498.0,524518.0,523843.0,523510.0,523347.0,523239.0,522637.0,523072.0,522988.0,522981.0,523059.0,523027.0,523091.0,523144.0,523332.0,523453.0,523517.0,523670.0,523764.0,523845.0,523969.0,523989.0,524085.0,524139.0,524183.0,524260.0,524315.0,524390.0,524450.0,524548.0,524673.0,524690.0,524858.0,524966.0,525110.0,525267.0,525419.0,525550.0,525765.0,525929.0,526074.0,526214.0,526412.0,526568.0,526733.0,526880.0,527045.0,527167.0,527343.0,527487.0,527630.0,527824.0,527920.0,528042.0,528243.0,528355.0,528505.0,528618.0,528784.0,528893.0,529044.0,529253.0,529311.0,529473.0,529585.0,529703.0,529867.0,529980.0,530118.0,530319.0,530448.0,530541.0,530738.0,530839.0,531013.0,531180.0,531256.0,531450.0,531592.0,531683.0,531853.0,532013.0,532151.0,532291.0,532441.0,532590.0,532699.0,532842.0,532966.0,533796.0,534171.0,534095.0,533899.0,532030.0,531269.0,530743.0,530362.0,529862.0,530794.0,530608.0,530148.0,529700.0,529538.0,529366.0,529259.0,529096.0,529043.0,528098.0,528990.0,528887.0,529014.0,529041.0,529111.0,529206.0,529284.0,529318.0,529434.0,529579.0,529706.0,529806.0,529890.0,530003.0,530107.0,530253.0,530292.0,530390.0,530526.0,530684.0,530747.0,530869.0,531067.0,531204.0,531323.0,531495.0,531676.0,531873.0,532035.0,532220.0,532463.0,532593.0,532836.0,533031.0,533281.0,533447.0,533596.0,533787.0,533963.0,534148.0,534317.0,534430.0,534646.0,534759.0,534946.0,535129.0,535267.0,535443.0,535529.0,535688.0,535858.0,536000.0,536125.0,536231.0,536399.0,536468.0,536670.0,536754.0,536957.0,537068.0,537244.0,537367.0,537490.0,537648.0,537725.0,537909.0,538064.0,538160.0,538301.0,538359.0,538390.0,538257.0,538012.0,537894.0,537527.0,537392.0,536859.0,535544.0,536316.0,534832.0,534776.0,534993.0,535257.0,535369.0,534589.0,534759.0,534696.0,534713.0,534614.0,534662.0,534708.0,534742.0,534826.0,534906.0,534972.0,535151.0,535227.0,535256.0,535387.0,535443.0,535478.0,535584.0,535688.0,535654.0,535753.0,535795.0,535925.0,535980.0,536038.0,536089.0,536172.0,536239.0,536337.0,536443.0,536584.0,536691.0,536836.0,536959.0,537127.0,537171.0,537314.0,537437.0,537582.0,537672.0,537803.0,537938.0,538060.0,538220.0,538335.0,538423.0,538582.0,538642.0,538724.0,538868.0,538964.0,539051.0,539110.0,539218.0,539357.0,539445.0,539535.0,539652.0,539794.0,539864.0,539927.0,540008.0,540148.0,540243.0,540371.0,540480.0,540584.0,540683.0,540804.0,540900.0,541039.0,541131.0,541257.0,541356.0,541436.0,541446.0,541440.0,541213.0,540959.0,540683.0,540379.0,540036.0,539531.0,539163.0,538732.0,538395.0,537980.0,537719.0,536173.0,536117.0,535995.0,535686.0,537078.0,536837.0,537194.0,537310.0,536804.0,536842.0,536846.0,536896.0,536979.0,537051.0,537031.0,537091.0,537148.0,537133.0,537128.0,537072.0,537117.0,537061.0,537002.0,537021.0,536984.0,536997.0,537045.0,537048.0,537009.0,537083.0,537126.0,537138.0,537226.0,537265.0,537345.0,537435.0,537510.0,537620.0,537673.0,537747.0,537794.0,537926.0,538045.0,538067.0,538142.0,538219.0,538259.0,538351.0,538430.0,538513.0,538544.0,538586.0,538636.0,538680.0,538802.0,538892.0,538874.0,538957.0,539018.0,539076.0,539081.0,539075.0,539202.0,539304.0,539299.0,539330.0,539377.0,539458.0,539518.0,539551.0,539673.0,539692.0,539759.0,539733.0,539621.0,539407.0,539076.0,538745.0,538298.0,537799.0,537169.0,536496.0,536006.0,535426.0,534884.0,534442.0,534043.0,533656.0,533371.0,533095.0,532903.0,532687.0,532513.0,532348.0,532273.0,532248.0,532174.0,532130.0,532013.0,532002.0,532011.0,532008.0,531911.0,531839.0,531795.0,531712.0,531690.0,531022.0,530738.0,530771.0,530711.0,531118.0,531787.0,531589.0,531724.0,531161.0,531175.0,531108.0,531164.0,531119.0,531090.0,531195.0,531187.0,531237.0,531318.0,531396.0,531368.0,531449.0,531514.0,531627.0,531676.0,531673.0,531724.0,531846.0,531905.0,531938.0,531990.0,532008.0,532046.0,532139.0,532120.0,532241.0,532217.0,532309.0,532369.0,532392.0,532462.0,532530.0,532522.0,532590.0,532593.0,532643.0,532712.0,532759.0,532854.0,532907.0,533040.0,533057.0,533133.0,533217.0,533320.0,533322.0,533376.0,533276.0,533058.0,532701.0,532287.0,531754.0,531162.0,530498.0,529841.0,529111.0,528545.0,527949.0,527450.0,527052.0,526714.0,526444.0,526165.0,525968.0,525840.0,525699.0,525654.0,525574.0,525583.0,525641.0,525671.0,525725.0,525771.0,525752.0,525832.0,525858.0,525872.0,525856.0,525873.0,525814.0,525814.0,525802.0,525725.0,525693.0,525695.0,525721.0,525724.0,525774.0,525853.0,525935.0,526090.0,525631.0,525628.0,525829.0,527639.0,525764.0,525921.0,526030.0,526378.0,527683.0,527825.0,528073.0,528158.0,528355.0,528318.0,528381.0,528404.0,527981.0,528031.0,528175.0,528149.0,528380.0,528526.0,528688.0,528783.0,528925.0,529060.0,529179.0,529264.0,529328.0,529424.0,529535.0,529559.0,529603.0,529673.0,529696.0,529729.0,529735.0,529798.0,529785.0,529848.0,529927.0,530001.0,530060.0,530190.0,530267.0,530370.0,530409.0,530483.0,530482.0,530544.0,530503.0,530391.0,530160.0,529758.0,529426.0,528874.0,528308.0,527764.0,527128.0,526581.0,526002.0,525570.0,525197.0,524856.0,524620.0,524462.0,524302.0,524034.0,523832.0,523712.0,523623.0,523517.0,523514.0,523404.0,523433.0,523401.0,523366.0,523386.0,523330.0,523326.0,523227.0,523162.0,523156.0,523147.0,523111.0,523088.0,522988.0,523479.0,523530.0,523354.0,523384.0,522859.0,523190.0,523215.0,522859.0,523045.0,523154.0,522914.0,523378.0,523274.0,523375.0,523450.0,523516.0,523551.0,523621.0,523727.0,523793.0,523850.0,523992.0,524096.0,524195.0,524276.0,524449.0,524524.0,524615.0,524735.0,524821.0,524927.0,525036.0,525106.0,525229.0,525325.0,525478.0,525602.0,525669.0,525806.0,525865.0,525904.0,526042.0,526108.0,526204.0,526265.0,526402.0,526545.0,526634.0,526749.0,526887.0,526927.0,526991.0,527107.0,527154.0,527240.0,527275.0,527309.0,527275.0,527232.0,527046.0,526802.0,526469.0,526114.0,525734.0,525381.0,524914.0,524507.0,524180.0,523823.0,523654.0,523403.0,523254.0,523194.0,523106.0,523128.0,523136.0,523243.0,523282.0,523382.0,523483.0,523487.0,523629.0,523726.0,523732.0,523775.0,523869.0,524009.0,523977.0,523948.0,523997.0,523992.0,523944.0,523901.0,523894.0,523950.0,523946.0,523971.0,523990.0,524059.0,524198.0,524254.0,524362.0,524523.0,524649.0,524744.0,524905.0,525033.0,525232.0,525352.0,525628.0,525741.0,525959.0,526175.0,526346.0,526562.0,526348.0,526548.0,526803.0,527043.0,527670.0,528973.0,528894.0,527900.0,527871.0,529062.0,527834.0,529041.0,527697.0,527573.0,527619.0,527544.0,527480.0,527443.0,527475.0,527497.0,527452.0,527496.0,527469.0,527491.0,527480.0,527477.0,527480.0,527482.0,527401.0,527349.0,527009.0,526738.0,526423.0,526038.0,525572.0,525170.0,524695.0,524290.0,523889.0,523647.0,523334.0,523115.0,522956.0,522732.0,522598.0,522463.0,522364.0,522270.0,522187.0,522128.0,522070.0,522042.0,522059.0,522093.0,522071.0,522064.0,522111.0,522219.0,522295.0,522451.0,522573.0,522777.0,522983.0,523227.0,523430.0,523682.0,523975.0,524210.0,524430.0,524637.0,524813.0,524964.0,525036.0,525171.0,525260.0,525363.0,525393.0,525470.0,525553.0,525631.0,525649.0,525716.0,525722.0,524480.0,525915.0,527170.0,526187.0,527593.0,526317.0,526748.0,527984.0,527268.0,527558.0,525948.0,527726.0,526893.0,527966.0,528171.0,528362.0,528530.0,528846.0,529111.0,529324.0,529658.0,529968.0,530340.0,530650.0,531077.0,531347.0,531736.0,532102.0,532408.0,532777.0,533093.0,533410.0,533666.0,533902.0,534267.0,534536.0,534739.0,535049.0,535254.0,535507.0,535633.0,535761.0,535824.0,535775.0,535681.0,535551.0,535367.0,535070.0,534790.0,534503.0,534240.0,533999.0,533791.0,533676.0,533510.0,533465.0,533373.0,533329.0,533326.0,533322.0,533346.0,533406.0,533466.0,533537.0,533612.0,533732.0,533780.0,533924.0,534005.0,534074.0,534076.0,534265.0,534222.0,534270.0,534308.0,534312.0,534390.0,534488.0,534520.0,534541.0,534567.0,534570.0,534783.0,534796.0,534900.0,535045.0,535106.0,535286.0,535343.0,535469.0,535610.0,535718.0,535831.0,535968.0,536083.0,536178.0,536259.0,536415.0,536504.0,536650.0,535549.0,538053.0,537830.0,538152.0,536254.0,536225.0,536354.0,536382.0,537445.0,537529.0,537576.0,537708.0,537781.0,537829.0,537935.0,538019.0,538085.0,538171.0,538232.0,538336.0,538376.0,538506.0,538588.0,538653.0,538763.0,538829.0,538905.0,539022.0,539111.0,539229.0,539295.0,539320.0,539351.0,539306.0,539120.0,538893.0,538633.0,538246.0,537861.0,537366.0,536624.0,536107.0,535707.0,535263.0,535622.0,535649.0,534558.0,534312.0,534066.0,533975.0,533807.0,533742.0,533653.0,533637.0,533587.0,533549.0,533588.0,533577.0,533550.0,533541.0,533539.0,533573.0,533472.0,533475.0,533396.0,533238.0,533230.0,533100.0,533017.0,532925.0,532859.0,532783.0,532692.0,532691.0,532685.0,532652.0,532658.0,532664.0,532665.0,532702.0,532712.0,532726.0,532751.0,532805.0,532852.0,532877.0,532952.0,532996.0,533024.0,533055.0,532982.0,533057.0,533088.0,533068.0,533169.0,533150.0,533172.0,533156.0,533160.0,533223.0,535500.0,534445.0,532198.0,533710.0,533756.0,533561.0,533416.0,532814.0,532786.0,532926.0,532829.0,533322.0,533404.0,533420.0,533438.0,533463.0,533452.0,533349.0,533145.0,532789.0,532348.0,531849.0,531238.0,530565.0,529843.0,529189.0,528506.0,527859.0,527316.0,526782.0,526410.0,526016.0,525745.0,525474.0,525231.0,525048.0,524868.0,524724.0,524658.0,525332.0,525351.0,525162.0,525256.0,525346.0,523160.0,523341.0,521154.0,524117.0,523944.0,523838.0,523629.0,523509.0,523339.0,523259.0,523179.0,523056.0,523000.0,522929.0,522866.0,522884.0,522835.0,522874.0,522837.0,522862.0,522895.0,522894.0,522975.0,522986.0,523020.0,523102.0,523128.0,523188.0,523163.0,523219.0,523186.0,523297.0,523308.0,523372.0,523388.0,523345.0,523406.0,523455.0,523461.0,523506.0,523488.0,523488.0,523521.0,523550.0,523614.0,523573.0,523597.0,523634.0,523675.0,523692.0,523715.0,523814.0,523793.0,523875.0,523816.0,523657.0,522993.0,523536.0,523001.0,522470.0,520099.0,521271.0,520332.0,519259.0,518283.0,517926.0,516208.0,517123.0,516548.0,517369.0,516123.0,515753.0,515590.0,515495.0,515427.0,515367.0,515355.0,515384.0,515446.0,515442.0,515510.0,515520.0,515606.0,515557.0,515551.0,515471.0,515433.0,515362.0,515285.0,515241.0,515244.0,515186.0,515201.0,515151.0,515188.0,515147.0,515177.0,515201.0,515643.0,515383.0,515866.0,515562.0,514934.0,515558.0,515793.0,515910.0,516031.0,516124.0,516249.0,516372.0,516419.0,516537.0,516624.0,516710.0,516790.0,516883.0,516954.0,516999.0,517141.0,517169.0,517255.0,517362.0,517495.0,517504.0,517601.0,517702.0,517736.0,517811.0,517880.0,517913.0,518028.0,518114.0,518219.0,518309.0,518391.0,518500.0,518588.0,518715.0,518808.0,518865.0,518803.0,518626.0,518326.0,517907.0,517405.0,516813.0,516263.0,515618.0,515030.0,514487.0,514030.0,513611.0,513285.0,513060.0,512881.0,512863.0,512903.0,512577.0,512882.0,512859.0,512954.0,512988.0,513063.0,513207.0,513331.0,513519.0,513616.0,513730.0,513846.0,513862.0,513964.0,514014.0,514039.0,514087.0,514090.0,514109.0,514162.0,514223.0,514323.0,514375.0,514472.0,514550.0,514743.0,514866.0,515021.0,515188.0,515506.0,515671.0,515870.0,516719.0,516516.0,516725.0,517690.0,517151.0,515759.0,515824.0,517195.0,516182.0,517272.0,518147.0,517271.0,517882.0,518269.0,518443.0,518567.0,519327.0,518836.0,518977.0,519072.0,519252.0,519348.0,519467.0,519664.0,519757.0,519943.0,520044.0,520251.0,520344.0,520520.0,520728.0,520885.0,520922.0,521017.0,520831.0,520644.0,520337.0,520020.0,519568.0,519153.0,518658.0,518216.0,517710.0,517350.0,517096.0,516854.0,516713.0,516577.0,516467.0,516495.0,516448.0,516470.0,516643.0,516729.0,516789.0,516966.0,517126.0,517296.0,517441.0,517527.0,517678.0,517793.0,517901.0,518032.0,518043.0,518121.0,518182.0,520379.0,518593.0,518351.0,518374.0,518547.0,519421.0,519921.0,520142.0,519947.0,519178.0,519373.0,519483.0,519677.0,519861.0,520034.0,520180.0,520379.0,520540.0,520712.0,520922.0,521102.0,521218.0,521404.0,521553.0,521753.0,521926.0,522066.0,522165.0,522373.0,522500.0,522643.0,522766.0,522953.0,523074.0,523275.0,523381.0,523484.0,523681.0,523835.0,523958.0,524087.0,523237.0,523348.0,523523.0,523718.0,525349.0,525445.0,525341.0,525627.0,525511.0,525552.0,525495.0,525355.0,525157.0,524874.0,524612.0,524203.0,523870.0,523437.0,523120.0,522786.0,522531.0,522379.0,522242.0,522149.0,522023.0,521996.0,522028.0,521997.0,522122.0,522197.0,522228.0,522379.0,522537.0,522668.0,522811.0,522979.0,523165.0,523254.0,523423.0,523512.0,523624.0,523734.0,523854.0,523889.0,524017.0,524163.0,524215.0,524353.0,524522.0,524641.0,524739.0,524876.0,525074.0,525167.0,525407.0,525549.0,525750.0,525925.0,526054.0,525339.0,526132.0,526641.0,526843.0,527030.0,527162.0,527350.0,527556.0,527730.0,527809.0,527994.0,528179.0,528339.0,528488.0,528623.0,528745.0,528870.0,529084.0,529198.0,529374.0,529501.0,529643.0,529731.0,529929.0,530046.0,530126.0,530262.0,530460.0,530609.0,530729.0,530833.0,531053.0,531132.0,531301.0,531442.0,531580.0,531666.0,531770.0,531825.0,531746.0,531671.0,531476.0,531267.0,531019.0,530667.0,530356.0,530050.0,529750.0,529534.0,529303.0,529213.0,529132.0,529011.0,528971.0,528980.0,529043.0,529045.0,529102.0,529212.0,529318.0,529464.0,529554.0,529709.0,529901.0,530021.0,530139.0,530378.0,530429.0,530570.0,530700.0,530845.0,530898.0,530993.0,531118.0,531194.0,531258.0,531396.0,531517.0,531595.0,531705.0,531899.0,532003.0,532195.0,532317.0,532457.0,532655.0,532779.0,532986.0,533148.0,533270.0,533493.0,533633.0,533830.0,533977.0,534144.0,534309.0,534478.0,534631.0,534759.0,534938.0,535006.0,535184.0,535538.0,535453.0,535633.0,535834.0,535928.0,536012.0,536096.0,536323.0,536353.0,536521.0,536611.0,536743.0,536870.0,536981.0,537112.0,537247.0,537417.0,537541.0,537607.0,537736.0,537790.0,537768.0,537671.0,537546.0,537374.0,537059.0,536820.0,536467.0,536203.0,535874.0,535574.0,535377.0,535199.0,535078.0,534932.0,534888.0,534797.0,534893.0,534780.0,534806.0,534857.0,534891.0,534963.0,535038.0,535175.0,535217.0,535287.0,535376.0,535398.0,535397.0,535473.0,535431.0,535516.0,535498.0,535490.0,535478.0,535529.0,535519.0,535517.0,535554.0,535624.0,535696.0,535688.0,535812.0,535909.0,535975.0,536016.0,536070.0,536179.0,536323.0,536393.0,536483.0,536568.0,536574.0,536625.0,536789.0,536845.0,536945.0,536986.0,537082.0,537043.0,537160.0,537207.0,537269.0,537324.0,537346.0,537459.0,537478.0,537482.0,537540.0,537629.0,537713.0,537730.0,537771.0,537823.0,537812.0,537899.0,537941.0,538005.0,538047.0,538056.0,538201.0,538194.0,538270.0,538186.0,538101.0,537950.0,537645.0,537313.0,536838.0,536355.0,535826.0,535303.0,534744.0,534310.0,533907.0,533550.0,533188.0,532967.0,532686.0,532488.0,532324.0,532200.0,532039.0,532003.0,531859.0,531809.0,531820.0,531829.0,531795.0,531762.0,531765.0,531783.0,531710.0,531669.0,531658.0,531590.0,531509.0,531459.0,531436.0,531323.0,531315.0,531290.0,531202.0,531220.0,531175.0,531173.0,531153.0,531171.0,531179.0,531172.0,531222.0,531279.0,531328.0,531376.0,531440.0,531477.0,531543.0,531577.0,531602.0,531616.0,531699.0,531772.0,531802.0,531808.0,531818.0,531891.0,531869.0,531911.0,531993.0,532006.0,532040.0,532107.0,532110.0,532157.0,532158.0,532098.0,532161.0,532189.0,532186.0,532252.0,532247.0,532299.0,532351.0,532423.0,532404.0,532527.0,532511.0,532523.0,532614.0,532647.0,532651.0,532672.0,532754.0,532607.0,532394.0,532193.0,531705.0,531219.0,530607.0,530000.0,529311.0,528632.0,528451.0,527464.0,526929.0,526544.0,526188.0,525923.0,525572.0,525349.0,525158.0,524978.0,524869.0,524740.0,524665.0,524612.0,524557.0,524531.0,524508.0,524550.0,524504.0,524500.0,524469.0,524351.0,524204.0,524110.0,524071.0,523948.0,523855.0,523762.0,523656.0,523601.0,523540.0,523526.0,523474.0,523508.0,523470.0,523467.0,523512.0,523563.0,523560.0,523596.0,523676.0,523659.0,523712.0,523799.0,523859.0,523850.0,523962.0,523992.0,524014.0,524088.0,524148.0,524239.0,524231.0,524307.0,524379.0,524418.0,524427.0,524458.0,524506.0,524544.0,524639.0,524640.0,524724.0,524783.0,524853.0,524868.0,524996.0,525027.0,525111.0,525156.0,525257.0,525330.0,525380.0,525467.0,525526.0,525698.0,525739.0,525797.0,525921.0,526063.0,526142.0,526335.0,526322.0,526279.0,526217.0,526047.0,525608.0,525239.0,524695.0,524162.0,523654.0,523060.0,522487.0,522035.0,521612.0,521228.0,520992.0,520795.0,520675.0,520538.0,520433.0,520433.0,520470.0,520426.0,520547.0,520540.0,520618.0,520701.0,520806.0,520917.0,520944.0,520976.0,520982.0,521067.0,521017.0,520991.0,520887.0,520854.0,520859.0,520819.0,520810.0,520748.0,520716.0,520733.0,520812.0,520870.0,520863.0,521010.0,521057.0,521230.0,521279.0,521359.0,521459.0,521580.0,521729.0,521851.0,521980.0,522081.0,522248.0,522305.0,522484.0,522547.0,522614.0,522719.0,522792.0,522979.0,522965.0,523079.0,523174.0,523286.0,523389.0,523419.0,523516.0,523595.0,523670.0,523778.0,523809.0,523897.0,524026.0,524091.0,524090.0,524117.0,524028.0,523788.0,523566.0,523194.0,522734.0,522228.0,521692.0,521223.0,520664.0,520290.0,519863.0,519457.0,519240.0,519094.0,518853.0,518797.0,518681.0,518586.0,518614.0,518609.0,518638.0,518754.0,518824.0,518971.0,519027.0,519106.0,519197.0,519251.0,519267.0,519272.0,519367.0,519335.0,519342.0,519333.0,519302.0,519314.0,519334.0,519296.0,519388.0,519441.0,519288.0,519581.0,519691.0,519786.0,519837.0,519983.0,520152.0,520248.0,520399.0,520518.0,520654.0,520781.0,520883.0,521049.0,521159.0,521309.0,521405.0,521587.0,521693.0,521757.0,521849.0,522025.0,522135.0,522201.0,522219.0,522095.0,521858.0,521564.0,521294.0,521069.0,520923.0,520822.0,520730.0,520678.0,520722.0,520789.0,520882.0,521013.0,521155.0,521305.0,521467.0,521697.0,521843.0,522077.0,522264.0,522343.0,522362.0,522335.0,522135.0,521915.0,521546.0,521123.0,520744.0,520288.0,519905.0,519552.0,519218.0,518968.0,518752.0,518576.0,518529.0,518462.0,518409.0,518427.0,518466.0,518550.0,518601.0,518699.0,518839.0,518945.0,519073.0,519203.0,519282.0,519370.0,519473.0,519549.0,519601.0,519653.0,519637.0,519721.0,519717.0,519729.0,519730.0,519780.0,519800.0,519823.0,519920.0,519987.0,520061.0,520163.0,520300.0,520368.0,520469.0,520543.0,520662.0,520732.0,520932.0,521065.0,521173.0,521284.0,521335.0,521506.0,521590.0,521720.0,521793.0,521909.0,522053.0,522192.0,522243.0,522385.0,522531.0,522627.0,522737.0,522850.0,522943.0,523111.0,523180.0,523291.0,523371.0,523481.0,523649.0,523700.0,523790.0,523959.0,524120.0,524214.0,524372.0,524432.0,524548.0,524726.0,524819.0,525003.0,525088.0,525247.0,525340.0,525419.0,525382.0,525354.0,525154.0,524897.0,524612.0,524259.0,523867.0,523436.0,522983.0,522553.0,522280.0,521957.0,521767.0,521591.0,521472.0,521408.0,521403.0,521393.0,521422.0,521548.0,521559.0,521731.0,521799.0,521917.0,522064.0,522184.0,522284.0,522430.0,522530.0,522574.0,522637.0,522648.0,522691.0,522757.0,522752.0,522776.0,522810.0,522872.0,522903.0,522963.0,523040.0,523117.0,523168.0,523338.0,523462.0,523527.0,523689.0,523775.0,523881.0,524031.0,524199.0,524323.0,524365.0,524564.0,524619.0,524689.0,524856.0,524926.0,525077.0,525153.0,525255.0,525399.0,525455.0,525569.0,525669.0,525794.0,525857.0,525974.0,526090.0,526130.0,526250.0,526373.0,526435.0,526555.0,526659.0,526764.0,526920.0,527051.0,527204.0,527277.0,527323.0,527268.0,527215.0,527016.0,526770.0,526421.0\nArousal Valence Label: AMVM\n\n==================================================\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(len(data))","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:44:29.72407Z","iopub.execute_input":"2024-11-13T17:44:29.724818Z","iopub.status.idle":"2024-11-13T17:44:29.729654Z","shell.execute_reply.started":"2024-11-13T17:44:29.724761Z","shell.execute_reply":"2024-11-13T17:44:29.728764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(data_intervals_not_normalized))","metadata":{"execution":{"iopub.status.busy":"2024-11-14T11:23:17.711605Z","iopub.execute_input":"2024-11-14T11:23:17.711979Z","iopub.status.idle":"2024-11-14T11:23:17.717407Z","shell.execute_reply.started":"2024-11-14T11:23:17.711945Z","shell.execute_reply":"2024-11-14T11:23:17.716498Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2336\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save to text file and csv file","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport pandas as pd\n\n\n# Define a path to save the files in the working directory\noutput_dir = '/kaggle/working/'\n\n# Ensure the output directory exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the data_intervals to a TXT file (saving as a readable string format)\ndef save_to_txt(data_intervals, filename='data_intervals_not_normalized1.txt'):\n    file_path = os.path.join(output_dir, filename)\n    with open(file_path, 'w') as f:\n        for item in data_intervals:\n            f.write(str(item) + \"\\n\\n\")  # Each dictionary entry on a new line with a space between them\n    print(f\"Data saved to {file_path}\")\n\n# Save the data_intervals to text format\n\nsave_to_txt(data_intervals_not_normalized)\n\n\nprint(\"Data saved to .txt file in Kaggle working directory.\")\n\n\n\n# Convert the data_intervals list of dictionaries to a DataFrame\ndata_intervals_dataframe = pd.DataFrame(data_intervals_not_normalized)\n\n# Save the DataFrame to a CSV file\ndata_intervals_dataframe.to_csv('data_intervals_not_normalized1.csv', index=False)\n\nprint(\"Data saved to data_intervals_not_normalized1.csv\")\n\n# Specify the path to the CSV file\nfile_path = '/kaggle/working/data_intervals_not_normalized1.csv'\n\n# Read the CSV file into a DataFrame\ndata_intervals_not_normalized = pd.read_csv(file_path)\n\n# # Display the first few rows to verify\n# print(data_intervals_not_normalized1.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T11:24:41.297744Z","iopub.execute_input":"2024-11-14T11:24:41.298500Z","iopub.status.idle":"2024-11-14T11:24:44.607480Z","shell.execute_reply.started":"2024-11-14T11:24:41.298454Z","shell.execute_reply":"2024-11-14T11:24:44.606298Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Data saved to /kaggle/working/data_intervals_not_normalized1.txt\nData saved to .txt file in Kaggle working directory.\nData saved to data_intervals_not_normalized1.csv\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m data_intervals_not_normalized \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Display the first few rows to verify\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata_intervals_not_normalized1\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n","\u001b[0;31mNameError\u001b[0m: name 'data_intervals_not_normalized1' is not defined"],"ename":"NameError","evalue":"name 'data_intervals_not_normalized1' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Display the first few rows to verify\nprint(data_intervals_not_normalized.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:27:19.780045Z","iopub.execute_input":"2024-11-14T14:27:19.780905Z","iopub.status.idle":"2024-11-14T14:27:19.835407Z","shell.execute_reply.started":"2024-11-14T14:27:19.780864Z","shell.execute_reply":"2024-11-14T14:27:19.834306Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display the first few rows to verify\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata_intervals_not_normalized\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n","\u001b[0;31mNameError\u001b[0m: name 'data_intervals_not_normalized' is not defined"],"ename":"NameError","evalue":"name 'data_intervals_not_normalized' is not defined","output_type":"error"}]},{"cell_type":"code","source":"data_intervals_not_normalized.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-14T11:25:13.206256Z","iopub.execute_input":"2024-11-14T11:25:13.206688Z","iopub.status.idle":"2024-11-14T11:25:13.214738Z","shell.execute_reply.started":"2024-11-14T11:25:13.206644Z","shell.execute_reply":"2024-11-14T11:25:13.213656Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(2336, 9)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Normalized PPG and GSR data for each sequence & using numerical values","metadata":{}},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler\n\n# base_dir = '/kaggle/input/raw_data_ppg_gsr/'\n# data_intervals = []\n\n# # Function to load and process PPG or GSR data\n# def load_sensor_data(file_path, start_trigger, stop_trigger, value_column):\n#     data = pd.read_csv(file_path, names=['timestamp', value_column, 'start_stop_trigger'], na_values=[''])\n#     start_index = data[data['start_stop_trigger'] == start_trigger].index\n#     stop_index = data[data['start_stop_trigger'] == stop_trigger].index\n#     if not start_index.empty and not stop_index.empty:\n#         return data.loc[start_index[0]:stop_index[0], value_column].values\n#     else:\n#         return None\n\n# # Initialize scalers for PPG and GSR data\n# scaler_ppg = StandardScaler()\n# scaler_gsr = StandardScaler()\n\n# # Iterate over each subject's folder\n# for subject_id in os.listdir(base_dir):\n#     subject_path = os.path.join(base_dir, subject_id, subject_id)\n    \n#     # Check if the folder name is purely numeric (indicating a subject)\n#     if os.path.isdir(subject_path) and subject_id.isdigit():\n        \n#         # Load arousal and valence labels for this subject\n#         arousal_valence_path = os.path.join(subject_path, 'Arousal_Valence.csv')\n#         arousal_valence_df = pd.read_csv(arousal_valence_path, names=['video_id', 'valence', 'arousal', 'dominance'])\n        \n#         # Paths to GSR and PPG data files\n#         gsr_path = os.path.join(subject_path, 'raw_gsr.csv')\n#         ppg_path = os.path.join(subject_path, 'raw_ppg.csv')\n\n#         # Only process if both GSR and PPG data files exist\n#         if os.path.exists(gsr_path) and os.path.exists(ppg_path):\n\n#             for k in range(32):  # Loop over each video ID's start/stop triggers\n#                 start_trigger = k + 10\n#                 stop_trigger = k + 100\n\n#                 # Load GSR data interval\n#                 gsr_interval = load_sensor_data(gsr_path, start_trigger, stop_trigger, 'gsr_value')\n\n#                 # Load PPG data interval\n#                 ppg_interval = load_sensor_data(ppg_path, start_trigger, stop_trigger, 'ppg_value')\n\n#                 # Check if intervals are valid and retrieve labels\n#                 if gsr_interval is not None and ppg_interval is not None:\n#                     valence = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'valence'].values[0]\n#                     arousal = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'arousal'].values[0]\n\n#                     # Normalize the PPG and GSR data using the scalers\n#                     normalized_ppg = scaler_ppg.fit_transform(ppg_interval.reshape(-1, 1)).flatten()\n#                     normalized_gsr = scaler_gsr.fit_transform(gsr_interval.reshape(-1, 1)).flatten()\n\n#                     # Append data to the intervals list for model input\n#                     data_intervals.append({\n#                         'subject_id': subject_id,\n#                         'video_id': k,\n#                         'gsr_data': normalized_gsr,\n#                         'ppg_data': normalized_ppg,\n#                         'valence': valence,\n#                         'arousal': arousal\n#                     })\n\n# # Print a few samples to verify data\n# for data in data_intervals[:5]:  # Print first 5 samples as a check\n#     print(f\"Subject ID: {data['subject_id']}, Video ID: {data['video_id']}\")\n#     print(\"Valence:\", data['valence'])\n#     print(\"Arousal:\", data['arousal'])\n#     print(\"Normalized GSR Data:\", data['gsr_data'])\n#     print(\"Normalized PPG Data:\", data['ppg_data'])\n#     print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between samples\n\n# # The data_intervals list now contains input sequences and labels for multi-input LSTM\n\n# print(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T18:38:49.478198Z","iopub.execute_input":"2024-11-13T18:38:49.478903Z","iopub.status.idle":"2024-11-13T18:42:56.372784Z","shell.execute_reply.started":"2024-11-13T18:38:49.478864Z","shell.execute_reply":"2024-11-13T18:42:56.371756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(data_intervals))","metadata":{"execution":{"iopub.status.busy":"2024-11-13T18:44:55.123356Z","iopub.execute_input":"2024-11-13T18:44:55.124457Z","iopub.status.idle":"2024-11-13T18:44:55.130246Z","shell.execute_reply.started":"2024-11-13T18:44:55.124399Z","shell.execute_reply":"2024-11-13T18:44:55.129006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save data_intervals to text file to load for future use","metadata":{}},{"cell_type":"code","source":"# import json\n# import os\n\n# # Define a path to save the files in the working directory\n# output_dir = '/kaggle/working/'\n\n# # Ensure the output directory exists\n# os.makedirs(output_dir, exist_ok=True)\n\n# # # Save the data_intervals to a JSON file\n# # def save_to_json(data_intervals, filename='data_intervals.json'):\n# #     file_path = os.path.join(output_dir, filename)\n# #     with open(file_path, 'w') as f:\n# #         json.dump(data_intervals, f)\n# #     print(f\"Data saved to {file_path}\")\n\n# # Save the data_intervals to a TXT file (saving as a readable string format)\n# def save_to_txt(data_intervals, filename='data_intervals.txt'):\n#     file_path = os.path.join(output_dir, filename)\n#     with open(file_path, 'w') as f:\n#         for item in data_intervals:\n#             f.write(str(item) + \"\\n\\n\")  # Each dictionary entry on a new line with a space between them\n#     print(f\"Data saved to {file_path}\")\n\n# # Save the data_intervals to both formats\n# # save_to_json(data_intervals)\n# save_to_txt(data_intervals)\n\n# # print(\"Data saved to both .json and .txt files.\")\n# print(\"Data saved to .txt file in Kaggle working directory.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T18:48:40.68952Z","iopub.execute_input":"2024-11-13T18:48:40.689951Z","iopub.status.idle":"2024-11-13T18:48:43.701591Z","shell.execute_reply.started":"2024-11-13T18:48:40.689912Z","shell.execute_reply":"2024-11-13T18:48:43.700577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalized PPG & GSR data with ","metadata":{}},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# import json\n# from sklearn.preprocessing import StandardScaler\n\n# base_dir = '/kaggle/input/raw_data_ppg_gsr/'\n# data_intervals = []\n\n# # Function to load and process PPG or GSR data\n# def load_sensor_data(file_path, start_trigger, stop_trigger, value_column):\n#     data = pd.read_csv(file_path, names=['timestamp', value_column, 'start_stop_trigger'], na_values=[''])\n#     start_index = data[data['start_stop_trigger'] == start_trigger].index\n#     stop_index = data[data['start_stop_trigger'] == stop_trigger].index\n#     if not start_index.empty and not stop_index.empty:\n#         return data.loc[start_index[0]:stop_index[0], value_column].values\n#     else:\n#         return None\n\n# # Initialize scalers for PPG and GSR data\n# scaler_ppg = StandardScaler()\n# scaler_gsr = StandardScaler()\n\n# # Mapping function for arousal and valence levels\n# def map_level(value):\n#     if 1 <= value <= 3:\n#         return 'L'  # Low\n#     elif 4 <= value <= 6:\n#         return 'M'  # Medium\n#     elif 7 <= value <= 9:\n#         return 'H'  # High\n\n# # Iterate over each subject's folder\n# for subject_id in os.listdir(base_dir):\n#     subject_path = os.path.join(base_dir, subject_id, subject_id)\n    \n#     # Check if the folder name is purely numeric (indicating a subject)\n#     if os.path.isdir(subject_path) and subject_id.isdigit():\n        \n#         # Load arousal and valence labels for this subject\n#         arousal_valence_path = os.path.join(subject_path, 'Arousal_Valence.csv')\n#         arousal_valence_df = pd.read_csv(arousal_valence_path, names=['video_id', 'valence', 'arousal', 'dominance'])\n        \n#         # Paths to GSR and PPG data files\n#         gsr_path = os.path.join(subject_path, 'raw_gsr.csv')\n#         ppg_path = os.path.join(subject_path, 'raw_ppg.csv')\n\n#         # Only process if both GSR and PPG data files exist\n#         if os.path.exists(gsr_path) and os.path.exists(ppg_path):\n\n#             for k in range(32):  # Loop over each video ID's start/stop triggers\n#                 start_trigger = k + 10\n#                 stop_trigger = k + 100\n\n#                 # Load GSR data interval\n#                 gsr_interval = load_sensor_data(gsr_path, start_trigger, stop_trigger, 'gsr_value')\n\n#                 # Load PPG data interval\n#                 ppg_interval = load_sensor_data(ppg_path, start_trigger, stop_trigger, 'ppg_value')\n\n#                 # Check if intervals are valid and retrieve labels\n#                 if gsr_interval is not None and ppg_interval is not None:\n#                     valence = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'valence'].values[0]\n#                     arousal = arousal_valence_df.loc[arousal_valence_df['video_id'] == k, 'arousal'].values[0]\n\n#                     # Map arousal and valence to Low, Medium, High\n#                     arousal_level = map_level(arousal)\n#                     valence_level = map_level(valence)\n#                     arousal_valence_label = f\"A{arousal_level}V{valence_level}\"\n\n#                     # Normalize the PPG and GSR data using the scalers\n#                     normalized_ppg = scaler_ppg.fit_transform(ppg_interval.reshape(-1, 1)).flatten()\n#                     normalized_gsr = scaler_gsr.fit_transform(gsr_interval.reshape(-1, 1)).flatten()\n\n#                     # Append data to the intervals list for model input\n#                     data_intervals.append({\n#                         'subject_id': subject_id,\n#                         'video_id': k,\n#                         'gsr_data': normalized_gsr.tolist(),\n#                         'ppg_data': normalized_ppg.tolist(),\n#                         'valence': valence,\n#                         'arousal': arousal,\n#                         'arousal_level': arousal_level,\n#                         'valence_level': valence_level,\n#                         'arousal_valence_label': arousal_valence_label\n#                     })\n\n# # # Save data_intervals to a .txt file\n# # output_path = '/kaggle/working/data_intervals_with_categories.txt'\n# # with open(output_path, 'w') as f:\n# #     json.dump(data_intervals, f)\n\n# # Function to load data_intervals from the .txt file\n# def load_data_intervals(file_path):\n#     with open(file_path, 'r') as f:\n#         loaded_data = json.load(f)\n#     return loaded_data\n\n# # Example usage of load_data_intervals\n# loaded_data_intervals = load_data_intervals(output_path)\n# # print(\"Loaded Data Intervals Sample:\", loaded_data_intervals[:1])  # Print first sample as a check\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T02:26:37.754586Z","iopub.execute_input":"2024-11-14T02:26:37.754878Z","iopub.status.idle":"2024-11-14T02:31:01.077348Z","shell.execute_reply.started":"2024-11-14T02:26:37.754845Z","shell.execute_reply":"2024-11-14T02:31:01.075927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# No need to do this as preprocessed data is now stored in text file\n# Save to text file","metadata":{}},{"cell_type":"code","source":"# import os\n\n# # Define a path to save the files in the working directory\n# output_dir = '/kaggle/working/'\n\n# # Ensure the output directory exists\n# os.makedirs(output_dir, exist_ok=True)\n\n\n\n# # Save the data_intervals to a TXT file (saving as a readable string format)\n# def save_to_txt(data_intervals, filename='data_intervals_with_categories.txt'):\n#     file_path = os.path.join(output_dir, filename)\n#     with open(file_path, 'w') as f:\n#         for item in data_intervals:\n#             f.write(str(item) + \"\\n\\n\")  # Each dictionary entry on a new line with a space between them\n#     print(f\"Data saved to {file_path}\")\n\n\n# save_to_txt(data_intervals)\n\n# # print(\"Data saved to both .json and .txt files.\")\n# print(\"Data saved to .txt file in Kaggle working directory.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-14T02:36:04.740762Z","iopub.execute_input":"2024-11-14T02:36:04.741209Z","iopub.status.idle":"2024-11-14T02:36:10.210763Z","shell.execute_reply.started":"2024-11-14T02:36:04.741171Z","shell.execute_reply":"2024-11-14T02:36:10.209766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n\n# # Define a path to save the files in the working directory\n# output_dir = '/kaggle/working/'\n\n# # Ensure the output directory exists\n# os.makedirs(output_dir, exist_ok=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(data_intervals))","metadata":{"execution":{"iopub.status.busy":"2024-11-14T02:36:25.94938Z","iopub.execute_input":"2024-11-14T02:36:25.949776Z","iopub.status.idle":"2024-11-14T02:36:25.95495Z","shell.execute_reply.started":"2024-11-14T02:36:25.949739Z","shell.execute_reply":"2024-11-14T02:36:25.95381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# If loading from input directory [ will work only after uploading the text file as input ]","metadata":{}},{"cell_type":"code","source":"# # # Load data from JSON file\n# # def load_from_json(filename='data_intervals.json'):\n# #     with open(filename, 'r') as f:\n# #         return json.load(f)\n\n# # Load data from TXT file\n# def load_from_txt(filename='/kaggle/input/multimodal-dataset-data-intervals-categories/data_intervals_with_categories.txt'):\n#     with open(filename, 'r') as f:\n#         data = f.readlines()\n#     return [eval(item.strip()) for item in data if item.strip()]  # Convert string representations back to dictionaries\n\n# # # Example of loading the saved data\n# # data_from_json = load_from_json()  # Load data from JSON file\n# data_from_txt = load_from_txt()    # Load data from TXT file\n\n# # print(\"Data loaded from JSON:\", data_from_json[:1])  # Print first item from loaded JSON data\n# print(\"Data loaded from TXT:\", data_from_txt[:1])    # Print first item from loaded TXT data\n\n# data_intervals = data_from_txt\n\n# print(len(data_intervals))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T06:44:23.942172Z","iopub.execute_input":"2024-11-14T06:44:23.942459Z","iopub.status.idle":"2024-11-14T06:44:46.478714Z","shell.execute_reply.started":"2024-11-14T06:44:23.942426Z","shell.execute_reply":"2024-11-14T06:44:46.477767Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Data loaded from TXT: [{'subject_id': '7', 'video_id': 0, 'gsr_data': [-2.6652724181107685, -3.0635546658287867, -2.4744150535088005, -0.9200520368985203, -0.29958332558745004, 0.5803235277088639, 0.6383827530500209, 0.7912368998282177, 0.9348931013243919, 1.0512022411162112, 1.1914260386412894, 1.1400184981199364, 0.9433058561555101, 0.6281640601818226, 0.2602238148880374, 0.06682018982385093, -0.20085123488944695, -0.33478229180084856, -0.34885963488491967, -0.3509908661088029, -0.1910363542531424, -0.05400940356388928, 0.009299380791885482, 0.12070668876877332, 0.21062221240376455, 0.27673524836991203, 0.36187232726082813, 0.476364312009126, 0.5638120942269892, 0.6325274756875626, 0.7236656530246763, 0.8271761884667546, 0.884607261447188, 0.9107877544816279, 0.9647864234909652, 0.99649690070106, 0.963159957556949, 0.9068954532464305, 0.8435081498455653, 0.7273560481439268, 0.6804128761862873, 0.6370479292834834, 0.6022639923084201, 0.6063021146273568, 0.5802001406380074, 0.6801324510252501, 0.7701825787375391, 0.7585729770705961, 0.8409506723769055, 0.8544335141195775, 0.886615105600215, 0.9510119395808143, 0.916228002605751, 0.9697106893187797, 0.9890151574045856, 0.991538983853921, 1.0598954211083667, 1.0747018696111348, 0.9373720597479613, 0.55855131820593, -0.023050465785374338, -0.6534462277971641, -1.3497082516333887, -1.7642551586915687, -1.9874399358579136, -2.1314887325795397, -2.2284261022469045, -2.105667183751228, -1.940608933964689, -1.6848275360793714, -1.4631570547826278, -1.2797141314384852, -1.134409029995412, -1.0041347171839363, -0.8637090135429115, -0.7443824990183311, -0.6588528249019627, -0.5241477945460983, -0.48442837473677897, -0.36196109840858104, -0.3215125731805648, -0.27029572176871725, -0.19565776090703665, -0.2115298250217463, -0.14131136469801311, -0.13130579495220324, -0.10521503796929534, -0.007425175812377483, 0.02033691513031255, 0.11441394815509691, 0.14931005519457516, 0.21011744711389746, 0.20978093692065272], 'ppg_data': [3.2473434821985347, 3.2162188111495116, 3.1916409275479327, 3.1665144304731045, 3.1400712610624772, 3.1147618928298986, 3.0908789196277695, 3.064106582133192, 3.0408819450989624, 3.022338809703128, 3.0068679097574913, 2.997760926101549, 2.995639620671651, 2.994469245262052, 2.9997725088367977, 3.0020401111928954, 3.0056975343478927, 3.0067216128312917, 3.0082211563248404, 3.010891075227988, 3.0129758064263363, 3.0206198208202797, 3.024021224354427, 3.0289953198452224, 3.037992580806515, 3.0397481439209133, 3.0425277855187107, 3.0464046540630076, 3.0534634807521512, 3.05521904386655, 3.0665936298785903, 3.070214478802037, 3.063557968659943, 3.0673616887411397, 3.0684954899191883, 3.0673251145095897, 3.071458002674736, 3.071421428443186, 3.0753714454505827, 3.0800529470889786, 3.087843258409122, 3.089013633818721, 3.093768283920217, 3.094719213940516, 3.1013757240826103, 3.106020651489456, 3.1111410439064517, 3.116590604407397, 3.1187850583003955, 3.117797554048546, 3.116663752870497, 3.1092757580974033, 3.1067521361204555, 3.096767370907314, 3.083161756770726, 3.0662644617946406, 3.0495500379763048, 3.0278980928987234, 3.0138901622150853, 3.0006137161624467, 2.989092833224207, 2.9743168436780194, 2.96188160495103, 2.9520797108956387, 2.9463741307738434, 2.937486592507201, 2.9323662000902053, 2.929037945019158, 2.925819412642761, 2.9302083204287572, 2.935255564382653, 2.9390958586953997, 2.942131519914047, 2.9509824839491396, 2.954969075188086, 2.958516775648433, 2.960820952236081, 2.9634908711392285, 2.9659047704215267, 2.9684649666300245, 2.971317756690922, 2.9721589640165713, 2.9763650006448175, 2.979437236095015, 2.989641446697456, 2.9978340745646492, 2.9947618391144517, 2.977096485275817, 2.9566149156078345, 2.9384740967590504, 2.9256365414850114, 2.909690176529225, 2.9024484786823312, 2.8949873354461375, 2.8880016572200935, 2.8801016232053005, 2.879114118953451, 2.8787483766379514, 2.8859169260217454, 2.8929391784793395, 2.9062887729950777, 2.916236963976669, 2.9273189561363098, 2.937084275960151, 2.943375043786746, 2.9471787638679428, 2.9506533158651895, 2.94999497969729, 2.9475810804149924, 2.943448192249846, 2.9383277998328503, 2.9332805558789548, 2.929659706955508, 2.925709689948111, 2.917846230164868, 2.909690176529225, 2.8950970581407875, 2.874542340009705, 2.8517200195225247, 2.8221314661986, 2.794554495609924, 2.761857132604252, 2.7334023804583762, 2.7018753928623034, 2.6725794333907786, 2.645258482422952, 2.620790321516023, 2.5981508721865927, 2.576681798266761, 2.5653803607178207, 2.5266848237379538, 2.5153102377259136, 2.5758405909411115, 2.49629163731993, 2.485648535938889, 2.4750420087893983, 2.4638502939351077, 2.458803049981212, 2.4498423632514696, 2.4410645476794772, 2.431957564023535, 2.4220459472734936, 2.4128292409229015, 2.4026250303204604, 2.3888731192576724, 2.377717978634932, 2.364807274897793, 2.3511650865296545, 2.343850240219661, 2.3291473991365734, 2.3164927150202845, 2.3046060897665446, 2.295755125731452, 2.285587489360561, 2.277321713030268, 2.268251303605876, 2.2600586757386827, 2.2559623618050866, 2.249196128968342, 2.241186372258899, 2.2347858817376545, 2.229775212015309, 2.223777038041114, 2.218912665244968, 2.2140117182172725, 2.2080866927061775, 2.20516075418218, 2.1995283225234847, 2.194956543579739, 2.187422251880445, 2.187385677648895, 2.184313442198698, 2.1786444363084527, 2.175974517405305, 2.172463391176508, 2.167708741075012, 2.167050404907113, 2.1625152001949166, 2.155236928116473, 2.1428382636210337, 2.1298909856523447, 2.1090070994373127, 2.0857458881715325, 2.060802262254454, 2.035273448632576, 2.0079524976647494, 1.981692199411872, 1.9592356212401911, 1.9354989449642617, 1.9114696748359323, 1.8924145001983985, 1.8749320175175137, 1.8609972352969755, 1.8449777218780894, 1.8349563824333979, 1.8227405890957085, 1.8111099834628184, 1.8038682856159247, 1.7942126884867329, 1.787300158723789, 1.7823992116960932, 1.7775348388999472, 1.7716098133888523, 1.7667088663611565, 1.760235227376812, 1.755736596896166, 1.747470820565873, 1.7402656969505292, 1.7285253686229893, 1.7245387773840428, 1.7166753176007996, 1.7074586112502075, 1.6992659833830144, 1.6914756720628712, 1.6837950834373778, 1.677614038305433, 1.6700065981430396, 1.6614848021918969, 1.6592903482988988, 1.6529264320092043, 1.6493421573173073, 1.6430148152591628, 1.639284243641066, 1.6362851566539687, 1.6343101481502702, 1.6238133436954292, 1.642795369869863, 1.6367606216641182, 1.5690617190651264, 1.6534018970193538, 1.648683821149408, 1.6436365771955121, 1.6349319100866198, 1.6115975503577398, 1.6593634967619988, 1.606660029098494, 1.6051970598364953, 1.6035512194167467, 1.6022711213124978, 1.5992354600938503, 1.5936030284351552, 1.586727072903761, 1.5761936942173702, 1.560101032335384, 1.5409727092347505, 1.5223198511442664, 1.501765133013184, 1.4796377229254531, 1.4593755986467705, 1.4385648608948385, 1.4198388543412546, 1.404148509006318, 1.3861174128521834, 1.3748159753032432, 1.3613200838613047, 1.3500186463123645, 1.3418625926767214, 1.3327556090207793, 1.325075020395286, 1.3197717568205405, 1.3149805324874946, 1.309786991607399, 1.3035693722439043, 1.3013017698878062, 1.2997290779311577, 1.2952670216820614, 1.2914633016008648, 1.2868915226571187, 1.282904931418172, 1.2733590569836302, 1.2697382080601833, 1.2637766083175386, 1.2558400000711953, 1.2495126580130507, 1.2441362459752054, 1.2372968646753613, 1.2292871079659182, 1.22705607984137, 1.220289847004626, 1.216449552691879, 1.2101953590968346, 1.2075254401936868, 1.2042703335857396, 1.1995522577157938, 1.1992230896318439, 1.1959679830238967, 1.1930054702683492, 1.190993887533101, 1.1895674925026523, 1.1870072962941545, 1.1855077528006057, 1.1855809012637057, 1.181923478108709, 1.1832767246760576, 1.1812651419408093, 1.1800947665312105, 1.1776808672489125, 1.179546153057961, 1.1753401164297144, 1.1729262171474166, 1.1672937854887213, 1.1600886618733774, 1.1505427874388356, 1.1377783806278967, 1.1203690464101117, 1.1067268580419733, 1.0882202968776893, 1.0716155957540034, 1.0565835865869664, 1.0401983308525806, 1.0262635486320424, 1.0144500718414027, 1.0008078834732643, 0.9922129390590217, 0.9816429861410808, 0.9739258232840374, 0.9681105204675924, 0.9594424275902499, 0.9564433406031525, 0.9516521162701066, 0.9656600469537445, 0.9472632084841104, 0.9424354099195145, 0.9401678075634164, 0.9782415826069337, 0.9702684001290406, 0.9764860194925352, 0.9653308788697949, 0.8783207820124196, 0.8740050226895233, 0.8677874033260286, 0.863910534781732, 0.8989120743750518, 0.8950717800623051, 0.8890004576250103, 0.8517678899071424, 0.8793082862642687, 0.8751753980991223, 0.8726517761221744, 0.8696161149035271, 0.86621471136938, 0.8643494255603316, 0.8619355262780336, 0.8586804196700865, 0.8597776466165855, 0.856851708092588, 0.8570711534818878, 0.853340581863791, 0.8529016910851914, 0.8528285426220914, 0.8520604837595421, 0.850999831044593, 0.8481470409836954, 0.8508901083499432, 0.8495368617825942, 0.8494637133194943, 0.847561853278896, 0.8438312816607991, 0.8407224719790518, 0.8350534660888067, 0.8250321266441153, 0.8132917983165754, 0.7995398872537872, 0.7844713038552001, 0.7674642861844647, 0.7533466328061769, 0.7360835955145917, 0.719515468622456, 0.706677913348417, 0.6940232292321279, 0.6827217916831876, 0.6706888695032479, 0.661179569300256, 0.6533526837485628, 0.6468058963011184, 0.6405517027060738, 0.6349924195104786, 0.6306400859560323, 0.6259951585491863, 0.6214233796054401, 0.619265499943992, 0.6168881748932441, 0.6150960375472956, 0.6090247151100009, 0.6094270316570505, 0.6047089557871046, 0.6021487595786067, 0.5976501290979606, 0.5884699969789184, 0.5857635038442208, 0.5815574672159745, 0.5765102232620788, 0.5689393573312352, 0.5653185084077884, 0.5610027490848921, 0.5574550486245451, 0.5520786365866998, 0.5477994514953535, 0.5461901853071548, 0.5421304456051083, 0.5387290420709612, 0.535583658157664, 0.5353276385368141, 0.5315604926871673, 0.5281225149214703, 0.5268789910487713, 0.5244285175349235, 0.5213562820847262, 0.5210271140007764, 0.5199298870542773, 0.5150655142581315, 0.5146266234795319, 0.5140048615431825, 0.5125418922811837, 0.5100182703042359, 0.5072020544748883, 0.5069094606224885, 0.5054830655920398, 0.5037640767091912, 0.5353642127683641, 0.5333160558015658, 0.5249405567766231, 0.5073483514010881, 0.4834653781989587, 0.43116422708250357, 0.3942242532170353, 0.4611185227219279, 0.4293720897365551, 0.3728283277603037, 0.3968210236570831, 0.311895657998056, 0.3467509006651761, 0.33541288888468584, 0.3254281236715444, 0.3150776161429033, 0.3063363748024608, 0.2965710549786192, 0.289841396373425, 0.2842089647147298, 0.27784504842503527, 0.271188538282941, 0.2687746390006431, 0.2632519300365978, 0.2602894172810504, 0.25791209223030237, 0.2543278175384055, 0.2520602151823074, 0.24690324853376186, 0.2447453688723137, 0.24203887573761604, 0.23977127338151796, 0.23593097906877128, 0.23187123936672477, 0.22656797579197932, 0.22444667036208113, 0.21800960560928667, 0.21402301437034008, 0.21310865858159087, 0.20871975079559466, 0.20429426877804843, 0.20334333875774926, 0.20125860755940106, 0.19869841135090324, 0.19665025438410502, 0.1953701562798561, 0.19357801893390764, 0.19167615889330927, 0.19101782272540985, 0.19324885084995794, 0.19229792082965874, 0.19240764352430864, 0.19299283122910815, 0.19350487047080772, 0.19043263502061034, 0.18999374424201074, 0.1912006938831597, 0.19065208040991016, 0.1913104165778096, 0.18929883384256133, 0.18867707190621186, 0.1879455872752125, 0.18948170500031117, 0.18706780571801324, 0.18717752841266314, 0.18586085607686428, 0.18743354803351292, 0.18523909414051482, 0.1864094695501138, 0.183849273341616, 0.18436131258331556, 0.18450760950951545, 0.18322751140526652, 0.1831543629421666, 0.1845441837410654, 0.18366640218386615, 0.18231315561651731, 0.1837395506469661, 0.18293491755286678, 0.17957008825026968, 0.18194741330101763, 0.18264232370046704, 0.18194741330101763, 0.17982610787111947, 0.17236496463492587, 0.16505011832493216, 0.1520662661246933, 0.13930185931375427, 0.12291660357936833, 0.10792116864388121, 0.0924502686982445, 0.0727733321243614, 0.05843623335677371, 0.044135708820735996, 0.032249083566996205, 0.018789766356607764, 0.007671199965417314, -0.0011797640696750847, -0.010433044651817137, -0.01840622712971029, -0.025245608429554416, -0.030731743162049702, -0.03720538214639414, -0.045983197718386604, -0.048835987779284155, -0.05252998516583098, -0.05560222061602834, -0.03548639326354562, -0.04634894003388629, -0.04894571047393406, -0.08072871769085677, -0.09422460913279518, -0.09287136256544634, -0.09480979683759468, -0.09466349991139479, -0.12161870856372164, -0.12593446788661794, -0.13079884068276376, -0.1831365660307688, -0.10190519775828857, -0.10450196819833635, -0.10889087598433257, -0.10998810293083164, -0.11280431876017921, -0.11415756532752805, -0.11675433576757582, -0.11686405846222572, -0.11916823504987376, -0.11865619580817419, -0.12055805584877255, -0.12150898586907175, -0.12322797475192027, -0.12030203622792278, -0.12245991588937093, -0.12253306435247086, -0.12205759934232127, -0.12414233054066948, -0.12377658822516979, -0.12509326056096867, -0.12534928018181843, -0.12622706173901768, -0.1259710421181679, -0.1259710421181679, -0.12406918207756953, -0.1276168825379165, -0.12750715984326658, -0.12812892177961605, -0.12776317946411636, -0.12769003100101642, -0.1287506837159655, -0.1277266052325664, -0.12805577331651613, -0.12897012910526534, -0.12933587142076503, -0.12948216834696488, -0.13142060261911323, -0.13065254375656388, -0.12988448489401455, -0.13142060261911323, -0.13105486030361355, -0.13083541491431372, -0.12867753525286557, -0.13057939529346396, -0.1307622664512138, -0.13171319647151297, -0.1283117929373659, -0.13010393028331435, -0.12864096102131561, -0.12929929718921504, -0.12736086291706672, -0.12864096102131561, -0.127031694833117, -0.12582474519196804, -0.12553215133956827, -0.1265562298229674, -0.12567844826576816, -0.12589789365506798, -0.12487381517166884, -0.12341084590967011, -0.12341084590967011, -0.1256418740342182, -0.12483724094011889, -0.12293538089952051, -0.1245812213192691, -0.12640993289676752, -0.13277384918646207, -0.1428683370942534, -0.1533285673175444, -0.16795825993753186, -0.18309999179921885, -0.20061904871165379, -0.21872329332888824, -0.2356571625365237, -0.25156695326076006, -0.2685373966999455, -0.28228930776273364, -0.2965898322987714, -0.3089153483311108, -0.3162667688726545, -0.3695554242409587, -0.3630086367935143, -0.3768702705509524, -0.3891226381201919, -0.33480990426848856, -0.34201502788383237, -0.3611433509844659, -0.4034597368877796, -0.3699211665564584, -0.3735420154799053, -0.37569989514135343, -0.37697999324560233, -0.3823564052834477, -0.38696475845874373, -0.3900735681404911, -0.3947550697788871, -0.3975712856082346, -0.40236250994128053, -0.4071903085058764, -0.41274959170147163, -0.4191500822227161, -0.42167370419966393, -0.4255139985124106, -0.42935429282515736, -0.43228023134915483, -0.43538904103090215, -0.4391927611120989, -0.4399973942061982, -0.4409117499949474, -0.4431793523510455, -0.44434972776064446, -0.44391083698204487, -0.4443863019921944, -0.443691391592745, -0.4454835289386935, -0.44493491546544395, -0.44434972776064446, -0.4431062038879455, -0.44424000506599454, -0.44508121239164383, -0.4461418651065929, -0.44336222350879534, -0.44387426275049485, -0.4429233327301957, -0.4440937081397947, -0.443764540055845, -0.44456917314994426, -0.4419724027098965, -0.4424112934884961, -0.4412043438473472, -0.4446057473814942, -0.4420821254045464, -0.4423381450253962, -0.4394122065013987, -0.4417163830890467, -0.43992424574309824, -0.43933905803829876, -0.43868072187039936, -0.4393756322698487, -0.43692515875600085, -0.4361936741250015, -0.4368885845244509, -0.43147559825505555, -0.4341455171582032, -0.43107328170800585, -0.4318047663390052, -0.43026864861390657, -0.42902512474120763, -0.42818391741555833, -0.42668437392200964, -0.42865938242570795, -0.4253677015862108, -0.4235389900087123, -0.42375843539801217, -0.42189314958896373, -0.42116166495796437, -0.4173213706452177, -0.42083249687401464, -0.42200287228361366, -0.4259528892910103, -0.4333408840641039, -0.44413028237134466, -0.45916229153838173, -0.47185354988622086, -0.489080012946256, -0.5050995263651423, -0.5226917317406772, -0.5357853066355659, -0.5523168592961517, -0.5647520980231411, -0.5774067821394302, -0.5860017265536728, -0.5961693629245641, -0.6007411418683102, -0.6077633943259041, -0.65922333811671, -0.653298312605615, -0.605568940432906, -0.6075439489366043, -0.6011800326469098, -0.6066295931478551, -0.602094388435659, -0.6054957919698061, -0.6211861373047426, -0.6232342942715409, -0.6210764146100927, -0.6241486500602901, -0.6235268881239405, -0.6259407874062385, -0.6285741320778362, -0.6298176559505352, -0.6316829417595836, -0.632268129464383, -0.6359255526193799, -0.6368033341765792, -0.6383394519016778, -0.6380834322808281, -0.6391075107642272, -0.6399852923214264, -0.6388149169118275, -0.6396926984690267, -0.6382663034385779, -0.6357061072300801, -0.634682028746681, -0.6338408214210317, -0.6326338717798827, -0.6292324682457356, -0.6274403308997872, -0.6239657789025402, -0.6216250280833422, -0.6205278011368431, -0.6167972295187464, -0.6140541621524988, -0.6103967389975019, -0.6115305401755509, -0.6099944224504522, -0.606666167379405, -0.6036670803923077, -0.6006314191736603, -0.5990221529854616, -0.5959133433037143, -0.5954378782935646, -0.5925850882326672, -0.5882693289097709, -0.586916082342422, -0.5860383007852228, -0.5826003230195257, -0.5812836506837269, -0.5803327206634277, -0.5765290005822309, -0.5735299135951335, -0.5720303701015849, -0.5713354597021354, -0.5682632242519381, -0.5651544145701907, -0.5627405152878928, -0.5606557840895445, -0.5575835486393472, -0.5556085401356489, -0.5517316715913523, -0.5478548030470556, -0.5437584891134591, -0.5394427297905628, -0.5386015224649136, -0.5367728108874151, -0.5402473628846621, -0.5455506264594076, -0.5535238089373007, -0.5653007114963906, -0.5766752975084308, -0.5874281215841216, -0.6027892988351083, -0.6165412098978966, -0.6291958940141857, -0.6411190934994755, -0.6535177579949148, -0.6618932570198577, -0.667269669057703, -0.6749868319147464, -0.6761937815558954, -0.68237482668784, -0.6856665075273373, -0.6873123479470858, -0.6883364264304849, -0.6858128044535371, -0.6831428855503894, -0.6802169470263919, -0.6794854623953925, -0.6758646134719456, -0.6759377619350455, -0.6716951510752492, -0.6718780222329991, -0.6700858848870506, -0.6698664394977508, -0.7006253682312744, -0.6976262812441769, -0.6811678770466911, -0.7039536233023215, -0.6575043492338614, -0.6584918534857106, -0.665002066701605, -0.6552733211093134, -0.6733409914949978, -0.6697567168031009, -0.6972971131602272, -0.6651117893962548, -0.6700858848870506, -0.6665747586582536, -0.6635756716711562, -0.6624784447246571, -0.657979814244011, -0.6540663714681644, -0.652127937196016, -0.6486899594303189, -0.6442644774127728, -0.6413019646572253, -0.6385588972909777, -0.6340236925787816, -0.6333287821793322, -0.6294519136350355, -0.6286472805409362, -0.6244778181442397, -0.6214787311571424, -0.6155537056460474, -0.6144564786995483, -0.6127009155851498, -0.6081291366414038, -0.603191615382158, -0.6016554976570594, -0.5983638168175622, -0.5957304721459644, -0.5933531470952165, -0.5885984969937206, -0.584648479986324, -0.5806618887473773, -0.5753951994041819, -0.5711525885443856, -0.5684826696412378, -0.5652641372648406, -0.5609483779419443, -0.5569983609345477, -0.5552793720516992, -0.5534872347057508, -0.5498298115507538, -0.5489154557620046, -0.5455140522278575, -0.5433195983348594, -0.5416737579151109, -0.538052908991664, -0.536809385118965, -0.5355658612462662, -0.5340297435211675, -0.5408691248210116, -0.5472330411107061, -0.5539261254843504, -0.5660687703589399, -0.5806984629789274, -0.5956573236828645, -0.6104698874606018, -0.6251361543121392, -0.6411190934994755, -0.649860334839918, -0.6616738116305578, -0.6688423610143517, -0.6771812858077445, -0.6848618744332379, -0.6884461491251348, -0.6899822668502334, -0.692871631142681, -0.6956512727404787, -0.6937494126998802, -0.6915549588068821, -0.6906406030181329, -0.6848618744332379, -0.6827771432348897, -0.6805826893418916, -0.6755720196195458, -0.6734872884211976, -0.6712562602966495, -0.6716585768436992, -0.6689520837090016, -0.6705613498972002, -0.669683568340001, -0.669610419876901, -0.6688423610143517, -0.6702687560448004, -0.6685497671619519, -0.6694275487191511, -0.667269669057703, -0.6683668960042021, -0.6671599463630531, -0.6577969430862611, -0.6441547547181229, -0.6442644774127728, -0.6628441870401568, -0.6648191955438552, -0.6604302877578588, -0.656772864602862, -0.6528228475954654, -0.6539200745419644, -0.6492019986720186, -0.633621376031732, -0.6231611458084408, -0.6175652883812957, -0.6151513890989978, -0.6109087782392014, -0.6057883858222058, -0.599826786079561, -0.5941212059577659, -0.5906100797289688, -0.5860748750167728, -0.583441530345175, -0.5785771575490292, -0.574224823994583, -0.5697261935139368, -0.5672025715369889, -0.5621187533515433, -0.5587539240489462, -0.5525728789170015, -0.5517682458229022, -0.5462821110904069, -0.5429904302509098, -0.5412348671365113, -0.5386380966964635, -0.5330788135008683, -0.5283241633993724, -0.5255810960331248, -0.5231671967508268, -0.5194366251327299, -0.5154866081253334, -0.5127069665275358, -0.5087203752885892, -0.5047337840496426, -0.5020272909149449, -0.49793097698134847, -0.4948587415311511, -0.49229854532265327, -0.49493188999425103, -0.4962485623300499, -0.5026124786197445, -0.5091592660671888, -0.5177542104814314, -0.5297139841982712, -0.5419663517675106, -0.5501224054031536, -0.5646423753284912, -0.5760901098036313, -0.5843924603654742, -0.5924753655380173, -0.6003022510897106, -0.6038499515500575, -0.610177293608202, -0.6107259070814516, -0.6116402628702008, -0.6151879633305477, -0.6147124983203982, -0.6131763805952994, -0.6098115512927024, -0.6076170973997043, -0.6023504080565087, -0.6004851222474604, -0.5964253825454139, -0.5922193459171675, -0.5895860012455697, -0.5873549731210217, -0.5859651523221229, -0.583478104576725, -0.581064205294427, -0.5820517095462762, -0.5833683818820751, -0.5808447599051272, -0.5787234544752291, -0.5794183648746785, -0.578906325632979, -0.5759803871089814, -0.5772239109816804, -0.5755780705619318, -0.5722863897224346, -0.5710794400812856, -0.5688484119567375, -0.5668734034530393, -0.5639474649290418, -0.5607289325526446, -0.5582784590387966, -0.5535969574004006, -0.5481108226679053, -0.5435756179557093, -0.5418566290728607, -0.5363704943403654, -0.5337371496687677, -0.5921461974540675, -0.5188514374279305, -0.5091592660671888, -0.46867159174137357, -0.4637706447136778, -0.4616493392837796, -0.45766274804483303, -0.5498663857823038, -0.541637183683561, -0.5403570855793121, -0.5362607716457155, -0.489153161409356, -0.48439851130786005, -0.4898480718088054, -0.5292385191881216, -0.4750720822626181, -0.4695859475301228, -0.47152438180227113, -0.46523361397567653, -0.46238082391477897, -0.45835765844428245, -0.4550659776047853, -0.45418819604758603, -0.45170114830218816, -0.44819002207339115, -0.444898341233894, -0.44054600767944774, -0.4368520102929009, -0.4335603294534037, -0.42818391741555833, -0.427415858553009, -0.4243801973343616, -0.4184551718232667, -0.4163704406249185, -0.4127130174699216, -0.40949448509352443, -0.4081778127577255, -0.4078486446737758, -0.4117255132180725, -0.41629729216181854, -0.42189314958896373, -0.4310367074764559, -0.4411677696157972, -0.45243263293318753, -0.46322203124042827, -0.47320679645356967, -0.4859712032645087, -0.49431012805790153, -0.505428694449092, -0.5109879776446873, -0.5171690227766319, -0.5213384851733284, -0.5245570175497256, -0.5274829560737231, -0.524044978308026, -0.5234597906032266, -0.5228014544353271, -0.5198755159113296, -0.5169495773873322, -0.5157060535146332, -0.5104759384029877, -0.5081717618153396, -0.505465268680642, -0.5021004393780448, -0.5004545989582962, -0.4959925427092001, -0.49584624578300024, -0.49270086186970297, -0.4928837330274528, -0.4905064079767048, -0.490286962587405, -0.4902138141243051, -0.48962862641950555, -0.4897017748826055, -0.49003094296655525, -0.48842167677835663, -0.4888605675569562, -0.4861906486538085, -0.4842522143816602, -0.4817285924047124, -0.4789123765753648, -0.4777054269342158, -0.47280447990652, -0.4699151156140725, -0.46556278205962626, -0.4639900901029776, -0.45587061069888457, -0.4549928291416853, -0.4499090109562397, -0.4447520443076941, -0.44259416464624596, -0.43981452304844837, -0.4345478337052529, -0.43169504364435535, -0.42796447202625854, -0.42503853350226106, -0.4205399030216149, -0.41684590563506807, -0.41274959170147163, -0.41026254395607376, -0.4353524667993522, -0.40539817115992793, -0.40086296644773184, -0.39837591870233396, -0.39230459626503916, -0.3906587558452906, -0.3659711495490618, -0.3871110553849436, -0.35602295856747035, -0.38103973294764887, -0.3313719265027915, -0.3318839657444911, -0.3661905949383616, -0.3620211325416652, -0.36319150795126415, -0.34413633331373056, -0.35434054391617176, -0.3076352502268619, -0.3631549337197142, -0.34655023259602846, -0.3439900363875307, -0.34475809525008, -0.34728171722702783, -0.3534993365905225, -0.3594609363331674, -0.36670263418006116, -0.3763582313092529, -0.3875865203950932, -0.39848564139698384, -0.40426436998187887, -0.41168893898652253, -0.41969869569596563, -0.4252945531231108, -0.4296834609091071, -0.4347672790945527, -0.4337066263796036, -0.4377663660816501, -0.43758349492390025, -0.43633997105120137, -0.434401536779053, -0.4289519762781077, -0.42887882781500775, -0.42295380230391283, -0.41980841839061556, -0.41783340988691725, -0.41435885788967025, -0.40989680164057407, -0.4068977146534766, -0.4055810423176778, -0.4038986276663792, -0.4038620534348292, -0.4022527872466306, -0.40397177612947915, -0.4027648264883302, -0.40371575650862934, -0.4062028042540272, -0.4052153000021781, -0.4044838153711787, -0.40382547920327927, -0.4040083503610291, -0.40243565840438045, -0.40294769764608, -0.39910740333333333, -0.398339344470784, -0.3955962771045363, -0.39219487357038924, -0.38751337193199326, -0.38370965185079653, -0.38125917833694867, -0.37873555636000084, -0.3735420154799053, -0.37160358120775694, -0.3705795027243578, -0.3667757826431611, -0.3644716060555131, -0.36147251906841565, -0.3600095498064169, -0.3566447205038198, -0.3533896138958726, -0.3511951600028745, -0.3465868068275784, -0.3448312437131799, -0.3452335602602296, -0.3396742770646344, -0.33934510898068465, -0.33949140590688454, -0.33693120969838675, -0.3330177669225401, -0.32965293761994297, -0.32859228490499387, -0.32617838562269597, -0.32336216979334836, -0.32003391472230125, -0.3174005700507035, -0.3180954804501529, -0.3136334242010567, -0.3103783175931095, -0.3060991325017632, -0.30697691405896244, -0.30299032282001587, -0.3077083986899618, -0.26820822861599575, -0.3592049167123176, -0.3475743110794276, -0.3593146394069675, -0.3666660599485112, -0.34409975908218055, -0.3546697120001215, -0.3654956845389122, -0.37895500174930064, -0.3883911534891925, -0.39775415676598447, -0.40770234774757597, -0.4145417290474201, -0.41841859759171673, -0.4237950096295621, -0.42803762048935845, -0.43037837130855644, -0.4326825478962045, -0.43184134057055523, -0.42993948052995684, -0.4278547493316086, -0.42631863160650996, -0.4238315838611121, -0.42214916920981355, -0.42083249687401464, -0.4175042418029675, -0.41479774866826985, -0.41318848248007123, -0.4128593143961215, -0.4112134739763729, -0.41234727515442193, -0.41373709595332075, -0.4118352359127224, -0.41340792786937103, -0.4144685805843201, -0.4155658075308192, -0.4240144550188619, -0.3844777107133459, -0.4142857094265703, -0.41472460020516994, -0.41479774866826985, -0.41128662243947284, -0.4118718101442724, -0.4108111574293233, -0.40806809006307565, -0.4045935380658286, -0.40298427187763, -0.40086296644773184, -0.39449905015803727, -0.39281663550673873, -0.39175598279178964, -0.38623327382774436, -0.3834536322299468, -0.38078371332679906, -0.37902815021240055, -0.37548044975205364, -0.37394433202695493, -0.36973829539870856, -0.36790958382121014, -0.3637766956560637, -0.3628623398673145, -0.3605215890481165, -0.3558400874097205, -0.35510860277872114, -0.3546697120001215, -0.35342618812742255, -0.3490007061098764, -0.347098846069278, -0.34413633331373056, -0.3423441959677821, -0.340113167843234, -0.33594370544653757, -0.3343710134898889, -0.3321765595968908, -0.32979923454614285, -0.33129877803969154, -0.32738533526384495, -0.3257394948440963, -0.3251543071392968, -0.327312186800745, -0.33418814233213906, -0.33897936666518497, -0.34684282644842823, -0.3573396309032692, -0.37043320579815797, -0.3822466825887978, -0.3891592123517419, -0.40664169503262687, -0.41457830327897005, -0.42178342689431386, -0.43037837130855644, -0.4368885845244509, -0.4412774923104471, -0.4446057473814942, -0.44782427975789146, -0.44855576438889083, -0.45053077289258914, -0.44859233862044084, -0.4486289128519908, -0.4213811103472642, -0.4766813484508167, -0.42492881080761113, -0.47152438180227113, -0.4707197487081718, -0.434438111010603, -0.43601080296725164, -0.43282884482240436, -0.4333774582956539, -0.4335969036849537, -0.43345060675875385, -0.43143902402350554, -0.43436496254750306, -0.43579135757795184, -0.4398876715115483, -0.43633997105120137, -0.4370348814506507, -0.4376932176185502, -0.43692515875600085, -0.4366325649036011, -0.43403579446355334, -0.4346575563999028, -0.4330848644432541, -0.42851308549950806, -0.4279278977947086, -0.42448992002901154, -0.42167370419966393, -0.41911350799116615, -0.4151269167522196, -0.411103751281723, -0.4125301463121718, -0.40730003120052627, -0.4049227061497783, -0.40258195533058033, -0.39698609790343514, -0.3963277617355357, -0.39336524897998826, -0.3922680220334892, -0.35390165313757216, -0.39837591870233396, -0.38349020646149673, -0.38290501875669725, -0.3792475956017004, -0.37811379442365134, -0.3767239736247526, -0.37420035164780474, -0.37207904621790655, -0.36812902921050994, -0.36783643535811017, -0.36480077413946277, -0.361033628289816, -0.35997297557486696, -0.36030214365881663, -0.35763222475566897, -0.35492573162097124, -0.3536090592851724, -0.3492201514991762, -0.3511220115397745, -0.35276785195952315, -0.3555840677888707, -0.36033871789036664, -0.3689702365361592, -0.3780406459605514, -0.38919578658329185, -0.40009490758518246, -0.4124204236175219, -0.42214916920981355, -0.43235337981225475, -0.44032656229014794, -0.44903122939904044, -0.45382245373208635, -0.4605886865688305, -0.46636741515372554, -0.4678303844157243, -0.4709757683290216, -0.47196327258087073, -0.4738651326214691, -0.4707563229397218, -0.4703540063926721, -0.4692202052146231, -0.46731834517402476, -0.46530676243877644, -0.4642461097238274, -0.4598206277062812, -0.46117387427363, -0.45722385726623344, -0.46062526080038046, -0.45916229153838173, -0.45974747924318127, -0.46007664732713094, -0.46241739814632893, -0.46439240665002723, -0.4645387035762271, -0.4639169416398777, -0.46728177094247475, -0.46676973170077524, -0.467025751321625, -0.4661113955328758, -0.46442898088157725, -0.4639900901029776, -0.44789742822099143, -0.4580650645918827, -0.4523594844700876, -0.448409467462691, -0.45378587950053634, -0.45107938636583866, -0.4473122405161919, -0.44555667740179344, -0.44413028237134466, -0.44182610578369663, -0.43736404953460045, -0.43538904103090215, -0.43425523985285314, -0.4325728252015546, -0.4282204916471083, -0.4278913235631586, -0.42229546613601343, -0.4220394465151636, -0.41666303447731823, -0.41644358908801843, -0.4149074713629198, -0.41260329477527175, -0.4118718101442724, -0.4092750397042246, -0.40920189124112466, -0.405361596928378, -0.40419122151877895, -0.4025088068674804, -0.39881480948093356, -0.398302770239234, -0.39384071399013787, -0.3939138624532378, -0.36600772378061175, -0.3709818192714075, -0.3725910854596061, -0.36593457531751183, -0.4221857434413635, -0.4249653850391611, -0.4253677015862108, -0.42701354200595937, -0.43282884482240436, -0.44478861853924406, -0.4198449926221655, -0.4311830044026558, -0.44226499656229623, -0.45565116530958477, -0.46523361397567653, -0.47485263687331825, -0.48443508553941006, -0.49094529875530446, -0.49632171079314985, -0.499613391632647, -0.501990716683395, -0.5059041594592416, -0.5056847140699418, -0.5068185152479908, -0.5057944367645917, -0.501990716683395, -0.5011860835892956, -0.5000522824112467, -0.4964314334877997, -0.49420040536325166, -0.49109159568150434, -0.49153048646010394, -0.4900675171981052, -0.4899212202719053, -0.49105502144995433, -0.4911281699130543, -0.4925179907119531, -0.4942369795948016, -0.49350549496380225, -0.49398095997395186, -0.4929568814905527, -0.49478559306805114, -0.493761514584652, -0.49109159568150434, -0.4927740103328029, -0.4907624275975546, -0.4887874190938563, -0.4871050044425577, -0.48578833210675887, -0.4843619370763101, -0.48209433472021207, -0.47825404040746533, -0.476352180366867, -0.47280447990652, -0.46925677944617306, -0.4679401071103742, -0.46647713784837547, -0.4641363870291775, -0.4616493392837796, -0.4598206277062812, -0.4578090449710329, -0.45554144261493484, -0.4536030083427865, -0.45093308943963883, -0.4496529913353899, -0.44753168590549175, -0.4455201031702435, -0.44434972776064446, -0.48838510254680667, -0.3945356243895872, -0.48779991484200713, -0.4451177866231938, -0.4487752097781907, -0.4337797748427035, -0.482167483183312, -0.43388949753735345, -0.4316218951812554, -0.42865938242570795, -0.4259163150594603, -0.42807419472090846, -0.42427047463971174, -0.423941306555762, -0.4235389900087123, -0.4279278977947086, -0.43063439092940625, -0.43911961264899896, -0.44979928826158977, -0.4628197146933786, -0.47631560613531704, -0.4888605675569562, -0.5034902601769436, -0.5172421712397318, -0.5305917657554704, -0.5396987494114126, -0.5500126827085037, -0.5592293890590958, -0.5669465519161392, -0.5721766670277847, -0.576455852119131, -0.5808081856735773, -0.581064205294427, -0.5835512530398249, -0.5831855107243252, -0.5828197684088255, -0.581100779525977, -0.5794549391062285, -0.5797475329586281, -0.5744808436154327, -0.5707868462288859, -0.5775165048340801, -0.5422223713883604, -0.5763827036560311, -0.57755307906563, -0.5804790175896275, -0.5820151353147263, -0.581137353757527, -0.5838804211237746, -0.584538757291674, -0.5853433903857733, -0.5848313511440738, -0.5861114492483227, -0.585745706932823, -0.585745706932823, -0.5844656088285741, -0.5844656088285741, -0.5847947769125238, -0.5832586591874251, -0.5819054126200763, -0.5778456729180298, -0.5748465859309324, -0.5734201909004836, -0.5710062916181857, -0.5681169273257382, -0.5655567311172404, -0.5609483779419443, -0.5596682798376954, -0.5580590136494968, -0.5569617867029978, -0.5557182628302988, -0.5503784250240034, -0.550744167339503, -0.5484399907518551, -0.5460626657011071, -0.5432830241033095, -0.5435756179557093, -0.540283937116212, -0.5387112451595635, -0.536882533582065, -0.5336640012056677, -0.533407981584818, -0.5321644577121191, -0.529933429587571, -0.5282875891678224, -0.5281047180100725, -0.5261297095063743, -0.5260931352748243, -0.5243375721604258, -0.5210093170893786, -0.5187051405017306, -0.518302823954681, -0.5151208658098337, -0.5138041934740348, -0.5118657592018865, -0.5089763949094389, -0.505428694449092, -0.5064161987009411, -0.5079157421944899, -0.511243997265537, -0.5161083700616829, -0.5222894151936275, -0.5352366931623164, -0.5490251784566546, -0.5637280195397419, -0.5798938298848281, -0.5948892648203151, -0.6097384028296025, -0.6207838207576929, -0.632268129464383, -0.6423260431406245, -0.6518719175751663, -0.6576506461600613, -0.6640511366813058, -0.667306243289253, -0.6705979241287502, -0.6713659829912995, -0.6759011877034956, -0.6744016442099469, -0.6733044172634478, -0.6692446775614013, -0.669646994108451, -0.6685497671619519, -0.6658798482588042, -0.6619664054829576, -0.6628441870401568, -0.6198694649689437, -0.6207838207576929, -0.623051423113791, -0.6252458770067891, -0.6830331628557395, -0.6799243531739921, -0.6680377279202524, -0.6590404669589601, -0.6538103518473146, -0.6776201765863441, -0.6748771092200965, -0.6828868659295396, -0.683618350560539, -0.6687692125512518, -0.668476618698852, -0.6662455905743039, -0.6647826213123051, -0.6621492766407074, -0.6419968750566747, -0.6593330608113598, -0.6553830438039632, -0.6503723740816175, -0.6499700575345678, -0.6464955055373208, -0.6438621608657231, -0.6424723400668243, -0.6382663034385779, -0.6362181464717797, -0.6355598103038802, -0.632414426390583, -0.630110249802935, -0.6304759921184346, -0.6272940339735873, -0.626525975111038, -0.6229051261875911, -0.6242949469864899, -0.621844473472642, -0.618369921475395, -0.617199546065796, -0.6175287141497457, -0.6135786971423491, -0.6118962824910505, -0.6109819267023013, -0.6092629378194528, -0.6071050581580046, -0.6065198704532052, -0.6007045676367602, -0.5977054806496628, -0.5956938979144145, -0.5938651863369161, -0.5921096232225176, -0.5888545166145703, -0.5864040431007225, -0.5843924603654742, -0.5808813341366772, -0.5770410398239305, -0.5762364067298312, -0.5746637147731826, -0.5711891627759356, -0.5708965689235358, -0.5663247899797897, -0.5653738599594905, -0.5633622772242423, -0.5603631902371449, -0.559009943669796, -0.5584979044280964, -0.5606192098579946, -0.56592247343274, -0.5725058351117344, -0.5832952334189752, -0.5960230659983642, -0.6076536716312542, -0.6223199384827917, -0.6351574937568306, -0.6511404329441669, -0.6608326043049085, -0.6710002406757998, -0.6793025912376427, -0.6891044852930343, -0.69404200655228, -0.697553132781077, -0.7011008332414239, -0.7043559398493712, -0.7036976036814717, -0.7065138195108193, -0.7077573433835183, -0.70574576064827, -0.70574576064827, -0.7016494467146734, -0.6981748947174264, -0.6980651720227765, -0.696346183139928, -0.6934568188474806, -0.6939688580891801, -0.6929447796057809, -0.6920669980485817, -0.6906406030181329, -0.6927984826795811, -0.6913355134175824, -0.6923961661325314, -0.6907868999443328, -0.6915915330384321, -0.6885192975882347, -0.6876049417994855, -0.7281291903568508, -0.7159499712507112, -0.7291532688402499, -0.7100980942027162, -0.6601742681370091, -0.6820456586038903, -0.6522010856591159, -0.6630270581979066, -0.6646363243861053, -0.6622955735669073, -0.6562242511296125, -0.6517987691120662, -0.6511038587126169, -0.645142258969972, -0.6419968750566747, -0.6315366448333837, -0.6355232360723303, -0.6325241490852328, -0.6293421909403856, -0.6278060732152869, -0.6226491065667413, -0.6213690084624924, -0.6180773276229953, -0.6161023191192969, -0.6131763805952994, -0.6127740640482499, -0.6095921059034025, -0.6071050581580046, -0.6034110607714579, -0.6016554976570594, -0.5972665898710632, -0.5968276990924635, -0.5932799986321166, -0.5923656428433673, -0.5895128527824698, -0.5874281215841216, -0.5840998665130744, -0.5812836506837269, -0.5785405833174793, -0.5754317736357318, -0.5745905663100827, -0.5712988854705854, -0.5684826696412378, -0.563618296845092, -0.5631062576033925, -0.5585710528911964, -0.5552427978201492, -0.549683514624554, -0.5477450803524057, -0.5440510829658588, -0.5428807075562598, -0.5365167912665653, -0.5358950293302158, -0.5365899397296653, -0.5359316035617658, -0.5402107886531121, -0.5467210018690065, -0.5561937278404484, -0.5653007114963906, -0.5798938298848281, -0.591670732443918, -0.6023504080565087, -0.6151513890989978, -0.626525975111038, -0.6358158299247301, -0.6447765166544723, -0.6512135814072668, -0.6582724080964107, -0.6583089823279608, -0.6601011196739092, -0.6633562262818564, -0.6605034362209589, -0.6612349208519582, -0.6585650019488105, -0.6547247076360638, -0.6540663714681644, -0.6500797802292178, -0.6437524381710732, -0.6406070542577759, -0.6403144604053762, -0.6382297292070279, -0.6353769391461304, -0.6339139698841316, -0.632377852159033, -0.6325607233167828, -0.6324875748536829, -0.6316829417595836, -0.6309148828970342, -0.6304394178868846, -0.6324510006221329, -0.6313171994440839, -0.629963952876735, -0.6310611798232341, -0.6285375578462863, -0.6251361543121392, -0.6320852583066332, -0.662624741650857, -0.6007045676367602, -0.6599548227477093, -0.6088606212724031, -0.6039962484762573, -0.5784674348543792, -0.6024601307511587, -0.5908660993498186, -0.5882327546782209, -0.5839901438184245, -0.5568520640083479, -0.57755307906563, -0.5712988854705854, -0.5664710869059896, -0.5641669103183415, -0.5600705963847451, -0.5564863216928482, -0.5527557500747513, -0.5475622091946558, -0.5449288645230581, -0.5435756179557093, -0.5379066120654641, -0.5340663177527174, -0.5301528749768708, -0.5302260234399707, -0.5254347991069248, -0.5213384851733284, -0.5208630201631788, -0.517132448545082, -0.5146454007996841, -0.5111708488024371, -0.5081351875837897, -0.5044411901972429, -0.4985527389176979, -0.49500503845735094, -0.4920425257018035, -0.489080012946256, -0.4849471247811096, -0.48220405741486194, -0.4793512673539644, -0.4741577264738689, -0.4709757683290216, -0.46812297826812405, -0.46428268395537736, -0.4678303844157243, -0.4703905806242221, -0.4770470907663164, -0.4836304524453107, -0.4945295734472014, -0.5037462797977934, -0.5150842915782837, -0.5281412922416225, -0.5392964328643629, -0.5491714753828545, -0.5565594701559481, -0.5644229299391913, -0.5704211039133862, -0.5766021490453309, -0.5783577121597294, -0.5818688383885263, -0.5831123622612253, -0.5830757880296753, -0.5855994100066232, -0.5818688383885263, -0.5794915133377784, -0.5782845636966294, -0.5752123282464321, -0.5709331431550857, -0.5689947088829375, -0.5667271065268393, -0.5621187533515433, -0.5599974479216452, -0.5614604171836439, -0.5586442013542963, -0.5596317056061455, -0.5597048540692454, -0.5587539240489462, -0.5585710528911964, -0.5593025375221957, -0.5587173498173963, -0.5571080836291976, -0.5560840051457985, -0.5550599266623993, -0.5538529770212505, -0.5497932373192039, -0.5489154557620046, -0.5440510829658588, -0.5420760744621606, -0.5381260574547639, -0.5380894832232139, -0.5331885361955182, -0.5276658272314729, -0.5237158102240763, -0.5203875551530291, -0.5155231823568833, -0.5129995603799355, -0.5109514034131373, -0.5047337840496426, -0.5311038049971699, -0.529933429587571, -0.5241181267711259, -0.5277389756945728, -0.4971994923503491, -0.493834663047752, -0.4910184472184044, -0.48743417252650745, -0.47207299527552066, -0.4691104825199732, -0.46592852437512594, -0.45843080690738236, -0.4642461097238274, -0.45773589650793295, -0.4567483922560838, -0.45181087099683803, -0.4511891090604886, -0.44811687361029123, -0.4452640835493937, -0.4432525008141454, -0.4387172961019493, -0.4365959906720511, -0.43290199328550427, -0.4301955001508066, -0.42624548314341004, -0.4228806538408129, -0.4199912895483654, -0.4175042418029675, -0.4153097879099694, -0.41092088012397315, -0.41062828627157344, -0.4051421515390781, -0.405288448465278, -0.4030939945722799, -0.4059833588647274, -0.4127861659330216, -0.41863804298101653, -0.4272695616268091, -0.43758349492390025, -0.44570297432799333, -0.4579187676656828, -0.46826927519432393, -0.47942441581706435, -0.4877633406104572, -0.4954439292359506, -0.5018078455256452, -0.5060138821538915, -0.5098541764666382, -0.5144259554103843, -0.5145356781050342, -0.5155963308199832, -0.5157792019777331, -0.5156329050515333, -0.5130727088430355, -0.5114634426548368, -0.5071111091003906, -0.5049898036704924, -0.5016249743678953, -0.49866246161234784, -0.4970897696556992, -0.49431012805790153, -0.4932129011114025, -0.492627713406603, -0.48977492334570544, -0.4902138141243051, -0.487946211768207, -0.4890068644831561, -0.4871781529056577, -0.4869587075163579, -0.48571518364365895, -0.4836304524453107, -0.48107025623681293, -0.47964386120636415, -0.4790952477331146, -0.47638875459841695, -0.4744503203262686, -0.4816920181731624, -0.47342624184286947, -0.4324265282753547, -0.42957373821445716, -0.5003083020320964, -0.4965045819508997, -0.4924814164804031, -0.48523971863350934, -0.4388270187965992, -0.4374737722292504, -0.43297514174860424, -0.4305246682347563, -0.4263552058380599, -0.4238315838611121, -0.4192232306858161, -0.4163338663933685, -0.41388339287952064, -0.408799574694075, -0.4075194765898261, -0.4049592803813283, -0.4023990841728305, -0.399473145648833, -0.39742498868203474, -0.39449905015803727, -0.39095134969769035, -0.3892323608148418, -0.3868184615325439, -0.38381937454544646, -0.38023509985354953, -0.37836981404450115, -0.3764313797723528, -0.3728471050804559, -0.3705063542612579, -0.3675804157372604, -0.3644350318239631, -0.4116523647549725, -0.4117255132180725, -0.4077754962106759, -0.40583706193852753, -0.33268859883859037, -0.327275612569195, -0.3518169219392239, -0.32200892322599955, -0.35009793305637543, -0.3396742770646344, -0.31758344120845333, -0.3499882103617255, -0.3270927414114452, -0.35829056092356837, -0.3497687649724257, -0.37730916132955206, -0.38751337193199326, -0.395998593651586, -0.40616623002247726, -0.41410283826882044, -0.42189314958896373, -0.42880567935190783, -0.43228023134915483, -0.4364862679774012, -0.4393756322698487, -0.4379492372394, -0.4394487807329487, -0.43758349492390025, -0.4368154360613509, -0.4373274753030505, -0.43282884482240436, -0.42913484743585756, -0.42547742428086066, -0.4244533457974616, -0.41980841839061556, -0.41874776567566646, -0.4145051548158701, -0.4135542247955709, -0.4115060678287727, -0.40942133663042446, -0.40843383237857533, -0.4077389219791259, -0.40850698084167525, -0.4072634569689763, -0.4070440115796765, -0.40894587162027485, -0.4063491011802271, -0.40433751844497884, -0.4026185295621303, -0.4027282522567802, -0.40108241183703164, -0.39815647331303416, -0.3964374844301856, -0.3935481201377381, -0.3891592123517419, -0.3871842038480436, -0.38451428494489587, -0.37939389252790023, -0.3788452790546507, -0.37474896512105427, -0.37138413581845714, -0.3681656034420599, -0.36319150795126415, -0.3601558467326168, -0.35726648244016923, -0.35434054391617176, -0.3525849808017733, -0.34925672573072614, -0.34556272834417934, -0.34446550139768023, -0.3391622378229348, -0.3391622378229348, -0.33550481466793797, -0.3314816491974414, -0.32767792911624466, -0.32584921753874624, -0.32500801021309694, -0.3208751220479505, -0.31608389771490464, -0.315608432704755, -0.31253619725455767, -0.30986627835140995, -0.3089153483311108, -0.3039046786087651, -0.3001375327591183, -0.29578519920467206, -0.2915791625764257, -0.2883972044315784, -0.27914392384943637, -0.2796925373226859, -0.273767511811591, -0.27131703829774306, -0.26564803240749796, -0.26798878322669595, -0.26367302390379965, -0.26645266550159724, -0.23046362165642817, -0.3324325792177406, -0.2915060141133257, -0.3505002496034251, -0.30950053603591027, -0.31977789510145144, -0.3242399513505476, -0.3345173104160888, -0.3410275236319832, -0.34527013449177957, -0.349403022656926, -0.35540119663112085, -0.3558400874097205, -0.35913176824921766, -0.3601192725010668, -0.35602295856747035, -0.35510860277872114, -0.35609610703057026, -0.35207294156007374, -0.3510488630766746, -0.349403022656926, -0.34845209263662685, -0.34516041179712964, -0.3420881763469323, -0.34201502788383237, -0.3409543751688833, -0.3395279801384345, -0.3398571482223842, -0.33843075319193544, -0.33890621820208505, -0.3396377028330844, -0.3397474255277343, -0.3411372463266331, -0.340076593611684, -0.34062520708493355, -0.3393816832122346, -0.33729695201388643, -0.3378821397186859, -0.336602041614437, -0.33495620119468844, -0.33418814233213906, -0.3322862822915407, -0.32936034376754325, -0.32442282250829746, -0.32365476364574813, -0.32237466554149924, -0.3197413208699015, -0.3165593627250542, -0.3142551861374062, -0.3113292476134087, -0.30957368449901024, -0.3050384797868141, -0.3047093117028644, -0.3015639277895671, -0.2988574346548694, -0.2967361292249712, -0.29381019070097375, -0.29092082640852623, -0.28920183752567774, -0.28722682902197944, -0.28345968317233267, -0.28283792123598317], 'valence': 4, 'arousal': 5, 'arousal_level': 'M', 'valence_level': 'M', 'arousal_valence_label': 'AMVM'}]\n2336\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Convert to dataframe and csv file","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# # Convert the data_intervals list of dictionaries to a DataFrame\n# data_intervals_dataframe = pd.DataFrame(data_intervals)\n\n# # Save the DataFrame to a CSV file\n# data_intervals_dataframe.to_csv('data_intervals.csv', index=False)\n\n# print(\"Data saved to data_intervals.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T06:54:16.612626Z","iopub.execute_input":"2024-11-14T06:54:16.613001Z","iopub.status.idle":"2024-11-14T06:54:27.588920Z","shell.execute_reply.started":"2024-11-14T06:54:16.612965Z","shell.execute_reply":"2024-11-14T06:54:27.587995Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Data saved to data_intervals.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LOAD THE CSV FILE HERE","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to the CSV file\nfile_path = '/kaggle/input/data-intervals-not-normalized-with-labels/data_intervals_not_normalized1.csv'\n\n# Read the CSV file into a DataFrame\ndata_intervals_dataframe = pd.read_csv(file_path)\n\n# Display the first few rows to verify\nprint(data_intervals_dataframe.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:41:50.302162Z","iopub.execute_input":"2024-11-14T14:41:50.302538Z","iopub.status.idle":"2024-11-14T14:41:50.843120Z","shell.execute_reply.started":"2024-11-14T14:41:50.302500Z","shell.execute_reply":"2024-11-14T14:41:50.842031Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"   subject_id  video_id                                           gsr_data  \\\n0           7         0  179125.171875,178570.375,179391.03125,181556.2...   \n1           7         1  182172.671875,182666.59375,183080.09375,184354...   \n2           7         2  168857.21875,168781.234375,169132.53125,169823...   \n3           7         3  178895.96875,178088.4375,177970.546875,177919....   \n4           7         4  195028.421875,195278.421875,195498.46875,19572...   \n\n                                            ppg_data  valence  arousal  \\\n0  612293.0,611442.0,610770.0,610083.0,609360.0,6...        4        5   \n1  551232.0,551091.0,550905.0,550746.0,550549.0,5...        3        5   \n2  605678.0,605853.0,606119.0,606351.0,606579.0,6...        4        5   \n3  565274.0,565598.0,566007.0,566224.0,566272.0,5...        5        6   \n4  549417.0,549378.0,549404.0,549342.0,549404.0,5...        6        6   \n\n  arousal_level valence_level arousal_valence_label  \n0             M             M                  AMVM  \n1             M             L                  AMVL  \n2             M             M                  AMVM  \n3             M             M                  AMVM  \n4             M             M                  AMVM  \n","output_type":"stream"}]},{"cell_type":"code","source":"print(data_intervals_dataframe.arousal_valence_label.nunique())\nprint(data_intervals_dataframe.valence_level.nunique())\nprint(data_intervals_dataframe.arousal_level.nunique())\n\nprint(data_intervals_dataframe.valence_level.nunique())\nprint(data_intervals_dataframe.arousal_level.nunique())\n\nprint(data_intervals_dataframe.valence.nunique())\nprint(data_intervals_dataframe.arousal.nunique())\n\nprint(data_intervals_dataframe.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:14:38.566063Z","iopub.execute_input":"2024-11-14T14:14:38.566422Z","iopub.status.idle":"2024-11-14T14:14:38.581700Z","shell.execute_reply.started":"2024-11-14T14:14:38.566383Z","shell.execute_reply":"2024-11-14T14:14:38.580616Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"9\n3\n3\n3\n3\n9\n9\n(2336, 9)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"subject id:\")\nprint(data_intervals_dataframe.subject_id.unique())\nprint(\"=================\")\nprint(\"Video id:\")\nprint(data_intervals_dataframe.video_id.unique())","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:14:42.623978Z","iopub.execute_input":"2024-11-14T14:14:42.624756Z","iopub.status.idle":"2024-11-14T14:14:42.631549Z","shell.execute_reply.started":"2024-11-14T14:14:42.624712Z","shell.execute_reply":"2024-11-14T14:14:42.630498Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"subject id:\n[ 7 47 19 22  2 35 50 23 10  5 61 36 20 45 60 64 41 39 32 25 42 52 75  8\n 38 12 55 49 62 53 70 34 18 79 65 67 78 28 66 56 72 26 74 15 69 77 43 71\n  1 58 59 30 14 76 57  9 46 21 44 40 80  6 11 68 63 37 51 33 54 48 29 24\n 73]\n=================\nVideo id:\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# For PPG data extract features based on these steps :\n\n1. Filter data with savgol filter","metadata":{}},{"cell_type":"code","source":"from scipy.signal import savgol_filter\nimport pandas as pd\n\n# Assume data_intervals_dataframe is already loaded from the CSV\n# Convert ppg_data column from string representations of lists to actual lists\n# data_intervals_dataframe['ppg_data'] = data_intervals_dataframe['ppg_data'].apply(eval)\n# Check if the value is a string and apply eval only in that case\ndata_intervals_dataframe['ppg_data'] = data_intervals_dataframe['ppg_data'].apply(\n    lambda x: eval(x) if isinstance(x, str) else x\n)\n\n# Apply Savitzky-Golay filter to each entry in the 'ppg_data' column\npreprocessed_dataframe = data_intervals_dataframe.copy()\npreprocessed_dataframe['ppg_filtered_data'] = preprocessed_dataframe['ppg_data'].apply(\n    lambda x: savgol_filter(x, window_length=5, polyorder=3) if len(x) >= 5 else x\n)\n\n# Drop the original 'ppg_data' column if you no longer need it\npreprocessed_dataframe = preprocessed_dataframe.drop(columns=['ppg_data'])\n\n# Save the new dataframe to a CSV if needed\npreprocessed_dataframe.to_csv('/kaggle/working/preprocessed_data_intervals.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:41:59.272003Z","iopub.execute_input":"2024-11-14T14:41:59.272376Z","iopub.status.idle":"2024-11-14T14:42:15.560571Z","shell.execute_reply.started":"2024-11-14T14:41:59.272339Z","shell.execute_reply":"2024-11-14T14:42:15.559739Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(preprocessed_dataframe)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:42:43.552307Z","iopub.execute_input":"2024-11-14T14:42:43.552814Z","iopub.status.idle":"2024-11-14T14:42:43.570229Z","shell.execute_reply.started":"2024-11-14T14:42:43.552772Z","shell.execute_reply":"2024-11-14T14:42:43.569144Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"      subject_id  video_id                                           gsr_data  \\\n0              7         0  179125.171875,178570.375,179391.03125,181556.2...   \n1              7         1  182172.671875,182666.59375,183080.09375,184354...   \n2              7         2  168857.21875,168781.234375,169132.53125,169823...   \n3              7         3  178895.96875,178088.4375,177970.546875,177919....   \n4              7         4  195028.421875,195278.421875,195498.46875,19572...   \n...          ...       ...                                                ...   \n2331          73        27  235083.5,235510.375,235109.140625,234961.0,234...   \n2332          73        28  234302.875,235212.921875,234673.390625,234568....   \n2333          73        29  234887.546875,234301.828125,234304.59375,23398...   \n2334          73        30  234102.984375,233839.90625,233937.96875,234354...   \n2335          73        31  233566.328125,233756.46875,233891.53125,234115...   \n\n      valence  arousal arousal_level valence_level arousal_valence_label  \\\n0           4        5             M             M                  AMVM   \n1           3        5             M             L                  AMVL   \n2           4        5             M             M                  AMVM   \n3           5        6             M             M                  AMVM   \n4           6        6             M             M                  AMVM   \n...       ...      ...           ...           ...                   ...   \n2331        3        5             M             L                  AMVL   \n2332        7        3             L             H                  ALVH   \n2333        5        5             M             M                  AMVM   \n2334        5        1             L             M                  ALVM   \n2335        3        5             M             L                  AMVL   \n\n                                      ppg_filtered_data  \n0     [612290.5285714285, 611451.8857142855, 610755....  \n1     [551233.9571428569, 551083.1714285711, 550916....  \n2     [605675.7857142856, 605861.8571428572, 606105....  \n3     [565269.7142857142, 565615.1428571427, 565981....  \n4     [549411.7857142854, 549398.8571428569, 549372....  \n...                                                 ...  \n2331  [930524.5285714284, 930933.8857142852, 931077....  \n2332  [964479.8428571424, 965561.6285714282, 966608....  \n2333  [914564.6142857143, 914270.5428571432, 914029....  \n2334  [924691.6142857135, 924947.5428571424, 925199....  \n2335  [993519.0285714283, 991646.8857142858, 989918....  \n\n[2336 rows x 9 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Increase window length to see a noticeable smoothing effect\nfrom scipy.signal import savgol_filter\nimport matplotlib.pyplot as plt\n\n# # Example of updating the preprocessed_dataframe with more noticeable filter\n# preprocessed_dataframe['ppg_filtered_data'] = data_intervals_dataframe['ppg_data'].apply(\n#     lambda x: savgol_filter(x, window_length=11, polyorder=3) if len(x) >= 11 else x\n# )\nsample_index = 53\n# Print the first few values for debugging\nprint(\"Original PPG data:\", data_intervals_dataframe.loc[sample_index, 'ppg_data'][:10])\nprint(\"Filtered PPG data:\", preprocessed_dataframe.loc[sample_index, 'ppg_filtered_data'][:10])\n\n# Now, re-plot\nsample_index = 53\noriginal_ppg_data = data_intervals_dataframe.loc[sample_index, 'ppg_data']\nfiltered_ppg_data = preprocessed_dataframe.loc[sample_index, 'ppg_filtered_data']\n\nplt.figure(figsize=(12, 6))\nplt.plot(original_ppg_data, label='Original PPG Data', color='blue')\nplt.plot(filtered_ppg_data, label='Filtered PPG Data (Savitzky-Golay)', color='red', linestyle='--')\nplt.xlabel('Time')\nplt.ylabel('PPG Signal')\nplt.legend()\nplt.title('Comparison of Original and Filtered PPG Data')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:41:44.764533Z","iopub.execute_input":"2024-11-14T14:41:44.765441Z","iopub.status.idle":"2024-11-14T14:41:44.802969Z","shell.execute_reply.started":"2024-11-14T14:41:44.765396Z","shell.execute_reply":"2024-11-14T14:41:44.801827Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Original PPG data: [747539. 747220. 746942. 746667. 746443. 746256. 746077. 745892. 745831.\n 745695.]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Print the first few values for debugging\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal PPG data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_intervals_dataframe\u001b[38;5;241m.\u001b[39mloc[sample_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppg_data\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered PPG data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpreprocessed_dataframe\u001b[49m\u001b[38;5;241m.\u001b[39mloc[sample_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppg_filtered_data\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Now, re-plot\u001b[39;00m\n\u001b[1;32m     15\u001b[0m sample_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m53\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'preprocessed_dataframe' is not defined"],"ename":"NameError","evalue":"name 'preprocessed_dataframe' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Calculate Features\n\n\nCalculate Following features:\n1. mean\n2. median\n3. maximum\n4. variance\n5. standard deviation\n6. maximum\n7. minimum\n\nTime Domain Features:\n\n1. ranges\n2. rmssd: root mean square of successive differences\n3. sdsd : standard deviation of successive differences\n4. nni_50: number of normal-to-normal intervals greater than 50ms\n5. pnni_50: proportion of normal-to-normal intervals > 50ms\n6. nni_20: number of normal-to-normal intervals greater than 20ms. Count of successive heartbeats where time difference exceeds 20ms\n7. pnni_20: proportion of normal-to-normal intervals greater than 20ms\n8. avg_hr\n9. std_hr\n10. min_hr\n11. max_hr\n12. energy\n13. abs_sum_diff","metadata":{}},{"cell_type":"code","source":"def ranges(x):\n    return x.max() - x.min()\n\ndef rmssd(x):\n    return np.sqrt(np.mean(np.diff(x)**2))\n\ndef sdsd(x):\n    return st.stdev(np.diff(x))\n\ndef nni_50(x):\n    return sum(np.abs(np.diff(x)) > 50)\n\ndef pnni_50(x):\n    return 100* nni_50(x) / len(x)\n\ndef nni_20(x):\n    return sum(np.abs(np.diff(x)) >20)\n\ndef pnni_20(x):\n    return 100 * nni_20(x) / len(x)\n\ndef avg_hr(x):\n    return st.mean(60000/x)\n\ndef std_hr(x):\n    return st.stdev(60000/x)\n\ndef min_hr(x):\n    return min(60000/x)\n\ndef max_hr(x):\n    return max(60000/x)\n\ndef energy(x):\n    return sum(np.square(x))\n\ndef abs_sum_diff(x):\n    # sum of absolute differences (SAD) is a measure of the similarity between signal\n    return sum(np.abs(np.diff(x)))","metadata":{"execution":{"iopub.status.busy":"2024-11-14T11:38:05.373464Z","iopub.execute_input":"2024-11-14T11:38:05.373839Z","iopub.status.idle":"2024-11-14T11:38:05.384072Z","shell.execute_reply.started":"2024-11-14T11:38:05.373805Z","shell.execute_reply":"2024-11-14T11:38:05.383216Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport statistics as st\n\n# Define the custom functions if not already defined\ndef ranges(x):\n    return x.max() - x.min()\n\ndef rmssd(x):\n    return np.sqrt(np.mean(np.diff(x)**2))\n\ndef sdsd(x):\n    return st.stdev(np.diff(x))\n\ndef nni_50(x):\n    return sum(np.abs(np.diff(x)) > 50)\n\ndef pnni_50(x):\n    return 100 * nni_50(x) / len(x)\n\ndef nni_20(x):\n    return sum(np.abs(np.diff(x)) > 20)\n\ndef pnni_20(x):\n    return 100 * nni_20(x) / len(x)\n\ndef avg_hr(x):\n    # Avoid division by zero or NaN values\n    hr = 60000 / np.array(x)\n    hr = hr[np.isfinite(hr)]  # Remove non-finite values (e.g., inf, NaN)\n    return st.mean(hr) if len(hr) > 0 else 0  # Return 0 if no valid HR data\n\ndef std_hr(x):\n    # Avoid division by zero or NaN values\n    hr = 60000 / np.array(x)\n    hr = hr[np.isfinite(hr)]  # Remove non-finite values (e.g., inf, NaN)\n    return st.stdev(hr) if len(hr) > 1 else 0  # Return 0 if not enough valid data for std dev\n\ndef min_hr(x):\n    # Avoid division by zero or NaN values\n    hr = 60000 / np.array(x)\n    hr = hr[np.isfinite(hr)]  # Remove non-finite values (e.g., inf, NaN)\n    return min(hr) if len(hr) > 0 else 0  # Return 0 if no valid HR data\n\ndef max_hr(x):\n    # Avoid division by zero or NaN values\n    hr = 60000 / np.array(x)\n    hr = hr[np.isfinite(hr)]  # Remove non-finite values (e.g., inf, NaN)\n    return max(hr) if len(hr) > 0 else 0  # Return 0 if no valid HR data\n\ndef energy(x):\n    return sum(np.square(x))\n\ndef abs_sum_diff(x):\n    return sum(np.abs(np.diff(x)))\n\n# Apply the functions to 'ppg_filtered_data' in preprocessed_dataframe\ntime_features = preprocessed_dataframe['ppg_filtered_data'].apply(\n    lambda x: pd.Series({\n        'mean': np.mean(x),\n        'var': np.var(x),\n        'median': np.median(x),\n        'max': np.max(x),\n        'min': np.min(x),\n        'range': ranges(x),\n        'rmssd': rmssd(x),\n        'sdsd': sdsd(x),\n        'nni_50': nni_50(x),\n        'pnni_50': pnni_50(x),\n        'nni_20': nni_20(x),\n        'pnni_20': pnni_20(x),\n        'avg_hr': avg_hr(x),\n        'std_hr': std_hr(x),\n        'min_hr': min_hr(x),\n        'max_hr': max_hr(x),\n        'energy': energy(x),\n        'abs_sum_diff': abs_sum_diff(x)\n    })\n)\n\n# Reset the index to align with the preprocessed_dataframe\ntime_features = time_features.reset_index(drop=True)\n\n# Add the 'subject_id', 'video_id', and 'arousal_valence_label' to the time_features dataframe\ntime_features['subject_id'] = preprocessed_dataframe['subject_id']\ntime_features['video_id'] = preprocessed_dataframe['video_id']\ntime_features['arousal_valence_label'] = preprocessed_dataframe['arousal_valence_label']\n\n# Optionally, check the result\nprint(time_features.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:44:09.862469Z","iopub.execute_input":"2024-11-14T14:44:09.863348Z","iopub.status.idle":"2024-11-14T14:45:00.011315Z","shell.execute_reply.started":"2024-11-14T14:44:09.863307Z","shell.execute_reply":"2024-11-14T14:45:00.010295Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3565737502.py:28: RuntimeWarning: divide by zero encountered in divide\n  hr = 60000 / np.array(x)\n/tmp/ipykernel_30/3565737502.py:34: RuntimeWarning: divide by zero encountered in divide\n  hr = 60000 / np.array(x)\n/tmp/ipykernel_30/3565737502.py:40: RuntimeWarning: divide by zero encountered in divide\n  hr = 60000 / np.array(x)\n/tmp/ipykernel_30/3565737502.py:46: RuntimeWarning: divide by zero encountered in divide\n  hr = 60000 / np.array(x)\n","output_type":"stream"},{"name":"stdout","text":"            mean           var         median            max            min  \\\n0  523505.254326  7.475432e+08  511816.485714  612290.528571  503611.914286   \n1  545864.656894  3.998819e+08  545232.657143  608431.714286  508410.371429   \n2  610096.347869  2.331632e+07  610328.371429  621036.285714  596463.114286   \n3  561703.954208  9.843148e+08  547563.742857  641537.542857  519073.514286   \n4  529994.326120  8.406916e+07  528735.542857  551489.257143  512744.657143   \n\n           range       rmssd        sdsd  nni_50    pnni_50  ...    pnni_20  \\\n0  108678.614286  228.366212  224.163580  1597.0  72.557928  ...  88.368923   \n1  100021.342857  366.326098  366.382285  2609.0  84.134150  ...  93.647211   \n2   24573.171429  297.220224  297.268152  2628.0  84.746856  ...  93.614963   \n3  122464.028571  414.100720  413.899262  2643.0  85.230571  ...  94.259916   \n4   38744.600000  234.629280  234.543208  2354.0  78.440520  ...  91.536155   \n\n     avg_hr    std_hr    min_hr    max_hr        energy   abs_sum_diff  \\\n0  0.114897  0.005454  0.097993  0.119139  6.048465e+14  327220.257143   \n1  0.110063  0.003968  0.098614  0.118015  9.252395e+14  775685.000000   \n2  0.098351  0.000781  0.096613  0.100593  1.154319e+15  668418.171429   \n3  0.107130  0.005597  0.093525  0.115591  9.814530e+14  848082.885714   \n4  0.113242  0.001942  0.108796  0.117017  8.432151e+14  475707.200000   \n\n   subject_id  video_id  arousal_valence_label  \n0           7         0                   AMVM  \n1           7         1                   AMVL  \n2           7         2                   AMVM  \n3           7         3                   AMVM  \n4           7         4                   AMVM  \n\n[5 rows x 21 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# PPG DATA EXTRACTED FEATURES FOR TIME DOMAIN","metadata":{}},{"cell_type":"code","source":"print(time_features.shape) ","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:46:00.113729Z","iopub.execute_input":"2024-11-14T14:46:00.114159Z","iopub.status.idle":"2024-11-14T14:46:00.119153Z","shell.execute_reply.started":"2024-11-14T14:46:00.114120Z","shell.execute_reply":"2024-11-14T14:46:00.118219Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"(2336, 21)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save time features to csv","metadata":{}},{"cell_type":"code","source":"# Save the DataFrame to a CSV file\ntime_features.to_csv('ppg_time_features.csv', index=False)\n\nprint(\"Data saved to ppg_time_features.csv\")\n\n# Specify the path to the CSV file\nfile_path = '/kaggle/working/ppg_time_features.csv'\n\n# Read the CSV file into a DataFrame\ntime_features = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:47:25.475138Z","iopub.execute_input":"2024-11-14T14:47:25.475488Z","iopub.status.idle":"2024-11-14T14:47:25.574922Z","shell.execute_reply.started":"2024-11-14T14:47:25.475455Z","shell.execute_reply":"2024-11-14T14:47:25.573994Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Data saved to ppg_time_features.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"print(time_features)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:47:49.446830Z","iopub.execute_input":"2024-11-14T14:47:49.447720Z","iopub.status.idle":"2024-11-14T14:47:49.466346Z","shell.execute_reply.started":"2024-11-14T14:47:49.447674Z","shell.execute_reply":"2024-11-14T14:47:49.465190Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"               mean           var         median           max            min  \\\n0     523505.254326  7.475432e+08  511816.485714  6.122905e+05  503611.914286   \n1     545864.656894  3.998819e+08  545232.657143  6.084317e+05  508410.371429   \n2     610096.347869  2.331632e+07  610328.371429  6.210363e+05  596463.114286   \n3     561703.954208  9.843148e+08  547563.742857  6.415375e+05  519073.514286   \n4     529994.326120  8.406916e+07  528735.542857  5.514893e+05  512744.657143   \n...             ...           ...            ...           ...            ...   \n2331  915590.416832  2.483477e+08  915314.885714  9.757432e+05  887529.400000   \n2332  945397.973011  1.437385e+09  932658.514286  1.034224e+06  898561.000000   \n2333  893872.405587  1.387493e+08  892156.857143  9.297469e+05  872291.685714   \n2334  893726.579874  1.623928e+08  893146.857143  9.277501e+05  869739.000000   \n2335  932138.016042  4.160659e+08  930075.028571  1.008694e+06  893427.285714   \n\n              range       rmssd        sdsd  nni_50    pnni_50  ...  \\\n0     108678.614286  228.366212  224.163580  1597.0  72.557928  ...   \n1     100021.342857  366.326098  366.382285  2609.0  84.134150  ...   \n2      24573.171429  297.220224  297.268152  2628.0  84.746856  ...   \n3     122464.028571  414.100720  413.899262  2643.0  85.230571  ...   \n4      38744.600000  234.629280  234.543208  2354.0  78.440520  ...   \n...             ...         ...         ...     ...        ...  ...   \n2331   88213.800000  650.950657  650.862921  2202.0  91.711787  ...   \n2332  135662.628571  490.299386  490.237833  2204.0  88.124750  ...   \n2333   57455.257143  408.022404  407.979204  2121.0  88.338192  ...   \n2334   58011.114286  375.774646  375.677105  1854.0  88.243693  ...   \n2335  115266.285714  491.791679  491.160417  2685.0  89.470177  ...   \n\n        pnni_20    avg_hr    std_hr    min_hr    max_hr        energy  \\\n0     88.368923  0.114897  0.005454  0.097993  0.119139  6.048465e+14   \n1     93.647211  0.110063  0.003968  0.098614  0.118015  9.252395e+14   \n2     93.614963  0.098351  0.000781  0.096613  0.100593  1.154319e+15   \n3     94.259916  0.107130  0.005597  0.093525  0.115591  9.814530e+14   \n4     91.536155  0.113242  0.001942  0.108796  0.117017  8.432151e+14   \n...         ...       ...       ...       ...       ...           ...   \n2331  96.543107  0.065551  0.001108  0.061492  0.067603  2.013369e+15   \n2332  95.041983  0.063564  0.002457  0.058015  0.066773  2.238932e+15   \n2333  95.043732  0.067135  0.000875  0.064534  0.068784  1.918751e+15   \n2334  94.859591  0.067148  0.000953  0.064673  0.068986  1.678509e+15   \n2335  95.334888  0.064398  0.001376  0.059483  0.067157  2.608761e+15   \n\n      abs_sum_diff  subject_id  video_id  arousal_valence_label  \n0     3.272203e+05           7         0                   AMVM  \n1     7.756850e+05           7         1                   AMVL  \n2     6.684182e+05           7         2                   AMVM  \n3     8.480829e+05           7         3                   AMVM  \n4     4.757072e+05           7         4                   AMVM  \n...            ...         ...       ...                    ...  \n2331  1.111424e+06          73        27                   AMVL  \n2332  8.941846e+05          73        28                   ALVH  \n2333  7.089703e+05          73        29                   AMVM  \n2334  5.669027e+05          73        30                   ALVM  \n2335  1.053917e+06          73        31                   AMVL  \n\n[2336 rows x 21 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Frequency Features","metadata":{}},{"cell_type":"code","source":"from scipy import signal \nfrom scipy.ndimage import label\nfrom scipy.stats import zscore\nfrom scipy.interpolate import interp1d\nfrom scipy import integrate\nfrom scipy.integrate import *  # this will include trapz. BUT using a wildcard import is not recommended as \n# it imports all functions and variables from the scipy.integrate module which can cause namespace pollution, conflicts btw similarly\n# named functions in different libraries\n\n# from scipy.integrate import trapz\n# from scipy import trapz\n\nfrom numpy import trapz # ORIGINALLY USED FROM SCIPY.INTEGRATE IMPORT TRAPZ WHICH DID NOT WORK","metadata":{"execution":{"iopub.status.busy":"2024-11-14T11:49:31.069380Z","iopub.execute_input":"2024-11-14T11:49:31.070174Z","iopub.status.idle":"2024-11-14T11:49:32.056135Z","shell.execute_reply.started":"2024-11-14T11:49:31.070126Z","shell.execute_reply":"2024-11-14T11:49:32.055031Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(preprocessed_dataframe)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:10:58.804808Z","iopub.execute_input":"2024-11-14T14:10:58.805759Z","iopub.status.idle":"2024-11-14T14:10:59.131540Z","shell.execute_reply.started":"2024-11-14T14:10:58.805713Z","shell.execute_reply":"2024-11-14T14:10:59.130197Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpreprocessed_dataframe\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'preprocessed_dataframe' is not defined"],"ename":"NameError","evalue":"name 'preprocessed_dataframe' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import ast\n# import re\n\n# # Example of how to preprocess the 'ppg_filtered_data' column\n# def preprocess_ppg_data(df):\n#     for i in range(len(df)):\n#         try:\n#             ppg_str = df['ppg_filtered_data'].iloc[i]\n#             # Remove any extraneous spaces, line breaks, and ellipses\n#             ppg_str = re.sub(r'\\s+', ',', ppg_str.replace(\"...\", \"\"))\n#             # Use ast.literal_eval to safely evaluate as a list\n#             ppg_list = np.array(ast.literal_eval(ppg_str), dtype=float)\n#             # Replace the string with the actual list of floats\n#             df.at[i, 'ppg_filtered_data'] = ppg_list\n#         except (ValueError, SyntaxError, TypeError) as e:\n#             print(f\"Error converting PPG data at index {i}: {e}\")\n#             continue\n\n# # Call the function on the dataframe\n# preprocess_ppg_data(preprocessed_dataframe)\n\n# Confirm the conversion worked by printing the first row\nprint(preprocessed_dataframe['ppg_filtered_data'].iloc[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T12:14:31.966664Z","iopub.execute_input":"2024-11-14T12:14:31.967064Z","iopub.status.idle":"2024-11-14T12:14:31.974702Z","shell.execute_reply.started":"2024-11-14T12:14:31.967026Z","shell.execute_reply":"2024-11-14T12:14:31.973569Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[612290.52857143 611451.88571429 610755.17142857 515667.17142857\n 515744.88571429 515774.52857143]\n","output_type":"stream"}]},{"cell_type":"code","source":"preprocessed_dataframe.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-14T12:14:55.443499Z","iopub.execute_input":"2024-11-14T12:14:55.443900Z","iopub.status.idle":"2024-11-14T12:14:55.451701Z","shell.execute_reply.started":"2024-11-14T12:14:55.443863Z","shell.execute_reply":"2024-11-14T12:14:55.450591Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(2336, 9)"},"metadata":{}}]},{"cell_type":"code","source":"# from scipy.interpolate import interp1d\n# import numpy as np\n# import pandas as pd\n# from scipy.interpolate import interp1d\n# from scipy import signal\n# from numpy import trapz # ORIGINALLY USED FROM SCIPY.INTEGRATE IMPORT TRAPZ WHICH DID NOT WORK\n# import matplotlib.pyplot as plt\n\n# # Assuming preprocessed_dataframe is already loaded\n\n# # Interpolate the PPG data\n# ppg_interpolated = []\n# for i in range(len(preprocessed_dataframe)):\n#     ppg_signal = preprocessed_dataframe['ppg_filtered_data'].iloc[i]  # Get the PPG signal for each subject/video\n#     x = np.cumsum(ppg_signal) / 1000.0  # Cumulative sum of PPG signal (time base)\n#     f = interp1d(x, ppg_signal, kind='cubic', fill_value=\"extrapolate\")  # Interpolation function\n#     fs = 4.0  # New sampling frequency\n#     steps = 1 / fs\n\n#     # Sample from the interpolation function at the new time points\n#     xx = np.arange(1, np.max(x), steps)\n#     ppg_interpolated.append(f(xx))\n\n# # Check dimensions of interpolated data\n# print(len(ppg_interpolated), ppg_interpolated[0].shape)\n\n# # Plot original and interpolated PPG signals for comparison\n# plt.plot(preprocessed_dataframe['ppg_filtered_data'].iloc[0], label='Original PPG Data')\n# plt.plot(ppg_interpolated[0], label='Interpolated PPG Data')\n# plt.xlabel('Time')\n# plt.ylabel('PPG Signal')\n# plt.title('Original and Interpolated PPG Data')\n# plt.legend()\n# plt.show()\n\n# # Frequency domain function to calculate frequency features\n# def frequency_domain(ppg_signal, fs=4):\n#     # Estimate the spectral density using Welch's method\n#     fxx, pxx = signal.welch(x=ppg_signal, fs=fs)\n\n#     # Segment frequencies into bands\n#     cond_vlf = (fxx >= 0) & (fxx < 0.04)\n#     cond_lf = (fxx >= 0.04) & (fxx < 0.15)\n#     cond_hf = (fxx >= 0.15) & (fxx < 0.4)\n\n#     # Calculate power in each band by integrating the spectral density\n#     vlf = trapz(pxx[cond_vlf], fxx[cond_vlf])\n#     lf = trapz(pxx[cond_lf], fxx[cond_lf])\n#     hf = trapz(pxx[cond_hf], fxx[cond_hf])\n\n#     # Sum these up to get total power\n#     total_power = vlf + lf + hf\n\n#     # Find which frequency has the most power in each band\n#     peak_vlf = fxx[cond_vlf][np.argmax(pxx[cond_vlf])]\n#     peak_lf = fxx[cond_lf][np.argmax(pxx[cond_lf])]\n#     peak_hf = fxx[cond_hf][np.argmax(pxx[cond_hf])]\n\n#     # Fraction of LF and HF\n#     lf_nu = 100 * lf / (lf + hf)\n#     hf_nu = 100 * hf / (lf + hf)\n\n#     # Return results\n#     result = [vlf, lf, hf, total_power, lf / hf, peak_vlf, peak_lf, peak_hf, lf_nu, hf_nu]\n#     return np.array(result), fxx, pxx\n\n# # Extract frequency features from the interpolated PPG data\n# freq_feat = []\n# for i in range(len(ppg_interpolated)):\n#     results, fxx, pxx = frequency_domain(ppg_interpolated[i])\n#     freq_feat.append(results)\n\n# # Convert list to numpy array and create a DataFrame with frequency features\n# freq_col = ['vlf', 'lf', 'hf', 'tot_pow', 'lf_hf_ratio', 'peak_vlf', 'peak_lf', 'peak_hf', 'lf_nu', 'hf_nu']\n# freq_features = pd.DataFrame(freq_feat, columns=freq_col)\n\n# # Add 'subject_id', 'video_id', and 'arousal_valence_label' to the frequency features DataFrame\n# freq_features['subject_id'] = preprocessed_dataframe['subject_id']\n# freq_features['video_id'] = preprocessed_dataframe['video_id']\n# freq_features['arousal_valence_label'] = preprocessed_dataframe['arousal_valence_label']\n\n# # Optionally, check the result\n# print(freq_features.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T12:15:03.048334Z","iopub.execute_input":"2024-11-14T12:15:03.049123Z","iopub.status.idle":"2024-11-14T12:15:03.420705Z","shell.execute_reply.started":"2024-11-14T12:15:03.049081Z","shell.execute_reply":"2024-11-14T12:15:03.419390Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(preprocessed_dataframe)):\n\u001b[1;32m     14\u001b[0m     ppg_signal \u001b[38;5;241m=\u001b[39m preprocessed_dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppg_filtered_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i]  \u001b[38;5;66;03m# Get the PPG signal for each subject/video\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppg_signal\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.0\u001b[39m  \u001b[38;5;66;03m# Cumulative sum of PPG signal (time base)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     f \u001b[38;5;241m=\u001b[39m interp1d(x, ppg_signal, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcubic\u001b[39m\u001b[38;5;124m'\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextrapolate\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Interpolation function\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4.0\u001b[39m  \u001b[38;5;66;03m# New sampling frequency\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2586\u001b[0m, in \u001b[0;36mcumsum\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m   2512\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_cumsum_dispatcher)\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcumsum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;124;03m    Return the cumulative sum of the elements along a given axis.\u001b[39;00m\n\u001b[1;32m   2516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2584\u001b[0m \n\u001b[1;32m   2585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcumsum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n","\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U108'), dtype('<U108')) -> None"],"ename":"UFuncTypeError","evalue":"ufunc 'add' did not contain a loop with signature matching types (dtype('<U108'), dtype('<U108')) -> None","output_type":"error"}]},{"cell_type":"code","source":"data_intervals_dataframe.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:33:25.915484Z","iopub.execute_input":"2024-11-14T14:33:25.915961Z","iopub.status.idle":"2024-11-14T14:33:25.925028Z","shell.execute_reply.started":"2024-11-14T14:33:25.915915Z","shell.execute_reply":"2024-11-14T14:33:25.923916Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(2336, 9)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Without Savgol filter","metadata":{}},{"cell_type":"code","source":"from scipy.interpolate import interp1d\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\nfrom scipy import signal\nfrom scipy.signal import savgol_filter\nfrom numpy import trapz # ORIGINALLY USED FROM SCIPY.INTEGRATE IMPORT TRAPZ WHICH DID NOT WORK\nimport matplotlib.pyplot as plt\n\n\n\n\n# Function to apply Savitzky-Golay filter on ppg_data column\ndef preprocess_ppg_data_with_savgol(df):\n    # Convert 'ppg_data' to list of floats if in string format\n    df['ppg_data'] = df['ppg_data'].apply(lambda x: np.array(list(map(float, x.split(',')))) if isinstance(x, str) else x)\n\n    # Apply Savitzky-Golay filter and store in 'ppg_filtered_data'\n    df['ppg_filtered_data'] = df['ppg_data'].apply(lambda ppg: savgol_filter(ppg, window_length=5, polyorder=3))\n\n    return df\n\n# Apply preprocessing function to create 'ppg_filtered_data' column\npreprocessed_dataframe_new = preprocess_ppg_data_with_savgol(data_intervals_dataframe)\n\n# Verify the result\nprint(preprocessed_dataframe_new[['ppg_data', 'ppg_filtered_data']].head())\n# Apply preprocessing function to create 'ppg_filtered_data' column\npreprocessed_dataframe_new = preprocess_ppg_data_with_savgol(data_intervals_dataframe)\n\n# Step 2: Interpolate and extract frequency features from 'ppg_filtered_data'\n# Interpolation function for PPG signal\ndef interpolate_ppg(ppg_signal):\n    # Define a time array based on a constant sampling interval (100 Hz)\n    x = np.linspace(0, len(ppg_signal) / 100.0, num=len(ppg_signal))  # 100 Hz sampling\n    f = interp1d(x, ppg_signal, kind='cubic', fill_value=\"extrapolate\")\n    \n    # Define new time points for the interpolated signal (4 Hz sampling)\n    fs = 4.0  # New sampling frequency\n    new_x = np.arange(0, x[-1], 1 / fs)\n    \n    # Interpolate the signal at the new time points\n    interpolated_signal = f(new_x)\n    return interpolated_signal\n\n# Frequency domain function to calculate frequency features\ndef frequency_domain(ppg_signal, fs=4):\n    fxx, pxx = signal.welch(x=ppg_signal, fs=fs)\n    cond_vlf = (fxx >= 0) & (fxx < 0.04)\n    cond_lf = (fxx >= 0.04) & (fxx < 0.15)\n    cond_hf = (fxx >= 0.15) & (fxx < 0.4)\n    vlf = trapz(pxx[cond_vlf], fxx[cond_vlf])\n    lf = trapz(pxx[cond_lf], fxx[cond_lf])\n    hf = trapz(pxx[cond_hf], fxx[cond_hf])\n    total_power = vlf + lf + hf\n    peak_vlf = fxx[cond_vlf][np.argmax(pxx[cond_vlf])] if np.any(cond_vlf) else 0\n    peak_lf = fxx[cond_lf][np.argmax(pxx[cond_lf])] if np.any(cond_lf) else 0\n    peak_hf = fxx[cond_hf][np.argmax(pxx[cond_hf])] if np.any(cond_hf) else 0\n    lf_nu = 100 * lf / (lf + hf) if (lf + hf) != 0 else 0\n    hf_nu = 100 * hf / (lf + hf) if (lf + hf) != 0 else 0\n    return [vlf, lf, hf, total_power, lf / hf if hf != 0 else 0, peak_vlf, peak_lf, peak_hf, lf_nu, hf_nu]\n\n# Step 3: Interpolate and extract frequency features for each PPG filtered signal\nppg_interpolated = preprocessed_dataframe_new['ppg_filtered_data'].apply(interpolate_ppg)\n# Verify the result\nprint(ppg_interpolated.head())\n\nfreq_feat = ppg_interpolated.apply(frequency_domain)\n\n# Convert frequency features to a DataFrame and add identifiers\nfreq_col = ['vlf', 'lf', 'hf', 'tot_pow', 'lf_hf_ratio', 'peak_vlf', 'peak_lf', 'peak_hf', 'lf_nu', 'hf_nu']\nfreq_features = pd.DataFrame(freq_feat.tolist(), columns=freq_col)\nfreq_features['subject_id'] = preprocessed_dataframe_new['subject_id']\nfreq_features['video_id'] = preprocessed_dataframe_new['video_id']\nfreq_features['arousal_valence_label'] = preprocessed_dataframe_new['arousal_valence_label']\n\n# Optional: Check the frequency features\nprint(freq_features.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:39:11.299102Z","iopub.execute_input":"2024-11-14T14:39:11.299941Z","iopub.status.idle":"2024-11-14T14:39:15.988198Z","shell.execute_reply.started":"2024-11-14T14:39:11.299898Z","shell.execute_reply":"2024-11-14T14:39:15.987382Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"                                            ppg_data  \\\n0  [612293.0, 611442.0, 610770.0, 610083.0, 60936...   \n1  [551232.0, 551091.0, 550905.0, 550746.0, 55054...   \n2  [605678.0, 605853.0, 606119.0, 606351.0, 60657...   \n3  [565274.0, 565598.0, 566007.0, 566224.0, 56627...   \n4  [549417.0, 549378.0, 549404.0, 549342.0, 54940...   \n\n                                   ppg_filtered_data  \n0  [612290.5285714285, 611451.8857142855, 610755....  \n1  [551233.9571428569, 551083.1714285711, 550916....  \n2  [605675.7857142856, 605861.8571428572, 606105....  \n3  [565269.7142857142, 565615.1428571427, 565981....  \n4  [549411.7857142854, 549398.8571428569, 549372....  \n0    [612290.5285714285, 606635.4810314195, 608682....\n1    [551233.9571428569, 544074.5860201311, 535879....\n2    [605675.7857142856, 611041.6099478622, 606677....\n3    [565269.7142857142, 556154.2273820221, 562873....\n4    [549411.7857142854, 547965.5832855243, 549530....\nName: ppg_filtered_data, dtype: object\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 89, using nperseg = 89\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 125, using nperseg = 125\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 121, using nperseg = 121\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 117, using nperseg = 117\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 93, using nperseg = 93\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 101, using nperseg = 101\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 113, using nperseg = 113\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 133, using nperseg = 133\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 109, using nperseg = 109\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 105, using nperseg = 105\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 97, using nperseg = 97\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 85, using nperseg = 85\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 129, using nperseg = 129\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 137, using nperseg = 137\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 81, using nperseg = 81\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n","output_type":"stream"},{"name":"stdout","text":"            vlf            lf            hf       tot_pow  lf_hf_ratio  \\\n0  0.000000e+00  6.147186e+07  3.030959e+05  6.177495e+07   202.813259   \n1  1.656730e+07  1.274268e+08  3.504205e+07  1.790361e+08     3.636396   \n2  2.496412e+06  9.414860e+06  1.981257e+06  1.389253e+07     4.751964   \n3  1.208835e+08  1.102812e+08  2.583060e+07  2.569952e+08     4.269400   \n4  1.917388e+06  1.034756e+07  1.214425e+07  2.440920e+07     0.852055   \n\n   peak_vlf   peak_lf   peak_hf      lf_nu      hf_nu  subject_id  video_id  \\\n0  0.000000  0.044944  0.224719  99.509355   0.490645           7         0   \n1  0.032000  0.128000  0.160000  78.431525  21.568475           7         1   \n2  0.032000  0.128000  0.160000  82.614633  17.385367           7         2   \n3  0.032000  0.096000  0.160000  81.022508  18.977492           7         3   \n4  0.033058  0.132231  0.165289  46.005921  53.994079           7         4   \n\n  arousal_valence_label  \n0                  AMVM  \n1                  AMVL  \n2                  AMVM  \n3                  AMVM  \n4                  AMVM  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save frequency features to csv file","metadata":{}},{"cell_type":"code","source":"\n\n# Save the DataFrame to a CSV file\nfreq_features.to_csv('ppg_freq_features.csv', index=False)\n\nprint(\"Data saved to ppg_freq_features.csv\")\n\n# Specify the path to the CSV file\nfile_path = '/kaggle/working/ppg_freq_features.csv'\n\n# Read the CSV file into a DataFrame\nppg_freq_features = pd.read_csv(file_path)\n\n\nprint(ppg_freq_features.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:52:30.277667Z","iopub.execute_input":"2024-11-14T14:52:30.278333Z","iopub.status.idle":"2024-11-14T14:52:30.344121Z","shell.execute_reply.started":"2024-11-14T14:52:30.278291Z","shell.execute_reply":"2024-11-14T14:52:30.343239Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Data saved to ppg_freq_features.csv\n            vlf            lf            hf       tot_pow  lf_hf_ratio  \\\n0  0.000000e+00  6.147186e+07  3.030959e+05  6.177495e+07   202.813259   \n1  1.656730e+07  1.274268e+08  3.504205e+07  1.790361e+08     3.636396   \n2  2.496412e+06  9.414860e+06  1.981257e+06  1.389253e+07     4.751964   \n3  1.208835e+08  1.102812e+08  2.583060e+07  2.569952e+08     4.269400   \n4  1.917388e+06  1.034756e+07  1.214425e+07  2.440920e+07     0.852055   \n\n   peak_vlf   peak_lf   peak_hf      lf_nu      hf_nu  subject_id  video_id  \\\n0  0.000000  0.044944  0.224719  99.509355   0.490645           7         0   \n1  0.032000  0.128000  0.160000  78.431525  21.568475           7         1   \n2  0.032000  0.128000  0.160000  82.614633  17.385367           7         2   \n3  0.032000  0.096000  0.160000  81.022508  18.977492           7         3   \n4  0.033058  0.132231  0.165289  46.005921  53.994079           7         4   \n\n  arousal_valence_label  \n0                  AMVM  \n1                  AMVL  \n2                  AMVM  \n3                  AMVM  \n4                  AMVM  \n","output_type":"stream"}]},{"cell_type":"code","source":"ppg_freq_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:52:46.903611Z","iopub.execute_input":"2024-11-14T14:52:46.904476Z","iopub.status.idle":"2024-11-14T14:52:46.910164Z","shell.execute_reply.started":"2024-11-14T14:52:46.904429Z","shell.execute_reply":"2024-11-14T14:52:46.909361Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(2336, 13)"},"metadata":{}}]},{"cell_type":"code","source":"print(time_features)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:49:03.944200Z","iopub.execute_input":"2024-11-14T14:49:03.945106Z","iopub.status.idle":"2024-11-14T14:49:03.962249Z","shell.execute_reply.started":"2024-11-14T14:49:03.945063Z","shell.execute_reply":"2024-11-14T14:49:03.961347Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"               mean           var         median           max            min  \\\n0     523505.254326  7.475432e+08  511816.485714  6.122905e+05  503611.914286   \n1     545864.656894  3.998819e+08  545232.657143  6.084317e+05  508410.371429   \n2     610096.347869  2.331632e+07  610328.371429  6.210363e+05  596463.114286   \n3     561703.954208  9.843148e+08  547563.742857  6.415375e+05  519073.514286   \n4     529994.326120  8.406916e+07  528735.542857  5.514893e+05  512744.657143   \n...             ...           ...            ...           ...            ...   \n2331  915590.416832  2.483477e+08  915314.885714  9.757432e+05  887529.400000   \n2332  945397.973011  1.437385e+09  932658.514286  1.034224e+06  898561.000000   \n2333  893872.405587  1.387493e+08  892156.857143  9.297469e+05  872291.685714   \n2334  893726.579874  1.623928e+08  893146.857143  9.277501e+05  869739.000000   \n2335  932138.016042  4.160659e+08  930075.028571  1.008694e+06  893427.285714   \n\n              range       rmssd        sdsd  nni_50    pnni_50  ...  \\\n0     108678.614286  228.366212  224.163580  1597.0  72.557928  ...   \n1     100021.342857  366.326098  366.382285  2609.0  84.134150  ...   \n2      24573.171429  297.220224  297.268152  2628.0  84.746856  ...   \n3     122464.028571  414.100720  413.899262  2643.0  85.230571  ...   \n4      38744.600000  234.629280  234.543208  2354.0  78.440520  ...   \n...             ...         ...         ...     ...        ...  ...   \n2331   88213.800000  650.950657  650.862921  2202.0  91.711787  ...   \n2332  135662.628571  490.299386  490.237833  2204.0  88.124750  ...   \n2333   57455.257143  408.022404  407.979204  2121.0  88.338192  ...   \n2334   58011.114286  375.774646  375.677105  1854.0  88.243693  ...   \n2335  115266.285714  491.791679  491.160417  2685.0  89.470177  ...   \n\n        pnni_20    avg_hr    std_hr    min_hr    max_hr        energy  \\\n0     88.368923  0.114897  0.005454  0.097993  0.119139  6.048465e+14   \n1     93.647211  0.110063  0.003968  0.098614  0.118015  9.252395e+14   \n2     93.614963  0.098351  0.000781  0.096613  0.100593  1.154319e+15   \n3     94.259916  0.107130  0.005597  0.093525  0.115591  9.814530e+14   \n4     91.536155  0.113242  0.001942  0.108796  0.117017  8.432151e+14   \n...         ...       ...       ...       ...       ...           ...   \n2331  96.543107  0.065551  0.001108  0.061492  0.067603  2.013369e+15   \n2332  95.041983  0.063564  0.002457  0.058015  0.066773  2.238932e+15   \n2333  95.043732  0.067135  0.000875  0.064534  0.068784  1.918751e+15   \n2334  94.859591  0.067148  0.000953  0.064673  0.068986  1.678509e+15   \n2335  95.334888  0.064398  0.001376  0.059483  0.067157  2.608761e+15   \n\n      abs_sum_diff  subject_id  video_id  arousal_valence_label  \n0     3.272203e+05           7         0                   AMVM  \n1     7.756850e+05           7         1                   AMVL  \n2     6.684182e+05           7         2                   AMVM  \n3     8.480829e+05           7         3                   AMVM  \n4     4.757072e+05           7         4                   AMVM  \n...            ...         ...       ...                    ...  \n2331  1.111424e+06          73        27                   AMVL  \n2332  8.941846e+05          73        28                   ALVH  \n2333  7.089703e+05          73        29                   AMVM  \n2334  5.669027e+05          73        30                   ALVM  \n2335  1.053917e+06          73        31                   AMVL  \n\n[2336 rows x 21 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"ppg_freq_features.columns","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:03:41.959092Z","iopub.execute_input":"2024-11-14T15:03:41.959490Z","iopub.status.idle":"2024-11-14T15:03:41.966646Z","shell.execute_reply.started":"2024-11-14T15:03:41.959452Z","shell.execute_reply":"2024-11-14T15:03:41.965764Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"Index(['vlf', 'lf', 'hf', 'tot_pow', 'lf_hf_ratio', 'peak_vlf', 'peak_lf',\n       'peak_hf', 'lf_nu', 'hf_nu', 'subject_id', 'video_id',\n       'arousal_valence_label'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"time_features.columns","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:04:58.256882Z","iopub.execute_input":"2024-11-14T15:04:58.257948Z","iopub.status.idle":"2024-11-14T15:04:58.265548Z","shell.execute_reply.started":"2024-11-14T15:04:58.257886Z","shell.execute_reply":"2024-11-14T15:04:58.264476Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Index(['mean', 'var', 'median', 'max', 'min', 'range', 'rmssd', 'sdsd',\n       'nni_50', 'pnni_50', 'nni_20', 'pnni_20', 'avg_hr', 'std_hr', 'min_hr',\n       'max_hr', 'energy', 'abs_sum_diff', 'subject_id', 'video_id',\n       'arousal_valence_label'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Concatenate time and frequency features of PPG","metadata":{}},{"cell_type":"code","source":"# Drop common columns from ppg_freq_features dataframe\nppg_freq_features_unique = ppg_freq_features.drop(columns=['subject_id', 'video_id', 'arousal_valence_label'])\n\n# Concatenate the dataframes\nppg_features_extracted = pd.concat([time_features, ppg_freq_features_unique], axis=1)\n\n# Verify the result\nprint(ppg_features_extracted.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:07:58.819212Z","iopub.execute_input":"2024-11-14T15:07:58.820160Z","iopub.status.idle":"2024-11-14T15:07:58.828402Z","shell.execute_reply.started":"2024-11-14T15:07:58.820115Z","shell.execute_reply":"2024-11-14T15:07:58.827462Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Index(['mean', 'var', 'median', 'max', 'min', 'range', 'rmssd', 'sdsd',\n       'nni_50', 'pnni_50', 'nni_20', 'pnni_20', 'avg_hr', 'std_hr', 'min_hr',\n       'max_hr', 'energy', 'abs_sum_diff', 'subject_id', 'video_id',\n       'arousal_valence_label', 'vlf', 'lf', 'hf', 'tot_pow', 'lf_hf_ratio',\n       'peak_vlf', 'peak_lf', 'peak_hf', 'lf_nu', 'hf_nu'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# READ ALL PPG EXTRACTED FEATURES CSV HERE","metadata":{}},{"cell_type":"code","source":"\nprint(ppg_features_extracted.head())\n\nppg_features_extracted.shape\n\nprint(\"===============================================\")\n\nppg_features_extracted.to_csv(\"ppg_all_features.csv\", index=False)\n\n\nprint(\"Data saved to ppg_all_features.csv\")\n\n# Specify the path to the CSV file\nfile_path = '/kaggle/working/ppg_all_features.csv'\n\n# Read the CSV file into a DataFrame\nppg_all_features = pd.read_csv(file_path)\n\n\nprint(ppg_all_features.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:12:36.862764Z","iopub.execute_input":"2024-11-14T15:12:36.863488Z","iopub.status.idle":"2024-11-14T15:12:37.029168Z","shell.execute_reply.started":"2024-11-14T15:12:36.863448Z","shell.execute_reply":"2024-11-14T15:12:37.028228Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"            mean           var         median            max            min  \\\n0  523505.254326  7.475432e+08  511816.485714  612290.528571  503611.914286   \n1  545864.656894  3.998819e+08  545232.657143  608431.714286  508410.371429   \n2  610096.347869  2.331632e+07  610328.371429  621036.285714  596463.114286   \n3  561703.954208  9.843148e+08  547563.742857  641537.542857  519073.514286   \n4  529994.326120  8.406916e+07  528735.542857  551489.257143  512744.657143   \n\n           range       rmssd        sdsd  nni_50    pnni_50  ...  \\\n0  108678.614286  228.366212  224.163580  1597.0  72.557928  ...   \n1  100021.342857  366.326098  366.382285  2609.0  84.134150  ...   \n2   24573.171429  297.220224  297.268152  2628.0  84.746856  ...   \n3  122464.028571  414.100720  413.899262  2643.0  85.230571  ...   \n4   38744.600000  234.629280  234.543208  2354.0  78.440520  ...   \n\n            vlf            lf            hf       tot_pow  lf_hf_ratio  \\\n0  0.000000e+00  6.147186e+07  3.030959e+05  6.177495e+07   202.813259   \n1  1.656730e+07  1.274268e+08  3.504205e+07  1.790361e+08     3.636396   \n2  2.496412e+06  9.414860e+06  1.981257e+06  1.389253e+07     4.751964   \n3  1.208835e+08  1.102812e+08  2.583060e+07  2.569952e+08     4.269400   \n4  1.917388e+06  1.034756e+07  1.214425e+07  2.440920e+07     0.852055   \n\n   peak_vlf   peak_lf   peak_hf      lf_nu      hf_nu  \n0  0.000000  0.044944  0.224719  99.509355   0.490645  \n1  0.032000  0.128000  0.160000  78.431525  21.568475  \n2  0.032000  0.128000  0.160000  82.614633  17.385367  \n3  0.032000  0.096000  0.160000  81.022508  18.977492  \n4  0.033058  0.132231  0.165289  46.005921  53.994079  \n\n[5 rows x 31 columns]\n===============================================\nData saved to ppg_all_features.csv\n            mean           var         median            max            min  \\\n0  523505.254326  7.475432e+08  511816.485714  612290.528571  503611.914286   \n1  545864.656894  3.998819e+08  545232.657143  608431.714286  508410.371429   \n2  610096.347869  2.331632e+07  610328.371429  621036.285714  596463.114286   \n3  561703.954208  9.843148e+08  547563.742857  641537.542857  519073.514286   \n4  529994.326120  8.406916e+07  528735.542857  551489.257143  512744.657143   \n\n           range       rmssd        sdsd  nni_50    pnni_50  ...  \\\n0  108678.614286  228.366212  224.163580  1597.0  72.557928  ...   \n1  100021.342857  366.326098  366.382285  2609.0  84.134150  ...   \n2   24573.171429  297.220224  297.268152  2628.0  84.746856  ...   \n3  122464.028571  414.100720  413.899262  2643.0  85.230571  ...   \n4   38744.600000  234.629280  234.543208  2354.0  78.440520  ...   \n\n            vlf            lf            hf       tot_pow  lf_hf_ratio  \\\n0  0.000000e+00  6.147186e+07  3.030959e+05  6.177495e+07   202.813259   \n1  1.656730e+07  1.274268e+08  3.504205e+07  1.790361e+08     3.636396   \n2  2.496412e+06  9.414860e+06  1.981257e+06  1.389253e+07     4.751964   \n3  1.208835e+08  1.102812e+08  2.583060e+07  2.569952e+08     4.269400   \n4  1.917388e+06  1.034756e+07  1.214425e+07  2.440920e+07     0.852055   \n\n   peak_vlf   peak_lf   peak_hf      lf_nu      hf_nu  \n0  0.000000  0.044944  0.224719  99.509355   0.490645  \n1  0.032000  0.128000  0.160000  78.431525  21.568475  \n2  0.032000  0.128000  0.160000  82.614633  17.385367  \n3  0.032000  0.096000  0.160000  81.022508  18.977492  \n4  0.033058  0.132231  0.165289  46.005921  53.994079  \n\n[5 rows x 31 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"labels = ppg_all_features['arousal_valence_label']\nprint(labels)\n\nppg_only_features =ppg_all_features.drop(columns=['subject_id', 'video_id', 'arousal_valence_label'])\nprint(ppg_only_features.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:29:24.757949Z","iopub.execute_input":"2024-11-14T15:29:24.758360Z","iopub.status.idle":"2024-11-14T15:29:24.778103Z","shell.execute_reply.started":"2024-11-14T15:29:24.758320Z","shell.execute_reply":"2024-11-14T15:29:24.776696Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"0       AMVM\n1       AMVL\n2       AMVM\n3       AMVM\n4       AMVM\n        ... \n2331    AMVL\n2332    ALVH\n2333    AMVM\n2334    ALVM\n2335    AMVL\nName: arousal_valence_label, Length: 2336, dtype: object\n            mean           var         median            max            min  \\\n0  523505.254326  7.475432e+08  511816.485714  612290.528571  503611.914286   \n1  545864.656894  3.998819e+08  545232.657143  608431.714286  508410.371429   \n2  610096.347869  2.331632e+07  610328.371429  621036.285714  596463.114286   \n3  561703.954208  9.843148e+08  547563.742857  641537.542857  519073.514286   \n4  529994.326120  8.406916e+07  528735.542857  551489.257143  512744.657143   \n\n           range       rmssd        sdsd  nni_50    pnni_50  ...  \\\n0  108678.614286  228.366212  224.163580  1597.0  72.557928  ...   \n1  100021.342857  366.326098  366.382285  2609.0  84.134150  ...   \n2   24573.171429  297.220224  297.268152  2628.0  84.746856  ...   \n3  122464.028571  414.100720  413.899262  2643.0  85.230571  ...   \n4   38744.600000  234.629280  234.543208  2354.0  78.440520  ...   \n\n            vlf            lf            hf       tot_pow  lf_hf_ratio  \\\n0  0.000000e+00  6.147186e+07  3.030959e+05  6.177495e+07   202.813259   \n1  1.656730e+07  1.274268e+08  3.504205e+07  1.790361e+08     3.636396   \n2  2.496412e+06  9.414860e+06  1.981257e+06  1.389253e+07     4.751964   \n3  1.208835e+08  1.102812e+08  2.583060e+07  2.569952e+08     4.269400   \n4  1.917388e+06  1.034756e+07  1.214425e+07  2.440920e+07     0.852055   \n\n   peak_vlf   peak_lf   peak_hf      lf_nu      hf_nu  \n0  0.000000  0.044944  0.224719  99.509355   0.490645  \n1  0.032000  0.128000  0.160000  78.431525  21.568475  \n2  0.032000  0.128000  0.160000  82.614633  17.385367  \n3  0.032000  0.096000  0.160000  81.022508  18.977492  \n4  0.033058  0.132231  0.165289  46.005921  53.994079  \n\n[5 rows x 28 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EXTRACT GSR FEATURES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to the CSV file\nfile_path = '/kaggle/input/data-intervals-not-normalized-with-labels/data_intervals_not_normalized1.csv'\n\n# Read the CSV file into a DataFrame\ndata_intervals_dataframe = pd.read_csv(file_path)\n\n# Display the first few rows to verify\nprint(data_intervals_dataframe.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:09:30.502004Z","iopub.execute_input":"2024-11-14T16:09:30.502396Z","iopub.status.idle":"2024-11-14T16:09:31.029356Z","shell.execute_reply.started":"2024-11-14T16:09:30.502359Z","shell.execute_reply":"2024-11-14T16:09:31.028368Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"   subject_id  video_id                                           gsr_data  \\\n0           7         0  179125.171875,178570.375,179391.03125,181556.2...   \n1           7         1  182172.671875,182666.59375,183080.09375,184354...   \n2           7         2  168857.21875,168781.234375,169132.53125,169823...   \n3           7         3  178895.96875,178088.4375,177970.546875,177919....   \n4           7         4  195028.421875,195278.421875,195498.46875,19572...   \n\n                                            ppg_data  valence  arousal  \\\n0  612293.0,611442.0,610770.0,610083.0,609360.0,6...        4        5   \n1  551232.0,551091.0,550905.0,550746.0,550549.0,5...        3        5   \n2  605678.0,605853.0,606119.0,606351.0,606579.0,6...        4        5   \n3  565274.0,565598.0,566007.0,566224.0,566272.0,5...        5        6   \n4  549417.0,549378.0,549404.0,549342.0,549404.0,5...        6        6   \n\n  arousal_level valence_level arousal_valence_label  \n0             M             M                  AMVM  \n1             M             L                  AMVL  \n2             M             M                  AMVM  \n3             M             M                  AMVM  \n4             M             M                  AMVM  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Apply CWT to GSR","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Print the first value in the 'gsr_data' column (first row) and its type\nfirst_value = data_intervals_dataframe['gsr_data'].iloc[0]\nprint(\"First value in 'gsr_data':\", first_value)\nprint(\"Type of the first value:\", type(first_value))\n\n# Print the third value in the 'gsr_data' column (third row) and its type\nthird_value = data_intervals_dataframe['gsr_data'].iloc[2]\nprint(\"Third value in 'gsr_data':\", third_value)\nprint(\"Type of the third value:\", type(third_value))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:00:54.748939Z","iopub.execute_input":"2024-11-14T16:00:54.749316Z","iopub.status.idle":"2024-11-14T16:00:54.756474Z","shell.execute_reply.started":"2024-11-14T16:00:54.749282Z","shell.execute_reply":"2024-11-14T16:00:54.755574Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"First value in 'gsr_data': 179125.171875,178570.375,179391.03125,181556.21875,182420.515625,183646.203125,183727.078125,183940.0,184140.109375,184302.125,184497.453125,184425.84375,184151.828125,183712.84375,183200.3125,182930.90625,182558.046875,182371.484375,182351.875,182348.90625,182571.71875,182762.59375,182850.78125,183005.96875,183131.21875,183223.3125,183341.90625,183501.390625,183623.203125,183718.921875,183845.875,183990.0625,184070.0625,184106.53125,184181.75,184225.921875,184179.484375,184101.109375,184012.8125,183851.015625,183785.625,183725.21875,183676.765625,183682.390625,183646.03125,183785.234375,183910.671875,183894.5,184009.25,184028.03125,184072.859375,184162.5625,184114.109375,184188.609375,184215.5,184219.015625,184314.234375,184334.859375,184143.5625,183615.875,182805.71875,181927.59375,180957.71875,180380.265625,180069.375,179868.71875,179733.6875,179904.6875,180134.609375,180490.90625,180799.6875,181055.21875,181257.625,181439.09375,181634.703125,181800.921875,181920.0625,182107.703125,182163.03125,182333.625,182389.96875,182461.3125,182565.28125,182543.171875,182640.984375,182654.921875,182691.265625,182827.484375,182866.15625,182997.203125,183045.8125,183130.515625,183130.046875\nType of the first value: <class 'str'>\nThird value in 'gsr_data': 168857.21875,168781.234375,169132.53125,169823.0,170710.34375,171388.953125,172187.515625,172716.078125,173357.40625,173904.28125,174323.5625,174718.265625,175014.71875,175309.296875,175530.109375,175777.78125,175984.109375,176195.375,176381.4375,176576.296875,176749.59375,176954.421875,177131.3125,177294.34375,177346.453125,177531.609375,177766.203125,177894.71875,177950.171875,178103.15625,178181.046875,178255.0,178363.609375,178503.625,178543.53125,178680.40625,178786.609375,178833.0625,178843.828125,179016.65625,179132.59375,179219.21875,179301.3125,179397.328125,179474.671875,179588.25,179678.40625,179676.828125,179090.765625,177644.015625,175528.703125,173140.734375,171016.78125,169359.78125,168094.109375,167512.359375,167284.734375,167543.796875,168265.234375,169093.046875,169971.828125,170824.0625,171611.28125,172348.578125,172893.34375,173391.296875,173790.359375,174225.703125,174506.8125,174826.515625,175100.28125,175360.75,175597.671875,175794.1875,176006.546875,176196.96875,176361.34375,176525.625,176728.859375,176877.265625,177051.09375,177108.890625,177245.3125,177345.75,177275.859375,176441.875,175111.859375,173636.34375,172269.125,171142.0,170401.078125,169992.875,169993.890625,170285.96875,170852.0,171463.015625,172214.46875,172766.828125,173270.65625,173752.75,173954.890625,173402.5625,171716.21875,169250.234375,166271.671875,163741.578125,161828.671875,160911.34375,160645.578125,160936.40625,161622.578125,162663.734375,163788.28125,164938.421875,165878.953125,166861.53125,167621.125,168391.609375,169140.03125,169697.65625,170152.859375,170501.0625,170859.03125,171183.90625,171558.15625,171780.59375,172048.140625,172277.703125,172467.484375\nType of the third value: <class 'str'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TODO EDIT HERE 15 NOV 1223 AM","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n# from scipy.signal import morlet\nfrom scipy.stats import kurtosis\nimport pywt\n\n# Function to convert the string in 'gsr_data' to a list of floats\ndef convert_gsr_data_string_to_list(gsr_data_str):\n    \"\"\"\n    Convert a string of comma-separated numbers into a list of floats.\n    \"\"\"\n    return list(map(float, gsr_data_str.split(',')))\n\n# Example function to extract CWT coefficients and frequencies from GSR data\ndef extract_cwt_features(gsr_data, sampling_rate=4, wavelet='morlet'):\n    \"\"\"\n    Perform Continuous Wavelet Transform on GSR data and return coefficients and frequencies.\n    \n    Parameters:\n    - gsr_data: List or array of GSR signal values\n    - sampling_rate: The sampling frequency of the GSR data (default is 100Hz)\n    - wavelet: Type of wavelet to use (default is 'morlet')\n    \n    Returns:\n    - cwt_coefficients: The coefficients resulting from the CWT\n    - frequencies: The corresponding frequencies used for the CWT\n    \"\"\"\n    # Ensure gsr_data is a numpy array\n    gsr_signal = np.array(gsr_data)\n    scales = np.arange(1,271 )\n    \n    # Generate a time array for the signal (assuming uniform sampling)\n#     time_points = np.arange(len(gsr_signal)) / sampling_rate\n\n    \n    # Perform CWT using the specified wavelet (e.g., Morlet)\n    cwt_coefficients, cwt_frequencies = pywt.cwt(gsr_signal , scales, 'morl')\n    \n    \n    return cwt_coefficients\n\n# Function to extract features from CWT coefficients\ndef extract_cwt_features_from_coefficients(cwt_coefficients):\n    \"\"\"\n    Extract time-domain features from CWT coefficients.\n    \n    Parameters:\n    - cwt_coefficients: 2D array (scales x time points)\n    \n    Returns:\n    - A dictionary containing mean, variance, max, and kurtosis of the coefficients across time\n    \"\"\"\n    # Take the magnitude of the complex CWT coefficients (optional)\n#     cwt_coefficients_mag = np.abs(cwt_coefficients)\n    \n    # Extract features across scales\n    mean_coefficients = np.mean(cwt_coefficients, axis=1)  # Mean across time for each scale\n    variance_coefficients = np.var(cwt_coefficients, axis=1)  # Variance across time for each scale\n    max_coefficients = np.max(cwt_coefficients, axis=1)  # Max value across time for each scale\n    kurtosis_coefficients = kurtosis(cwt_coefficients, axis=1)  # Kurtosis across time for each scale\n    \n    return {\n        'mean_coefficients': mean_coefficients,\n        'variance_coefficients': variance_coefficients,\n        'max_coefficients': max_coefficients,\n        'kurtosis_coefficients': kurtosis_coefficients\n    }\n\n\n# TODO EDIT HERE\n# Apply CWT to the GSR data and extract features\ndef extract_features_for_all_rows(df):\n    features = []\n    \n    for _, row in df.iterrows():\n        gsr_data_str = row['gsr_data']\n        \n        # Convert the GSR data string into a list of numbers\n        gsr_data = convert_gsr_data_string_to_list(gsr_data_str)\n        \n        # Extract CWT coefficients for the GSR data\n        cwt_coefficients = extract_cwt_features(gsr_data)\n        \n        # Extract features from the CWT coefficients\n        cwt_features = extract_cwt_features_from_coefficients(cwt_coefficients)\n        \n        # Append features along with subject_id, video_id, and arousal_valence_label\n        feature_dict = {\n            'subject_id': row['subject_id'],\n            'video_id': row['video_id'],\n            'arousal_valence_label': row['arousal_valence_label'],\n            'mean_coefficients': cwt_features['mean_coefficients'],\n            'variance_coefficients': cwt_features['variance_coefficients'],\n            'max_coefficients': cwt_features['max_coefficients'],\n            'kurtosis_coefficients': cwt_features['kurtosis_coefficients']\n        }\n        features.append(feature_dict)\n    \n    # Convert the list of features to a DataFrame\n    gsr_features = pd.DataFrame(features)\n    return gsr_features\n\n\n\n# Extract features from all rows in the dataframe\ngsr_features = extract_features_for_all_rows(data_intervals_dataframe)\n\n# Verify the result\nprint(gsr_features.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:18:46.582743Z","iopub.execute_input":"2024-11-14T16:18:46.583724Z","iopub.status.idle":"2024-11-14T16:19:59.583427Z","shell.execute_reply.started":"2024-11-14T16:18:46.583680Z","shell.execute_reply":"2024-11-14T16:19:59.582430Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"   subject_id  video_id arousal_valence_label  \\\n0           7         0                  AMVM   \n1           7         1                  AMVL   \n2           7         2                  AMVM   \n3           7         3                  AMVM   \n4           7         4                  AMVM   \n\n                                   mean_coefficients  \\\n0  [-489.9000642715322, 249.95762568207218, 695.7...   \n1  [-371.2989675276448, 188.24113022620622, 525.7...   \n2  [-332.6078408435772, 169.23948007562888, 471.8...   \n3  [-372.0883787120657, 189.65727544186242, 527.2...   \n4  [-382.7985041236772, 194.2971010261761, 541.83...   \n\n                               variance_coefficients  \\\n0  [10424253.014866954, 47513031.93392268, 119945...   \n1  [8615438.61031906, 38099101.09500066, 98870125...   \n2  [6746045.8880098285, 30099843.863651246, 76993...   \n3  [8604120.643016601, 38406572.917543426, 961804...   \n4  [8918730.417302297, 39859255.04288079, 1013952...   \n\n                                    max_coefficients  \\\n0  [339.4089643271199, 32354.91354616913, 62725.4...   \n1  [2196.5536986904553, 35063.16562950425, 68007....   \n2  [375.37618328145095, 30463.368016095417, 59111...   \n3  [2164.1569483368257, 35808.82562922646, 69437....   \n4  [351.2256156866179, 34893.8636077458, 67664.89...   \n\n                               kurtosis_coefficients  \n0  [41.44520067298723, 17.03968557684593, 21.7668...  \n1  [56.61807830847583, 24.275741024689577, 29.752...  \n2  [59.281974659306364, 24.85957835324311, 31.587...  \n3  [58.431300011193805, 24.604896432587143, 31.81...  \n4  [59.32110029591758, 24.823198318525332, 31.665...  \n","output_type":"stream"}]},{"cell_type":"code","source":"gsr_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:02:08.295598Z","iopub.execute_input":"2024-11-14T16:02:08.296015Z","iopub.status.idle":"2024-11-14T16:02:08.303425Z","shell.execute_reply.started":"2024-11-14T16:02:08.295978Z","shell.execute_reply":"2024-11-14T16:02:08.302369Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"(2336, 7)"},"metadata":{}}]},{"cell_type":"markdown","source":"# IGNORE FROM HERE\n\n# Multi input LSTM with separate layer for PPG & GSR","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, random_split, TensorDataset\n# import matplotlib.pyplot as plt\n\n# # Check if GPU is available\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(f\"Using device: {device}\")\n\n# # Assume data_intervals is already populated from previous steps\n\n# # Split data into train and test sets (80% train, 20% test)\n# train_size = int(0.8 * len(data_intervals))\n# test_size = len(data_intervals) - train_size\n# train_data, test_data = random_split(data_intervals, [train_size, test_size])\n\n# # Convert data_intervals entries into tensors for PyTorch\n# def prepare_data(data):\n#     ppg_sequences = []\n#     gsr_sequences = []\n#     labels = []\n\n#     for item in data:\n#         ppg_sequences.append(torch.tensor(item['ppg_data'], dtype=torch.float32))\n#         gsr_sequences.append(torch.tensor(item['gsr_data'], dtype=torch.float32))\n#         labels.append(torch.tensor([item['arousal'], item['valence']], dtype=torch.float32))\n\n#     # Stack sequences and labels to create tensor batches\n#     ppg_sequences = nn.utils.rnn.pad_sequence(ppg_sequences, batch_first=True)\n#     gsr_sequences = nn.utils.rnn.pad_sequence(gsr_sequences, batch_first=True)\n#     labels = torch.stack(labels)\n\n#     return TensorDataset(ppg_sequences, gsr_sequences, labels)\n\n# train_dataset = prepare_data(train_data)\n# test_dataset = prepare_data(test_data)\n\n# # Load data into DataLoaders\n# batch_size = 32\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# # Define the LSTM model with separate layers for PPG and GSR data\n# class MultiInputLSTM(nn.Module):\n#     def __init__(self, input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers=1, dropout=0.3):\n#         super(MultiInputLSTM, self).__init__()\n#         # LSTM for PPG data\n#         self.lstm_ppg = nn.LSTM(input_size_ppg, hidden_size, num_layers, batch_first=True, dropout=dropout)\n        \n#         # LSTM for GSR data\n#         self.lstm_gsr = nn.LSTM(input_size_gsr, hidden_size, num_layers, batch_first=True, dropout=dropout)\n        \n#         # Fully connected layer to combine the outputs and predict arousal and valence\n#         self.fc = nn.Linear(hidden_size * 2, output_size)\n#         self.fc_dropout = nn.Dropout(dropout)  # Dropout after fully connected layer\n\n#     def forward(self, ppg_data, gsr_data):\n#         # PPG LSTM forward\n#         _, (h_ppg, _) = self.lstm_ppg(ppg_data)\n#         h_ppg = h_ppg[-1]  # Take the final hidden state\n\n#         # GSR LSTM forward\n#         _, (h_gsr, _) = self.lstm_gsr(gsr_data)\n#         h_gsr = h_gsr[-1]  # Take the final hidden state\n\n#         # Concatenate the hidden states from both LSTMs\n#         combined = torch.cat((h_ppg, h_gsr), dim=1)\n        \n#         # Apply dropout after fully connected layer\n#         output = self.fc(combined)\n#         output = self.fc_dropout(output)\n#         return output\n\n# # Model parameters\n# input_size_ppg = 1  # Assuming PPG data is a single feature per timestep\n# input_size_gsr = 1  # Assuming GSR data is a single feature per timestep\n# hidden_size = 128   # Changed hidden size\n# output_size = 2  # Arousal and Valence\n# num_layers = 2    # Changed number of layers\n# dropout = 0.3     # Dropout rate\n\n# # Instantiate the model, loss function, and optimizer with weight decay (L2 regularization)\n# model = MultiInputLSTM(input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers, dropout)\n# model.to(device)  # Move model to GPU if available\n# criterion = nn.MSELoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)  # Added weight decay\n\n# # Training function with validation evaluation\n# def train(model, train_loader, test_loader, criterion, optimizer, epochs=500):\n#     train_losses = []\n#     val_losses = []\n#     train_accuracies = []\n#     val_accuracies = []\n\n#     for epoch in range(epochs):\n#         model.train()\n#         total_train_loss = 0\n#         correct_train_predictions = 0\n#         total_train_predictions = 0\n\n#         for ppg_data, gsr_data, labels in train_loader:\n#             ppg_data = ppg_data.unsqueeze(-1).to(device)  # Move data to GPU\n#             gsr_data = gsr_data.unsqueeze(-1).to(device)  # Move data to GPU\n#             labels = labels.to(device)  # Move labels to GPU\n            \n#             # Zero the gradients\n#             optimizer.zero_grad()\n            \n#             # Forward pass\n#             outputs = model(ppg_data, gsr_data)\n#             loss = criterion(outputs, labels)\n#             total_train_loss += loss.item()\n            \n#             # Backward and optimize\n#             loss.backward()\n#             optimizer.step()\n            \n#             # Calculate training accuracy\n#             predictions = outputs.round()  # Rounding predictions to nearest integer\n#             correct_train_predictions += (predictions == labels).sum().item()\n#             total_train_predictions += labels.numel()\n\n#         # Calculate average training loss and accuracy\n#         avg_train_loss = total_train_loss / len(train_loader)\n#         train_losses.append(avg_train_loss)\n#         train_accuracy = (correct_train_predictions / total_train_predictions) * 100\n#         train_accuracies.append(train_accuracy)\n        \n#         # Evaluate on validation set\n#         val_loss, val_accuracy = evaluate(model, test_loader, criterion)\n#         val_losses.append(val_loss)\n#         val_accuracies.append(val_accuracy)\n\n#         print(f\"Epoch [{epoch+1}/{epochs}], \"\n#               f\"Train Loss: {avg_train_loss:.4f}, \"\n#               f\"Train Accuracy: {train_accuracy:.2f}%, \"\n#               f\"Validation Loss: {val_loss:.4f}, \"\n#               f\"Validation Accuracy: {val_accuracy:.2f}%\")\n\n#     return train_losses, val_losses, train_accuracies, val_accuracies\n\n# # Evaluation function\n# def evaluate(model, data_loader, criterion):\n#     model.eval()\n#     total_loss = 0\n#     correct_predictions = 0\n#     total_predictions = 0\n\n#     with torch.no_grad():\n#         for ppg_data, gsr_data, labels in data_loader:\n#             ppg_data = ppg_data.unsqueeze(-1).to(device)  # Move data to GPU\n#             gsr_data = gsr_data.unsqueeze(-1).to(device)  # Move data to GPU\n#             labels = labels.to(device)  # Move labels to GPU\n#             outputs = model(ppg_data, gsr_data)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n\n#             # Calculate accuracy based on mean absolute error tolerance\n#             predictions = outputs.round()  # Rounding predictions to nearest integer\n#             correct_predictions += (predictions == labels).sum().item()\n#             total_predictions += labels.numel()\n\n#     avg_loss = total_loss / len(data_loader)\n#     accuracy = (correct_predictions / total_predictions) * 100\n#     return avg_loss, accuracy\n\n# # Train the model and retrieve metrics\n# epochs = 500  # Changed epochs to 500\n# train_losses, val_losses, train_accuracies, val_accuracies = train(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n\n# # Plotting training and validation loss\n# plt.figure(figsize=(12, 6))\n# plt.subplot(1, 2, 1)\n# plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n# plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.title('Training and Validation Loss')\n\n# # Plotting training and validation accuracy\n# plt.subplot(1, 2, 2)\n# plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')\n# plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy (%)')\n# plt.legend()\n# plt.title('Training and Validation Accuracy')\n\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:02:41.732485Z","iopub.execute_input":"2024-11-13T19:02:41.732924Z","iopub.status.idle":"2024-11-13T19:09:55.918421Z","shell.execute_reply.started":"2024-11-13T19:02:41.732886Z","shell.execute_reply":"2024-11-13T19:09:55.91716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categorical Ouptuts without label encoding","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, random_split, TensorDataset\n# import matplotlib.pyplot as plt\n\n# # Assume data_intervals is already populated from previous steps\n\n# # Split data into train and test sets (80% train, 20% test)\n# train_size = int(0.8 * len(data_intervals))\n# test_size = len(data_intervals) - train_size\n# train_data, test_data = random_split(data_intervals, [train_size, test_size])\n\n# # Convert data_intervals entries into tensors for PyTorch\n# def prepare_data(data):\n#     ppg_sequences = []\n#     gsr_sequences = []\n#     labels = []\n\n#     for item in data:\n#         ppg_sequences.append(torch.tensor(item['ppg_data'], dtype=torch.float32))\n#         gsr_sequences.append(torch.tensor(item['gsr_data'], dtype=torch.float32))\n#         # Convert arousal and valence to integer class indices (1-9 for both, adjusting to 0-8)\n#         arousal_class = int(item['arousal']) - 1  # Adjusting arousal to range 0-8\n#         valence_class = int(item['valence']) - 1  # Adjusting valence to range 0-8\n#         # Combine the classes into a single value (arousal class * 9 + valence class)\n#         combined_class = arousal_class * 9 + valence_class\n#         labels.append(torch.tensor(combined_class, dtype=torch.long))  # Long tensor for classification\n\n#     # Stack sequences and labels to create tensor batches\n#     ppg_sequences = nn.utils.rnn.pad_sequence(ppg_sequences, batch_first=True)\n#     gsr_sequences = nn.utils.rnn.pad_sequence(gsr_sequences, batch_first=True)\n#     labels = torch.stack(labels)\n\n#     return TensorDataset(ppg_sequences, gsr_sequences, labels)\n\n# train_dataset = prepare_data(train_data)\n# test_dataset = prepare_data(test_data)\n\n# # Load data into DataLoaders\n# batch_size = 32\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# # Define the LSTM model with separate layers for PPG and GSR data\n# class MultiInputLSTM(nn.Module):\n#     def __init__(self, input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers=1):\n#         super(MultiInputLSTM, self).__init__()\n#         # LSTM for PPG data\n#         self.lstm_ppg = nn.LSTM(input_size_ppg, hidden_size, num_layers, batch_first=True)\n        \n#         # LSTM for GSR data\n#         self.lstm_gsr = nn.LSTM(input_size_gsr, hidden_size, num_layers, batch_first=True)\n        \n#         # Fully connected layer to combine the outputs and predict arousal and valence\n#         self.fc = nn.Linear(hidden_size * 2, output_size)\n\n#     def forward(self, ppg_data, gsr_data):\n#         # PPG LSTM forward\n#         _, (h_ppg, _) = self.lstm_ppg(ppg_data)\n#         h_ppg = h_ppg[-1]  # Take the final hidden state\n\n#         # GSR LSTM forward\n#         _, (h_gsr, _) = self.lstm_gsr(gsr_data)\n#         h_gsr = h_gsr[-1]  # Take the final hidden state\n\n#         # Concatenate the hidden states from both LSTMs\n#         combined = torch.cat((h_ppg, h_gsr), dim=1)\n        \n#         # Fully connected output layer with softmax for categorical prediction\n#         output = self.fc(combined)\n#         return output\n\n# # Model parameters\n# input_size_ppg = 1  # Assuming PPG data is a single feature per timestep\n# input_size_gsr = 1  # Assuming GSR data is a single feature per timestep\n# hidden_size = 128   # Updated hidden size\n# output_size = 81    # 81 classes for combined arousal (0-8) and valence (0-8)\n# num_layers = 1\n\n# # Instantiate the model, loss function, and optimizer\n# model = MultiInputLSTM(input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers)\n# criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification\n# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# # Training function with validation evaluation\n# def train(model, train_loader, test_loader, criterion, optimizer, epochs=500):\n#     train_losses = []\n#     val_losses = []\n#     train_accuracies = []\n#     val_accuracies = []\n\n#     for epoch in range(epochs):\n#         model.train()\n#         total_train_loss = 0\n#         correct_train_predictions = 0\n#         total_train_predictions = 0\n\n#         for ppg_data, gsr_data, labels in train_loader:\n#             ppg_data = ppg_data.unsqueeze(-1)\n#             gsr_data = gsr_data.unsqueeze(-1)\n            \n#             # Zero the gradients\n#             optimizer.zero_grad()\n            \n#             # Forward pass\n#             outputs = model(ppg_data, gsr_data)\n#             loss = criterion(outputs, labels)  # Directly use labels without reshaping\n#             total_train_loss += loss.item()\n            \n#             # Backward and optimize\n#             loss.backward()\n#             optimizer.step()\n            \n#             # Calculate training accuracy\n#             _, predicted = torch.max(outputs, 1)  # Get the predicted class index\n#             correct_train_predictions += (predicted == labels).sum().item()\n#             total_train_predictions += labels.numel()\n        \n#         # Calculate average training loss and accuracy\n#         avg_train_loss = total_train_loss / len(train_loader)\n#         train_losses.append(avg_train_loss)\n#         train_accuracy = (correct_train_predictions / total_train_predictions) * 100\n#         train_accuracies.append(train_accuracy)\n        \n#         # Evaluate on validation set\n#         val_loss, val_accuracy = evaluate(model, test_loader, criterion)\n#         val_losses.append(val_loss)\n#         val_accuracies.append(val_accuracy)\n\n#         print(f\"Epoch [{epoch+1}/{epochs}], \"\n#               f\"Train Loss: {avg_train_loss:.4f}, \"\n#               f\"Train Accuracy: {train_accuracy:.2f}%, \"\n#               f\"Validation Loss: {val_loss:.4f}, \"\n#               f\"Validation Accuracy: {val_accuracy:.2f}%\")\n\n#     return train_losses, val_losses, train_accuracies, val_accuracies\n\n# # Evaluation function\n# def evaluate(model, data_loader, criterion):\n#     model.eval()\n#     total_loss = 0\n#     correct_predictions = 0\n#     total_predictions = 0\n\n#     with torch.no_grad():\n#         for ppg_data, gsr_data, labels in data_loader:\n#             ppg_data = ppg_data.unsqueeze(-1)\n#             gsr_data = gsr_data.unsqueeze(-1)\n#             outputs = model(ppg_data, gsr_data)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n\n#             # Calculate accuracy\n#             _, predicted = torch.max(outputs, 1)\n#             correct_predictions += (predicted == labels).sum().item()\n#             total_predictions += labels.numel()\n\n#     avg_loss = total_loss / len(data_loader)\n#     accuracy = (correct_predictions / total_predictions) * 100\n#     return avg_loss, accuracy\n\n# # Train the model and retrieve metrics\n# epochs = 500  # Updated epochs to 500\n# train_losses, val_losses, train_accuracies, val_accuracies = train(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n\n# # Plotting training and validation loss\n# plt.figure(figsize=(12, 6))\n# plt.subplot(1, 2, 1)\n# plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n# plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.title('Training and Validation Loss')\n\n# # Plotting training and validation accuracy\n# plt.subplot(1, 2, 2)\n# plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')\n# plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy (%)')\n# plt.legend()\n# plt.title('Training and Validation Accuracy')\n\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:27:46.018357Z","iopub.execute_input":"2024-11-13T19:27:46.019327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categorical Outputs with label encoding low medium high A & V","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split, TensorDataset\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport random\nimport numpy as np\n\n# Set seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Check if GPU is available and set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Assume data_intervals is already populated from previous steps\n\n# Initialize the LabelEncoder for 9 unique arousal-valence combinations\nlabel_encoder = LabelEncoder()\narousal_valence_labels = [item['arousal_valence_label'] for item in data_intervals]\nlabel_encoder.fit(arousal_valence_labels)\n\n# Split data into train and test sets (80% train, 20% test)\ntrain_size = int(0.8 * len(data_intervals))\ntest_size = len(data_intervals) - train_size\ntrain_data, test_data = random_split(data_intervals, [train_size, test_size])\n\ndef prepare_data(data):\n    ppg_sequences = []\n    gsr_sequences = []\n    labels = []\n\n    for item in data:\n        ppg_sequences.append(torch.tensor(item['ppg_data'], dtype=torch.float32))\n        gsr_sequences.append(torch.tensor(item['gsr_data'], dtype=torch.float32))\n        combined_class = label_encoder.transform([item['arousal_valence_label']])[0]\n        labels.append(torch.tensor(combined_class, dtype=torch.long))\n\n    ppg_sequences = nn.utils.rnn.pad_sequence(ppg_sequences, batch_first=True)\n    gsr_sequences = nn.utils.rnn.pad_sequence(gsr_sequences, batch_first=True)\n    labels = torch.stack(labels)\n\n    return TensorDataset(ppg_sequences, gsr_sequences, labels)\n\ntrain_dataset = prepare_data(train_data)\ntest_dataset = prepare_data(test_data)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nclass MultiInputLSTM(nn.Module):\n    def __init__(self, input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers=1):\n        super(MultiInputLSTM, self).__init__()\n        self.lstm_ppg = nn.LSTM(input_size_ppg, hidden_size, num_layers, batch_first=True)\n        self.lstm_gsr = nn.LSTM(input_size_gsr, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n\n    def forward(self, ppg_data, gsr_data):\n        _, (h_ppg, _) = self.lstm_ppg(ppg_data)\n        h_ppg = h_ppg[-1]\n        _, (h_gsr, _) = self.lstm_gsr(gsr_data)\n        h_gsr = h_gsr[-1]\n        combined = torch.cat((h_ppg, h_gsr), dim=1)\n        output = self.fc(combined)\n        return output\n\ninput_size_ppg = 1\ninput_size_gsr = 1\nhidden_size = 128\noutput_size = 9\nnum_layers = 1\n\n# Instantiate the model, loss function, and optimizer\nmodel = MultiInputLSTM(input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Function to train the model with saving best model state\ndef train(model, train_loader, test_loader, criterion, optimizer, epochs=500):\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n    best_val_accuracy = 0.0  # Track the best validation accuracy\n    best_model_state = None  # To store the best model state\n\n    for epoch in range(epochs):\n        model.train()\n        total_train_loss = 0\n        correct_train_predictions = 0\n        total_train_predictions = 0\n\n        for ppg_data, gsr_data, labels in train_loader:\n            ppg_data = ppg_data.unsqueeze(-1).to(device)\n            gsr_data = gsr_data.unsqueeze(-1).to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(ppg_data, gsr_data)\n            loss = criterion(outputs, labels)\n            total_train_loss += loss.item()\n\n            loss.backward()\n            optimizer.step()\n\n            _, predicted = torch.max(outputs, 1)\n            correct_train_predictions += (predicted == labels).sum().item()\n            total_train_predictions += labels.numel()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        train_accuracy = (correct_train_predictions / total_train_predictions) * 100\n        train_accuracies.append(train_accuracy)\n\n        val_loss, val_accuracy = evaluate(model, test_loader, criterion)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n\n        # Check if current model is the best, and save if it is\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            best_model_state = model.state_dict()  # Save the model state\n\n        print(f\"Epoch [{epoch+1}/{epochs}], \"\n              f\"Train Loss: {avg_train_loss:.4f}, \"\n              f\"Train Accuracy: {train_accuracy:.2f}%, \"\n              f\"Validation Loss: {val_loss:.4f}, \"\n              f\"Validation Accuracy: {val_accuracy:.2f}%\")\n\n    # Save the best model state to a file\n    torch.save(best_model_state, \"best_model.pth\")\n    print(f\"Best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n\n    return train_losses, val_losses, train_accuracies, val_accuracies\n\ndef evaluate(model, data_loader, criterion):\n    model.eval()\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for ppg_data, gsr_data, labels in data_loader:\n            ppg_data = ppg_data.unsqueeze(-1).to(device)\n            gsr_data = gsr_data.unsqueeze(-1).to(device)\n            labels = labels.to(device)\n\n            outputs = model(ppg_data, gsr_data)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_predictions += labels.numel()\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = (correct_predictions / total_predictions) * 100\n    return avg_loss, accuracy\n\nepochs = 500\ntrain_losses, val_losses, train_accuracies, val_accuracies = train(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\nplt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')\nplt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy (%)')\nplt.legend()\nplt.title('Training and Validation Accuracy')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T03:55:49.516294Z","iopub.execute_input":"2024-11-14T03:55:49.516687Z","iopub.status.idle":"2024-11-14T04:00:16.972822Z","shell.execute_reply.started":"2024-11-14T03:55:49.516642Z","shell.execute_reply":"2024-11-14T04:00:16.971517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regression values as outputs","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, random_split, TensorDataset\n# import matplotlib.pyplot as plt\n\n# # Assume data_intervals is already populated from previous steps\n\n# # Check if GPU is available and use it\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Split data into train and test sets (80% train, 20% test)\n# train_size = int(0.8 * len(data_intervals))\n# test_size = len(data_intervals) - train_size\n# train_data, test_data = random_split(data_intervals, [train_size, test_size])\n\n# # Convert data_intervals entries into tensors for PyTorch\n# def prepare_data(data):\n#     ppg_sequences = []\n#     gsr_sequences = []\n#     labels = []\n\n#     for item in data:\n#         ppg_sequences.append(torch.tensor(item['ppg_data'], dtype=torch.float32))\n#         gsr_sequences.append(torch.tensor(item['gsr_data'], dtype=torch.float32))\n#         labels.append(torch.tensor([item['arousal'], item['valence']], dtype=torch.float32))\n\n#     # Stack sequences and labels to create tensor batches\n#     ppg_sequences = nn.utils.rnn.pad_sequence(ppg_sequences, batch_first=True)\n#     gsr_sequences = nn.utils.rnn.pad_sequence(gsr_sequences, batch_first=True)\n#     labels = torch.stack(labels)\n\n#     return TensorDataset(ppg_sequences, gsr_sequences, labels)\n\n# train_dataset = prepare_data(train_data)\n# test_dataset = prepare_data(test_data)\n\n# # Load data into DataLoaders\n# batch_size = 32\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# # Define the LSTM model with separate layers for PPG and GSR data\n# class MultiInputLSTM(nn.Module):\n#     def __init__(self, input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers=1):\n#         super(MultiInputLSTM, self).__init__()\n#         # LSTM for PPG data\n#         self.lstm_ppg = nn.LSTM(input_size_ppg, hidden_size, num_layers, batch_first=True)\n        \n#         # LSTM for GSR data\n#         self.lstm_gsr = nn.LSTM(input_size_gsr, hidden_size, num_layers, batch_first=True)\n        \n#         # Fully connected layer to combine the outputs and predict arousal and valence\n#         self.fc = nn.Linear(hidden_size * 2, output_size)\n\n#     def forward(self, ppg_data, gsr_data):\n#         # PPG LSTM forward\n#         _, (h_ppg, _) = self.lstm_ppg(ppg_data)\n#         h_ppg = h_ppg[-1]  # Take the final hidden state\n\n#         # GSR LSTM forward\n#         _, (h_gsr, _) = self.lstm_gsr(gsr_data)\n#         h_gsr = h_gsr[-1]  # Take the final hidden state\n\n#         # Concatenate the hidden states from both LSTMs\n#         combined = torch.cat((h_ppg, h_gsr), dim=1)\n        \n#         # Fully connected output layer\n#         output = self.fc(combined)\n#         return output\n\n# # Model parameters\n# input_size_ppg = 1  # Assuming PPG data is a single feature per timestep\n# input_size_gsr = 1  # Assuming GSR data is a single feature per timestep\n# hidden_size = 128   # Updated hidden size\n# output_size = 2     # Arousal and Valence\n# num_layers = 1\n\n# # Instantiate the model, loss function, and optimizer\n# model = MultiInputLSTM(input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers)\n# model = model.to(device)  # Move model to GPU if available\n# criterion = nn.MSELoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# # Training function with validation evaluation\n# def train(model, train_loader, test_loader, criterion, optimizer, epochs=500):\n#     train_losses = []\n#     val_losses = []\n#     train_accuracies = []\n#     val_accuracies = []\n\n#     for epoch in range(epochs):\n#         model.train()\n#         total_train_loss = 0\n#         correct_train_predictions = 0\n#         total_train_predictions = 0\n\n#         for ppg_data, gsr_data, labels in train_loader:\n#             ppg_data = ppg_data.unsqueeze(-1).to(device)  # Move data to GPU\n#             gsr_data = gsr_data.unsqueeze(-1).to(device)  # Move data to GPU\n#             labels = labels.to(device)  # Move labels to GPU\n            \n#             # Zero the gradients\n#             optimizer.zero_grad()\n            \n#             # Forward pass\n#             outputs = model(ppg_data, gsr_data)\n#             loss = criterion(outputs, labels)\n#             total_train_loss += loss.item()\n            \n#             # Backward and optimize\n#             loss.backward()\n#             optimizer.step()\n            \n#             # Calculate training accuracy\n#             predictions = outputs.round()  # Rounding predictions to nearest integer\n#             correct_train_predictions += (predictions == labels).sum().item()\n#             total_train_predictions += labels.numel()\n        \n#         # Calculate average training loss and accuracy\n#         avg_train_loss = total_train_loss / len(train_loader)\n#         train_losses.append(avg_train_loss)\n#         train_accuracy = (correct_train_predictions / total_train_predictions) * 100\n#         train_accuracies.append(train_accuracy)\n        \n#         # Evaluate on validation set\n#         val_loss, val_accuracy = evaluate(model, test_loader, criterion)\n#         val_losses.append(val_loss)\n#         val_accuracies.append(val_accuracy)\n\n#         print(f\"Epoch [{epoch+1}/{epochs}], \"\n#               f\"Train Loss: {avg_train_loss:.4f}, \"\n#               f\"Train Accuracy: {train_accuracy:.2f}%, \"\n#               f\"Validation Loss: {val_loss:.4f}, \"\n#               f\"Validation Accuracy: {val_accuracy:.2f}%\")\n\n#     return train_losses, val_losses, train_accuracies, val_accuracies\n\n# # Evaluation function\n# def evaluate(model, data_loader, criterion):\n#     model.eval()\n#     total_loss = 0\n#     correct_predictions = 0\n#     total_predictions = 0\n\n#     with torch.no_grad():\n#         for ppg_data, gsr_data, labels in data_loader:\n#             ppg_data = ppg_data.unsqueeze(-1).to(device)  # Move data to GPU\n#             gsr_data = gsr_data.unsqueeze(-1).to(device)  # Move data to GPU\n#             labels = labels.to(device)  # Move labels to GPU\n#             outputs = model(ppg_data, gsr_data)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n\n#             # Calculate accuracy based on mean absolute error tolerance\n#             predictions = outputs.round()  # Rounding predictions to nearest integer\n#             correct_predictions += (predictions == labels).sum().item()\n#             total_predictions += labels.numel()\n\n#     avg_loss = total_loss / len(data_loader)\n#     accuracy = (correct_predictions / total_predictions) * 100\n#     return avg_loss, accuracy\n\n# # Train the model and retrieve metrics\n# epochs = 500  # Updated epochs to 500\n# train_losses, val_losses, train_accuracies, val_accuracies = train(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n\n# # Plotting training and validation loss\n# plt.figure(figsize=(12, 6))\n# plt.subplot(1, 2, 1)\n# plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n# plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.title('Training and Validation Loss')\n\n# # Plotting training and validation accuracy\n# plt.subplot(1, 2, 2)\n# plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')\n# plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy (%)')\n# plt.legend()\n# plt.title('Training and Validation Accuracy')\n\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T19:15:37.52818Z","iopub.execute_input":"2024-11-13T19:15:37.528672Z","iopub.status.idle":"2024-11-13T19:20:12.779908Z","shell.execute_reply.started":"2024-11-13T19:15:37.528631Z","shell.execute_reply":"2024-11-13T19:20:12.778575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, random_split, TensorDataset\n# import matplotlib.pyplot as plt\n\n# # Assume data_intervals is already populated from previous steps\n\n# # Split data into train and test sets (80% train, 20% test)\n# train_size = int(0.8 * len(data_intervals))\n# test_size = len(data_intervals) - train_size\n# train_data, test_data = random_split(data_intervals, [train_size, test_size])\n\n# # Convert data_intervals entries into tensors for PyTorch\n# def prepare_data(data):\n#     ppg_sequences = []\n#     gsr_sequences = []\n#     labels = []\n\n#     for item in data:\n#         ppg_sequences.append(torch.tensor(item['ppg_data'], dtype=torch.float32))\n#         gsr_sequences.append(torch.tensor(item['gsr_data'], dtype=torch.float32))\n#         labels.append(torch.tensor([item['arousal'], item['valence']], dtype=torch.float32))\n\n#     # Stack sequences and labels to create tensor batches\n#     ppg_sequences = nn.utils.rnn.pad_sequence(ppg_sequences, batch_first=True)\n#     gsr_sequences = nn.utils.rnn.pad_sequence(gsr_sequences, batch_first=True)\n#     labels = torch.stack(labels)\n\n#     return TensorDataset(ppg_sequences, gsr_sequences, labels)\n\n# train_dataset = prepare_data(train_data)\n# test_dataset = prepare_data(test_data)\n\n# # Load data into DataLoaders\n# batch_size = 32\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# # Define the LSTM model with separate layers for PPG and GSR data\n# class MultiInputLSTM(nn.Module):\n#     def __init__(self, input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers=1):\n#         super(MultiInputLSTM, self).__init__()\n#         # LSTM for PPG data\n#         self.lstm_ppg = nn.LSTM(input_size_ppg, hidden_size, num_layers, batch_first=True)\n        \n#         # LSTM for GSR data\n#         self.lstm_gsr = nn.LSTM(input_size_gsr, hidden_size, num_layers, batch_first=True)\n        \n#         # Fully connected layer to combine the outputs and predict arousal and valence\n#         self.fc = nn.Linear(hidden_size * 2, output_size)\n\n#     def forward(self, ppg_data, gsr_data):\n#         # PPG LSTM forward\n#         _, (h_ppg, _) = self.lstm_ppg(ppg_data)\n#         h_ppg = h_ppg[-1]  # Take the final hidden state\n\n#         # GSR LSTM forward\n#         _, (h_gsr, _) = self.lstm_gsr(gsr_data)\n#         h_gsr = h_gsr[-1]  # Take the final hidden state\n\n#         # Concatenate the hidden states from both LSTMs\n#         combined = torch.cat((h_ppg, h_gsr), dim=1)\n        \n#         # Fully connected output layer\n#         output = self.fc(combined)\n#         return output\n\n# # Model parameters\n# input_size_ppg = 1  # Assuming PPG data is a single feature per timestep\n# input_size_gsr = 1  # Assuming GSR data is a single feature per timestep\n# hidden_size = 64\n# output_size = 2  # Arousal and Valence\n# num_layers = 1\n\n# # Instantiate the model, loss function, and optimizer\n# model = MultiInputLSTM(input_size_ppg, input_size_gsr, hidden_size, output_size, num_layers)\n# criterion = nn.MSELoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# # Training function with validation evaluation\n# def train(model, train_loader, test_loader, criterion, optimizer, epochs=50):\n#     train_losses = []\n#     val_losses = []\n#     train_accuracies = []\n#     val_accuracies = []\n\n#     for epoch in range(epochs):\n#         model.train()\n#         total_train_loss = 0\n\n#         for ppg_data, gsr_data, labels in train_loader:\n#             ppg_data = ppg_data.unsqueeze(-1)\n#             gsr_data = gsr_data.unsqueeze(-1)\n            \n#             # Zero the gradients\n#             optimizer.zero_grad()\n            \n#             # Forward pass\n#             outputs = model(ppg_data, gsr_data)\n#             loss = criterion(outputs, labels)\n#             total_train_loss += loss.item()\n            \n#             # Backward and optimize\n#             loss.backward()\n#             optimizer.step()\n        \n#         # Calculate average training loss\n#         avg_train_loss = total_train_loss / len(train_loader)\n#         train_losses.append(avg_train_loss)\n        \n#         # Evaluate on validation set\n#         val_loss, val_accuracy = evaluate(model, test_loader, criterion)\n#         val_losses.append(val_loss)\n#         val_accuracies.append(val_accuracy)\n\n#         print(f\"Epoch [{epoch+1}/{epochs}], \"\n#               f\"Train Loss: {avg_train_loss:.4f}, \"\n#               f\"Validation Loss: {val_loss:.4f}, \"\n#               f\"Validation Accuracy: {val_accuracy:.2f}%\")\n\n#     return train_losses, val_losses, val_accuracies\n\n# # Evaluation function\n# def evaluate(model, data_loader, criterion):\n#     model.eval()\n#     total_loss = 0\n#     correct_predictions = 0\n#     total_predictions = 0\n\n#     with torch.no_grad():\n#         for ppg_data, gsr_data, labels in data_loader:\n#             ppg_data = ppg_data.unsqueeze(-1)\n#             gsr_data = gsr_data.unsqueeze(-1)\n#             outputs = model(ppg_data, gsr_data)\n#             loss = criterion(outputs, labels)\n#             total_loss += loss.item()\n\n#             # Calculate accuracy based on mean absolute error tolerance\n#             predictions = outputs.round()  # Rounding predictions to nearest integer\n#             correct_predictions += (predictions == labels).sum().item()\n#             total_predictions += labels.numel()\n\n#     avg_loss = total_loss / len(data_loader)\n#     accuracy = (correct_predictions / total_predictions) * 100\n#     return avg_loss, accuracy\n\n# # Train the model and retrieve metrics\n# epochs = 200\n# train_losses, val_losses, val_accuracies = train(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n\n# # Plotting training and validation loss\n# plt.figure(figsize=(12, 6))\n# plt.subplot(1, 2, 1)\n# plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n# plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.title('Training and Validation Loss')\n\n# # Plotting validation accuracy\n# plt.subplot(1, 2, 2)\n# plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy (%)')\n# plt.legend()\n# plt.title('Validation Accuracy')\n\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T18:01:36.457614Z","iopub.execute_input":"2024-11-13T18:01:36.457985Z","iopub.status.idle":"2024-11-13T18:02:52.103983Z","shell.execute_reply.started":"2024-11-13T18:01:36.457952Z","shell.execute_reply":"2024-11-13T18:02:52.102672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Get unique values in the start_stop_trigger column\n# unique_triggers = sample_raw_ppg['start_stop_trigger'].unique()\n# unique_triggers_num = sample_raw_ppg['start_stop_trigger'].nunique()\n# # Print the unique start_stop_trigger values\n# print(\"Unique start_stop_triggers in PPG data:\", unique_triggers)\n# print(unique_triggers_num)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T17:29:30.368297Z","iopub.execute_input":"2024-11-13T17:29:30.368995Z","iopub.status.idle":"2024-11-13T17:29:30.373354Z","shell.execute_reply.started":"2024-11-13T17:29:30.368944Z","shell.execute_reply":"2024-11-13T17:29:30.372288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(sample_raw_gsr['gsr_value'])\n\n# gsr_10 = sample_raw_gsr['gsr_value']\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:31.670392Z","iopub.execute_input":"2024-11-10T09:07:31.671179Z","iopub.status.idle":"2024-11-10T09:07:31.679014Z","shell.execute_reply.started":"2024-11-10T09:07:31.671132Z","shell.execute_reply":"2024-11-10T09:07:31.678008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(gsr_10)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:31.680253Z","iopub.execute_input":"2024-11-10T09:07:31.681168Z","iopub.status.idle":"2024-11-10T09:07:31.689479Z","shell.execute_reply.started":"2024-11-10T09:07:31.681082Z","shell.execute_reply":"2024-11-10T09:07:31.688427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gsr_10.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:31.690581Z","iopub.execute_input":"2024-11-10T09:07:31.691518Z","iopub.status.idle":"2024-11-10T09:07:31.698723Z","shell.execute_reply.started":"2024-11-10T09:07:31.691469Z","shell.execute_reply":"2024-11-10T09:07:31.697936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # for ppg\n\n# # sample_raw_ppg = pd.read_csv(\n# #    '/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/10/10/raw_ppg.csv', \n# #     names=['timestamp', 'ppg_value', 'extra_column'],  # Define expected columns\n# #     na_values=[''],  # Treat empty strings as NaN\n# #     skiprows=1  # Skip header if needed\n# # )\n# sample_raw_ppg = pd.read_csv(\n#    '/kaggle/input/raw_data_ppg_gsr/10/10/raw_ppg.csv', \n#     names=['timestamp', 'ppg_value', 'extra_column'],  # Define expected columns\n#     na_values=[''],  # Treat empty strings as NaN\n#     skiprows=1  # Skip header if needed\n# )\n# print(sample_raw_ppg['ppg_value'])\n# ppg_10 = sample_raw_ppg['ppg_value']","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:31.699707Z","iopub.execute_input":"2024-11-10T09:07:31.700053Z","iopub.status.idle":"2024-11-10T09:07:31.820494Z","shell.execute_reply.started":"2024-11-10T09:07:31.70002Z","shell.execute_reply":"2024-11-10T09:07:31.819387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ppg_10.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:31.824436Z","iopub.execute_input":"2024-11-10T09:07:31.824745Z","iopub.status.idle":"2024-11-10T09:07:31.830657Z","shell.execute_reply.started":"2024-11-10T09:07:31.824713Z","shell.execute_reply":"2024-11-10T09:07:31.829838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Organize data as shown below :\n\nX_train :\n\n| subject_id | sequence_id | ppg_series | gsr_series |\n|------------|-------------|------------|------------|\n| 1          | 1           | [...]      | [...]      |\n| 1          | 2           | [...]      | [...]      |\n| 1          | 3           | [...]      | [...]      |\n| 2          | 1           | [...]      | [...]      |\n| 2          | 2           | [...]      | [...]      |\n| 2          | 3           | [...]      | [...]      |\n\n\ny_train :\n\n| subject_id | sequence_id | arousal | valence |\n|------------|-------------|---------|---------|\n| 1          | 1           | 7       | 6       |\n| 1          | 2           | 9       | 8       |\n| 1          | 3           | 4       | 5       |\n| 2          | 1           | 6       | 2       |\n| 2          | 2           | 8       | 4       |\n| 2          | 3           | 5       | 6       |","metadata":{}},{"cell_type":"markdown","source":"# Updated features_target dataframe\n\n| subject_id | series_id | ppg_series | gsr_series | Target |\n|------------|-----------|------------|------------|--------|\n| 1          | 1         | [...]      | [...]      | AxVy   |\n| 1          | 2         | [...]      | [...]      | AxVy   |\n| 1          | 3         | [...]      | [...]      | AxVy   |\n| 2          | 1         | [...]      | [...]      | AxVy   |\n| 2          | 2         | [...]      | [...]      | AxVy   |\n| 2          | 3         | [...]      | [...]      | AxVy   |\n\n","metadata":{}},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# import numpy as np\n\n# # Initialize an empty list to store data for each row\n# data = []\n\n# # Define the base directory\n# # base_dir = '/kaggle/input/raw-ppg-gsr-data-for-emotion-recognition/raw_data_ppg_gsr/'\n# base_dir = '/kaggle/input/raw_data_ppg_gsr/'\n\n# # Loop through each subject's subdirectory\n# for subject_folder in os.listdir(base_dir):\n#     subject_dir = os.path.join(base_dir, subject_folder, subject_folder)\n    \n#     if os.path.isdir(subject_dir):  # Check if it’s a directory\n#         subject_id = int(subject_folder)  # Extract subject_id from folder name\n        \n#         # Load raw_gsr.csv and raw_ppg.csv with specific column names\n#         raw_gsr_path = os.path.join(subject_dir, 'raw_gsr.csv')\n#         raw_ppg_path = os.path.join(subject_dir, 'raw_ppg.csv')\n        \n#         # Define column names and load the data, treating empty strings as NaN and skipping the header row\n#         raw_gsr = pd.read_csv(\n#             raw_gsr_path, \n#             names=['timestamp', 'gsr_value', 'extra_column'], \n#             na_values=[''], \n#             skiprows=1\n#         )['gsr_value']  # Select only the 'gsr_value' column\n\n#         raw_ppg = pd.read_csv(\n#             raw_ppg_path, \n#             names=['timestamp', 'ppg_value', 'extra_column'], \n#             na_values=[''], \n#             skiprows=1\n#         )['ppg_value']  # Select only the 'ppg_value' column\n        \n#         # Calculate downsampling factor\n#         gsr_downsample_factor = len(raw_gsr) // 6000\n#         ppg_downsample_factor = len(raw_ppg) // 6000\n        \n#         # Downsample by taking every nth data point\n#         downsampled_gsr = raw_gsr.iloc[::gsr_downsample_factor].head(6000)\n#         downsampled_ppg = raw_ppg.iloc[::ppg_downsample_factor].head(6000)\n        \n#         # Split the downsampled data into chunks of 200 for each row in the DataFrame\n#         gsr_chunks = np.array_split(downsampled_gsr.values.flatten(), 30)\n#         ppg_chunks = np.array_split(downsampled_ppg.values.flatten(), 30)\n        \n#         # Load Arousal_Valence.csv for target labels\n#         av_path = os.path.join(subject_dir, 'Arousal_Valence.csv')\n#         av_data = pd.read_csv(av_path)\n\n#         for series_id in range(1, 31):  # For each of the 30 sequences per subject\n#             # Extract the chunk of 200 values for gsr_series and ppg_series\n#             gsr_series = gsr_chunks[series_id - 1]\n#             ppg_series = ppg_chunks[series_id - 1]\n            \n#             # Create target label in \"AxVy\" format\n#             valence = int(av_data.iloc[series_id - 1, 1])  # Second column is Valence\n#             arousal = int(av_data.iloc[series_id - 1, 2])  # Third column is Arousal\n#             target = f\"A{arousal}V{valence}\"\n            \n#             # Append the data to the list\n#             data.append({\n#                 'subject_id': subject_id,\n#                 'series_id': series_id,\n#                 'ppg_series': ppg_series,\n#                 'gsr_series': gsr_series,\n#                 'target': target\n#             })\n\n# # Convert the list of dictionaries to a DataFrame\n# features_target = pd.DataFrame(data)\n\n# # Display the first few rows of the DataFrame\n# features_target.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:31.832055Z","iopub.execute_input":"2024-11-10T09:07:31.832402Z","iopub.status.idle":"2024-11-10T09:07:40.501827Z","shell.execute_reply.started":"2024-11-10T09:07:31.83237Z","shell.execute_reply":"2024-11-10T09:07:40.500802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function to categorize Arousal (A) and Valence (V)\n# def categorize_arousal_valence(target_value):\n#     # Extract the Arousal (Ax) and Valence (Vy) digits\n#     arousal_level = int(target_value[1])\n#     valence_level = int(target_value[3])\n    \n#     # Determine arousal label\n#     if 1 <= arousal_level <= 3:\n#         arousal_label = 'L'\n#     elif 4 <= arousal_level <= 6:\n#         arousal_label = 'M'\n#     elif 7 <= arousal_level <= 9:\n#         arousal_label = 'H'\n    \n#     # Determine valence label\n#     if 1 <= valence_level <= 3:\n#         valence_label = 'L'\n#     elif 4 <= valence_level <= 6:\n#         valence_label = 'M'\n#     elif 7 <= valence_level <= 9:\n#         valence_label = 'H'\n    \n#     # Create the grouped target label\n#     return f\"A{arousal_label}V{valence_label}\"\n\n# # Apply the function to each row in the DataFrame\n# features_target['grouped_target'] = features_target['target'].apply(categorize_arousal_valence)\n\n# # Display the updated DataFrame\n# print(features_target)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:40.503023Z","iopub.execute_input":"2024-11-10T09:07:40.503368Z","iopub.status.idle":"2024-11-10T09:07:40.529477Z","shell.execute_reply.started":"2024-11-10T09:07:40.503334Z","shell.execute_reply":"2024-11-10T09:07:40.528458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_target.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:40.530897Z","iopub.execute_input":"2024-11-10T09:07:40.531575Z","iopub.status.idle":"2024-11-10T09:07:40.542167Z","shell.execute_reply.started":"2024-11-10T09:07:40.531526Z","shell.execute_reply":"2024-11-10T09:07:40.541037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# List all unique subject_id values","metadata":{}},{"cell_type":"code","source":"unique_subject_ids = features_target['subject_id'].unique()\nprint(unique_subject_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:40.543396Z","iopub.execute_input":"2024-11-10T09:07:40.543776Z","iopub.status.idle":"2024-11-10T09:07:40.550729Z","shell.execute_reply.started":"2024-11-10T09:07:40.54374Z","shell.execute_reply":"2024-11-10T09:07:40.549601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Count of unique subject_id values","metadata":{}},{"cell_type":"code","source":"unique_subject_count = features_target['subject_id'].nunique()\nprint(unique_subject_count)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:40.551876Z","iopub.execute_input":"2024-11-10T09:07:40.552663Z","iopub.status.idle":"2024-11-10T09:07:40.558607Z","shell.execute_reply.started":"2024-11-10T09:07:40.552622Z","shell.execute_reply":"2024-11-10T09:07:40.557551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# After preprocessing check that each subject id has the same number of sequence ids (value counts)","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom matplotlib.ticker import MaxNLocator\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom multiprocessing import cpu_count\n\nfrom sklearn.metrics import classification_report , confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:40.559781Z","iopub.execute_input":"2024-11-10T09:07:40.56013Z","iopub.status.idle":"2024-11-10T09:07:42.811689Z","shell.execute_reply.started":"2024-11-10T09:07:40.560088Z","shell.execute_reply":"2024-11-10T09:07:42.810684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing to convert string values of surfaces to integers for neural network","metadata":{}},{"cell_type":"code","source":"# USING NUMERIC VALUES OF AROUSAL & VALENCE AS IS\n\n# label_encoder = LabelEncoder()\n# encoded_labels = label_encoder.fit_transform(features_target.target)\n\n# # GROUPING NUMERIC VALUES BASED ON RANGES: 1-3, 4-6 AND 7-9\n# label_encoder = LabelEncoder()\n# encoded_labels = label_encoder.fit_transform(features_target.grouped_target)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:42.81299Z","iopub.execute_input":"2024-11-10T09:07:42.813533Z","iopub.status.idle":"2024-11-10T09:07:42.819681Z","shell.execute_reply.started":"2024-11-10T09:07:42.813485Z","shell.execute_reply":"2024-11-10T09:07:42.81871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_labels","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:42.820833Z","iopub.execute_input":"2024-11-10T09:07:42.82123Z","iopub.status.idle":"2024-11-10T09:07:42.834108Z","shell.execute_reply.started":"2024-11-10T09:07:42.821186Z","shell.execute_reply":"2024-11-10T09:07:42.832909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#labels are stored in the classes_ property in label encoder\nlabel_encoder.classes_\n# to reverse transformation if and when needed","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:42.835268Z","iopub.execute_input":"2024-11-10T09:07:42.835663Z","iopub.status.idle":"2024-11-10T09:07:42.843395Z","shell.execute_reply.started":"2024-11-10T09:07:42.835617Z","shell.execute_reply":"2024-11-10T09:07:42.842415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_target['label']=encoded_labels","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:42.844613Z","iopub.execute_input":"2024-11-10T09:07:42.844906Z","iopub.status.idle":"2024-11-10T09:07:42.851777Z","shell.execute_reply.started":"2024-11-10T09:07:42.84487Z","shell.execute_reply":"2024-11-10T09:07:42.85087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_labels_count = features_target['label'].nunique()\nprint(unique_labels_count)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:42.852928Z","iopub.execute_input":"2024-11-10T09:07:42.853268Z","iopub.status.idle":"2024-11-10T09:07:42.86081Z","shell.execute_reply.started":"2024-11-10T09:07:42.853236Z","shell.execute_reply":"2024-11-10T09:07:42.859937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_target.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:42.861978Z","iopub.execute_input":"2024-11-10T09:07:42.862336Z","iopub.status.idle":"2024-11-10T09:07:42.888941Z","shell.execute_reply.started":"2024-11-10T09:07:42.862303Z","shell.execute_reply":"2024-11-10T09:07:42.888103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import torch\n# from sklearn.model_selection import train_test_split\n# from torch.utils.data import Dataset\n\n# # Step 1: Initialize an empty list to store sequences and labels\n# sequences = []\n\n# # Step 2: Iterate through each row in features_target DataFrame to create sequence_features and add to sequences list\n# for _, row in features_target.iterrows():\n#     # Extract the first 200 values from ppg_series and gsr_series, assuming they are stored as arrays\n#     sequence_features = pd.DataFrame({\n#         'ppg_series': row['ppg_series'][:200],\n#         'gsr_series': row['gsr_series'][:200]\n#     })\n    \n#     # Append (sequence_features, label) as a tuple to the sequences list\n#     sequences.append((sequence_features, row['label']))","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:42.890047Z","iopub.execute_input":"2024-11-10T09:07:42.890411Z","iopub.status.idle":"2024-11-10T09:07:43.424666Z","shell.execute_reply.started":"2024-11-10T09:07:42.890368Z","shell.execute_reply":"2024-11-10T09:07:43.423871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train_sequences, test_sequences = train_test_split(sequences, test_size=0.2)\n\nlen(train_sequences) , len(test_sequences)","metadata":{}},{"cell_type":"code","source":"# Step 3: Split into training and test sets\ntrain_sequences, test_sequences = train_test_split(sequences, test_size=0.2)\nlen(train_sequences), len(test_sequences)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:43.425765Z","iopub.execute_input":"2024-11-10T09:07:43.426082Z","iopub.status.idle":"2024-11-10T09:07:43.433784Z","shell.execute_reply.started":"2024-11-10T09:07:43.426038Z","shell.execute_reply":"2024-11-10T09:07:43.432811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Dataset class\n# class MixedEmotionDataset(Dataset): \n#     def __init__(self, sequences):\n#         self.sequences = sequences\n        \n#     def __len__(self):\n#         return len(self.sequences)\n    \n#     def __getitem__(self, idx):\n#         sequence, label = self.sequences[idx]\n#         return dict(\n#             sequence=torch.Tensor(sequence.to_numpy()),  # Convert DataFrame to tensor\n#             label=torch.tensor(label).long()\n#         )","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:43.434858Z","iopub.execute_input":"2024-11-10T09:07:43.435199Z","iopub.status.idle":"2024-11-10T09:07:43.442404Z","shell.execute_reply.started":"2024-11-10T09:07:43.435166Z","shell.execute_reply":"2024-11-10T09:07:43.441361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # MixedEmotionDataLoader for train, validation, and test DataLoaders\n# class MixedEmotionDataLoader:\n    \n#     def __init__(self, train_sequences, test_sequences, batch_size):\n#         self.train_sequences = train_sequences\n#         self.test_sequences = test_sequences\n#         self.batch_size = batch_size\n#         self.setup()\n        \n#     def setup(self):\n#         self.train_dataset = MixedEmotionDataset(self.train_sequences)\n#         self.test_dataset = MixedEmotionDataset(self.test_sequences)\n        \n#     def get_train_loader(self):\n#         return DataLoader(\n#             self.train_dataset,\n#             batch_size=self.batch_size,\n#             shuffle=True,\n#             num_workers=cpu_count()\n#         )\n    \n#     def get_val_loader(self):\n#         return DataLoader(\n#             self.test_dataset,\n#             batch_size=self.batch_size,\n#             shuffle=False,\n#             num_workers=cpu_count()\n#         )\n    \n#     def get_test_loader(self):\n#         return DataLoader(\n#             self.test_dataset,\n#             batch_size=self.batch_size,\n#             shuffle=False,\n#             num_workers=cpu_count()\n#         )\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:43.443577Z","iopub.execute_input":"2024-11-10T09:07:43.443882Z","iopub.status.idle":"2024-11-10T09:07:43.451987Z","shell.execute_reply.started":"2024-11-10T09:07:43.443852Z","shell.execute_reply":"2024-11-10T09:07:43.45112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# OLDER VERSION\n\n# class SequenceModel(nn.Module):\n#     # classification, number of hidden units, number of layers for LSTM\n    \n#     def __init__(self, n_features, n_classes , n_hidden=256, n_layers=3):\n#         super().__init__()\n        \n#         self.lstm = nn.LSTM(\n#         input_size = n_features,\n#         hidden_size=n_hidden,\n#         num_layers=n_layers,\n#         batch_first=True,\n#         dropout=0.75)\n        \n#         self.classifier = nn.Linear(n_hidden , n_classes)\n        \n#     def forward(self, x):\n#         self.lstm.flatten_parameters()\n#         _, (hidden, _) = self.lstm(x)\n        \n#         out = hidden[-1]\n        \n#         return self.classifier(out)\n    \n# NEWER VERSION\n\n\n# import torch\n# import torch.nn as nn\n\n# class ImprovedSequenceModel(nn.Module):\n#     def __init__(self, n_features, n_classes, n_hidden=512, n_layers=3):\n#         super().__init__()\n        \n#         # One-directional LSTM with lower dropout\n#         self.lstm = nn.LSTM(\n#             input_size=n_features,\n#             hidden_size=n_hidden,\n#             num_layers=n_layers,\n#             batch_first=True,\n#             dropout=0.3,\n#             bidirectional=False  # One-directional LSTM\n#         )\n        \n#         # Layer Normalization for LSTM outputs\n#         self.layer_norm = nn.LayerNorm(n_hidden)\n\n#         # Classifier Layer\n#         self.classifier = nn.Linear(n_hidden, n_classes)  # Adjust for one-directional output\n        \n#     def forward(self, x):\n#         self.lstm.flatten_parameters()\n        \n#         # Pass through LSTM\n#         output, (hidden, _) = self.lstm(x)\n        \n#         # Only use the last hidden state\n#         out = hidden[-1]  # Take the final hidden state only\n        \n#         # Apply layer normalization\n#         out = self.layer_norm(out)\n        \n#         return self.classifier(out)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:43.453031Z","iopub.execute_input":"2024-11-10T09:07:43.453367Z","iopub.status.idle":"2024-11-10T09:07:43.464144Z","shell.execute_reply.started":"2024-11-10T09:07:43.453335Z","shell.execute_reply":"2024-11-10T09:07:43.463232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torchmetrics import Accuracy\n# import matplotlib.pyplot as plt\n\n# class MixedEmotionPredictor(nn.Module): \n#     def __init__(self, n_features: int, n_classes: int):\n#         super(MixedEmotionPredictor, self).__init__()\n#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# #         self.model = SequenceModel(n_features, n_classes).to(self.device)\n#         self.model = ImprovedSequenceModel(n_features, n_classes).to(self.device)\n\n#         self.criterion = nn.CrossEntropyLoss()\n#         self.accuracy_metric = Accuracy(task='multiclass', num_classes=n_classes).to(self.device)\n        \n#         # Initialize lists to store metrics for each epoch\n#         self.epoch_train_losses = []\n#         self.epoch_val_losses = []\n#         self.epoch_train_accuracies = []\n#         self.epoch_val_accuracies = []\n        \n#     def forward(self, x):\n#         return self.model(x)\n    \n#     def compute_loss_and_accuracy(self, outputs, labels):\n#         loss = self.criterion(outputs, labels)\n#         predictions = torch.argmax(outputs, dim=1)\n#         accuracy = self.accuracy_metric(predictions, labels)\n#         return loss, accuracy\n        \n#     def plot_metrics(self):\n#         num_epochs = min(len(self.epoch_train_losses), len(self.epoch_val_losses))\n#         epochs = range(1, num_epochs + 1)\n        \n#         train_losses = self.epoch_train_losses[:num_epochs]\n#         val_losses = self.epoch_val_losses[:num_epochs]\n#         train_accuracies = self.epoch_train_accuracies[:num_epochs]\n#         val_accuracies = self.epoch_val_accuracies[:num_epochs]\n        \n#         plt.figure(figsize=(12, 5))\n        \n#         # Plot Loss\n#         plt.subplot(1, 2, 1)\n#         plt.plot(epochs, train_losses, 'b', label='Training Loss')\n#         plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n#         plt.title('Training and Validation Loss')\n#         plt.xlabel('Epochs')\n#         plt.ylabel('Loss')\n#         plt.legend()\n        \n#         # Plot Accuracy\n#         plt.subplot(1, 2, 2)\n#         plt.plot(epochs, train_accuracies, 'b', label='Training Accuracy')\n#         plt.plot(epochs, val_accuracies, 'r', label='Validation Accuracy')\n#         plt.title('Training and Validation Accuracy')\n#         plt.xlabel('Epochs')\n#         plt.ylabel('Accuracy')\n#         plt.legend()\n        \n#         plt.tight_layout()\n#         plt.show()\n    \n#     def train_model(self, train_loader, val_loader, num_epochs=10, lr=0.0001):\n#         optimizer = optim.Adam(self.parameters(), lr=lr)\n\n#         for epoch in range(num_epochs):\n#             # Training phase\n#             self.train()\n#             train_losses = []\n#             train_accuracies = []\n\n#             for batch in train_loader:\n#                 sequences = batch[\"sequence\"].to(self.device)\n#                 labels = batch[\"label\"].to(self.device)\n\n#                 optimizer.zero_grad()\n#                 outputs = self(sequences)\n#                 loss, accuracy = self.compute_loss_and_accuracy(outputs, labels)\n\n#                 loss.backward()\n#                 optimizer.step()\n\n#                 train_losses.append(loss.item())\n#                 train_accuracies.append(accuracy.item())\n\n#             avg_train_loss = sum(train_losses) / len(train_losses)\n#             avg_train_accuracy = sum(train_accuracies) / len(train_accuracies)\n\n#             # Validation phase\n#             self.eval()\n#             val_losses = []\n#             val_accuracies = []\n\n#             with torch.no_grad():\n#                 for batch in val_loader:\n#                     sequences = batch[\"sequence\"].to(self.device)\n#                     labels = batch[\"label\"].to(self.device)\n\n#                     outputs = self(sequences)\n#                     loss, accuracy = self.compute_loss_and_accuracy(outputs, labels)\n\n#                     val_losses.append(loss.item())\n#                     val_accuracies.append(accuracy.item())\n\n#             avg_val_loss = sum(val_losses) / len(val_losses)\n#             avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n\n#             # Store epoch metrics for plotting\n#             self.epoch_train_losses.append(avg_train_loss)\n#             self.epoch_val_losses.append(avg_val_loss)\n#             self.epoch_train_accuracies.append(avg_train_accuracy)\n#             self.epoch_val_accuracies.append(avg_val_accuracy)\n\n#             # Print metrics\n#             print(f\"Epoch {epoch + 1}/{num_epochs}\")\n#             print(f\"  Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_accuracy:.4f}\")\n#             print(f\"  Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}\")\n\n#         # Plot training and validation metrics\n#         self.plot_metrics()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:43.465572Z","iopub.execute_input":"2024-11-10T09:07:43.466267Z","iopub.status.idle":"2024-11-10T09:07:44.423468Z","shell.execute_reply.started":"2024-11-10T09:07:43.466222Z","shell.execute_reply":"2024-11-10T09:07:44.422618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import torch\n# from torch.utils.tensorboard import SummaryWriter\n# import os\n\n# # Parameters\n# N_EPOCHS = 250\n# BATCH_SIZE = 64\n# CHECKPOINT_DIR = \"checkpoints\"\n# LOG_DIR = \"lightning_logs/surface\"\n# BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best-checkpoint.pth\")\n\n# # Ensure directories exist\n# os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# # Instantiate TensorBoard writer\n# writer = SummaryWriter(LOG_DIR)\n\n# # Function to save the model checkpoint\n# def save_checkpoint(model, optimizer, epoch, val_loss, best_val_loss):\n#     # Only save the model if the current val_loss is better than the best_val_loss\n#     if val_loss < best_val_loss:\n#         print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model...\")\n#         torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'val_loss': val_loss,\n#         }, BEST_MODEL_PATH)\n#         return val_loss\n#     return best_val_loss\n\n# # Training loop\n# def train(model, train_loader, val_loader, criterion, optimizer):\n#     best_val_loss = float('inf')\n\n#     for epoch in range(1, N_EPOCHS + 1):\n#         model.train()\n#         train_loss = 0.0\n#         train_correct = 0\n#         train_total = 0\n        \n#         # Training step\n#         for batch in train_loader:\n#             sequences, labels = batch[\"sequence\"].to(model.device), batch[\"label\"].to(model.device)\n#             optimizer.zero_grad()\n#             outputs = model(sequences)\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n            \n#             train_loss += loss.item() * sequences.size(0)\n#             train_correct += (outputs.argmax(1) == labels).sum().item()\n#             train_total += labels.size(0)\n        \n#         avg_train_loss = train_loss / train_total\n#         train_accuracy = train_correct / train_total\n        \n#         # Validation step\n#         model.eval()\n#         val_loss = 0.0\n#         val_correct = 0\n#         val_total = 0\n#         with torch.no_grad():\n#             for batch in val_loader:\n#                 sequences, labels = batch[\"sequence\"].to(model.device), batch[\"label\"].to(model.device)\n#                 outputs = model(sequences)\n#                 loss = criterion(outputs, labels)\n#                 val_loss += loss.item() * sequences.size(0)\n#                 val_correct += (outputs.argmax(1) == labels).sum().item()\n#                 val_total += labels.size(0)\n                \n#         avg_val_loss = val_loss / val_total\n#         val_accuracy = val_correct / val_total\n\n#         # Logging to TensorBoard\n#         writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n#         writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n#         writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n#         writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n\n#         # Print metrics\n#         print(f\"Epoch {epoch}/{N_EPOCHS}, \"\n#               f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n#               f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n        \n#         # Save metrics to model for plotting\n#         model.epoch_train_losses.append(avg_train_loss)\n#         model.epoch_train_accuracies.append(train_accuracy)\n#         model.epoch_val_losses.append(avg_val_loss)\n#         model.epoch_val_accuracies.append(val_accuracy)\n        \n#         # Save checkpoint\n#         best_val_loss = save_checkpoint(model, optimizer, epoch, avg_val_loss, best_val_loss)\n\n#     writer.close()\n\n# # Example usage\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# n_features = 2  # using ppg_series and gsr_series features\n# n_classes = len(label_encoder.classes_)  # Replace with actual number of classes\n\n# # Initialize model, optimizer, and criterion\n# model = MixedEmotionPredictor(n_features=n_features, n_classes=n_classes).to(device)\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n# criterion = torch.nn.CrossEntropyLoss()\n\n# # Instantiate data loader\n# data_loader = MixedEmotionDataLoader(train_sequences, test_sequences, BATCH_SIZE)\n# train_loader = data_loader.get_train_loader()\n# val_loader = data_loader.get_val_loader()\n\n# # Train the model\n# train(model, train_loader, val_loader, criterion, optimizer)\n\n# # Plot metrics\n# model.plot_metrics()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T09:07:44.425239Z","iopub.execute_input":"2024-11-10T09:07:44.425807Z","iopub.status.idle":"2024-11-10T09:28:36.487951Z","shell.execute_reply.started":"2024-11-10T09:07:44.425761Z","shell.execute_reply":"2024-11-10T09:28:36.486857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}